The virtual environment was not created successfully because ensurepip is not
available.  On Debian/Ubuntu systems, you need to install the python3-venv
package using the following command.

    apt install python3.10-venv

You may need to use sudo with that command.  After installing the python3-venv
package, recreate your virtual environment.

Failing command: /mnt/llama.cpp/venv/bin/python3

ci/run.sh: line 582: /mnt/llama.cpp/venv/bin/activate: No such file or directory
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: numpy~=1.24.4 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 1)) (1.24.4)
Requirement already satisfied: sentencepiece~=0.1.98 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.1.98)
Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.37.1)
Requirement already satisfied: gguf>=0.1.0 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 4)) (0.7.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 5)) (4.23.4)
Collecting torch~=2.1.1
  Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)
Requirement already satisfied: regex!=2019.12.17 in /home/ggml/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.6.3)
Requirement already satisfied: safetensors>=0.3.1 in /home/ggml/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.1)
Requirement already satisfied: packaging>=20.0 in /home/ggml/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.1)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ggml/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.15.1)
Requirement already satisfied: requests in /home/ggml/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)
Requirement already satisfied: tqdm>=4.27 in /home/ggml/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.65.0)
Requirement already satisfied: filelock in /home/ggml/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.12.2)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ggml/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.19.4)
Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (5.4.1)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ggml/.local/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.0.3)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ggml/.local/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/ggml/.local/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.18.1)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ggml/.local/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.0.106)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ggml/.local/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ggml/.local/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.4.5.107)
Requirement already satisfied: networkx in /home/ggml/.local/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ggml/.local/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ggml/.local/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (8.9.2.26)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ggml/.local/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ggml/.local/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (10.3.2.106)
Requirement already satisfied: typing-extensions in /home/ggml/.local/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.7.1)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ggml/.local/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.3.1)
Requirement already satisfied: fsspec in /home/ggml/.local/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.6.0)
Collecting triton==2.1.0
  Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)
Requirement already satisfied: sympy in /home/ggml/.local/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ggml/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.3.101)
Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ggml/.local/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.1.0)
Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3)
Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2020.6.20)
Requirement already satisfied: charset-normalizer<4,>=2 in /home/ggml/.local/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3.2)
Requirement already satisfied: mpmath>=0.19 in /home/ggml/.local/lib/python3.10/site-packages (from sympy->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)
Installing collected packages: triton, torch
  Attempting uninstall: triton
    Found existing installation: triton 2.0.0
    Uninstalling triton-2.0.0:
      Successfully uninstalled triton-2.0.0
  Attempting uninstall: torch
    Found existing installation: torch 2.0.1
    Uninstalling torch-2.0.1:
      Successfully uninstalled torch-2.0.1
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
torchvision 0.15.2 requires torch==2.0.1, but you have torch 2.1.2 which is incompatible.
torchaudio 2.0.2 requires torch==2.0.1, but you have torch 2.1.2 which is incompatible.
Successfully installed torch-2.1.2 triton-2.1.0
Defaulting to user installation because normal site-packages is not writeable
Obtaining file:///home/ggml/work/llama.cpp/gguf-py
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /home/ggml/.local/lib/python3.10/site-packages (from gguf==0.7.0) (1.24.4)
Building wheels for collected packages: gguf
  Building editable for gguf (pyproject.toml): started
  Building editable for gguf (pyproject.toml): finished with status 'done'
  Created wheel for gguf: filename=gguf-0.7.0-py3-none-any.whl size=3229 sha256=e77d3489e0beea138863799a05abb421092af60b67ffc67b376508ee3f0720bf
  Stored in directory: /tmp/pip-ephem-wheel-cache-abzum2s4/wheels/a3/4c/52/c5934ad001d1a70ca5434f11ddc622cad9c0a484e9bf6feda3
Successfully built gguf
Installing collected packages: gguf
  Attempting uninstall: gguf
    Found existing installation: gguf 0.7.0
    Uninstalling gguf-0.7.0:
      Successfully uninstalled gguf-0.7.0
Successfully installed gguf-0.7.0
+ gg_run_ctest_debug
+ cd /home/ggml/work/llama.cpp
+ tee /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/ctest_debug.log
+ rm -rf build-ci-debug
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_CUBLAS=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.140") 
-- cuBLAS found
-- The CUDA compiler identification is NVIDIA 12.2.140
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Using CUDA architectures: 52;61;70
-- CUDA host compiler is GNU 11.4.0

-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (3.1s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m3.269s
user	0m2.561s
sys	0m0.694s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/ctest_debug-make.log
+ make -j
[  1%] Generating build details from Git
[  2%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Built target build_info
/home/ggml/work/llama.cpp/ggml-cuda.cu(645): warning #177-D: function "warp_reduce_sum(half2)" was declared but never referenced
                                   half2 warp_reduce_sum(half2 a) {
                                         ^

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/home/ggml/work/llama.cpp/ggml-cuda.cu(666): warning #177-D: function "warp_reduce_max(half2)" was declared but never referenced
                                   half2 warp_reduce_max(half2 x) {
                                         ^

/home/ggml/work/llama.cpp/ggml-cuda.cu(1696): warning #177-D: variable "ksigns64" was declared but never referenced
                         uint64_t ksigns64[128] = {
                                  ^

[  5%] Built target ggml
[  6%] Linking CUDA static library libggml_static.a
[  7%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  7%] Built target ggml_static
[  8%] Linking CXX static library libllama.a
[  8%] Built target llama
[  9%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 10%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 11%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 11%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 12%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 16%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 17%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 18%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 19%] Linking CXX executable ../bin/test-c
[ 19%] Built target test-c
/home/ggml/work/llama.cpp/common/sampling.cpp: In function ‘void sampler_queue(llama_context*, const llama_sampling_params&, llama_token_data_array&, size_t&)’:
/home/ggml/work/llama.cpp/common/sampling.cpp:130:15: warning: unused variable ‘n_vocab’ [-Wunused-variable]
  130 |     const int n_vocab = llama_n_vocab(llama_get_model(ctx_main));
      |               ^~~~~~~
[ 20%] Linking CXX executable ../../bin/benchmark
[ 20%] Built target benchmark
[ 21%] Linking CXX executable ../../bin/quantize
[ 21%] Built target quantize
[ 22%] Linking CXX executable ../../bin/quantize-stats
[ 22%] Built target quantize-stats
[ 22%] Built target llava
[ 23%] Linking CXX static library libllava_static.a
[ 23%] Built target llava_static
[ 23%] Linking CXX static library libcommon.a
[ 23%] Built target common
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/get-model.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 43%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 44%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 46%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 47%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 48%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 49%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 50%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 50%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 51%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 52%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 52%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 53%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 54%] Linking CXX executable ../bin/test-model-load-cancel
[ 54%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 55%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 55%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 56%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 57%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 57%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 58%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 59%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 60%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 61%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 62%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 63%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 63%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 64%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 65%] Linking CXX executable ../bin/test-grad0
[ 66%] Linking CXX executable ../bin/test-rope
[ 67%] Linking CXX executable ../bin/test-autorelease
[ 67%] Built target test-grad0
[ 67%] Built target test-rope
[ 68%] Linking CXX executable ../bin/test-quantize-fns
[ 68%] Built target test-model-load-cancel
[ 69%] Linking CXX executable ../bin/test-sampling
[ 70%] Linking CXX executable ../bin/test-grammar-parser
[ 70%] Built target test-quantize-fns
[ 70%] Built target test-grammar-parser
[ 71%] Linking CXX executable ../../bin/baby-llama
[ 71%] Built target test-autorelease
[ 72%] Linking CXX executable ../../bin/q8dot
[ 73%] Linking CXX executable ../../bin/tokenize
[ 74%] Linking CXX executable ../../bin/vdot
[ 74%] Built target q8dot
[ 74%] Built target test-sampling
[ 75%] Linking CXX executable ../../bin/embedding
[ 75%] Built target vdot
[ 75%] Linking CXX executable ../../bin/batched-bench
[ 76%] Linking CXX executable ../../bin/beam-search
[ 77%] Linking CXX executable ../../bin/batched
[ 78%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 79%] Linking CXX executable ../../bin/simple
[ 80%] Linking CXX executable ../../bin/save-load-state
[ 81%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 82%] Linking CXX executable ../../bin/llava-cli
[ 83%] Linking CXX executable ../../bin/passkey
[ 84%] Linking CXX executable ../bin/test-quantize-perf
[ 85%] Linking CXX executable ../../bin/lookup
[ 85%] Linking CXX executable ../../bin/lookahead
[ 86%] Linking CXX executable ../../bin/speculative
[ 87%] Linking CXX executable ../../bin/parallel
[ 87%] Built target baby-llama
[ 88%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 88%] Built target tokenize
[ 88%] Linking CXX executable ../../bin/export-lora
[ 89%] Linking CXX executable ../../bin/train-text-from-scratch
[ 89%] Built target test-quantize-perf
[ 90%] Linking CXX executable ../../bin/infill
[ 91%] Linking CXX executable ../../bin/finetune
[ 92%] Linking CXX executable ../../bin/main
[ 92%] Built target embedding
[ 92%] Built target convert-llama2c-to-ggml
[ 92%] Built target export-lora
[ 92%] Built target simple
[ 92%] Built target test-tokenizer-0-falcon
[ 92%] Built target batched
[ 92%] Built target beam-search
[ 92%] Built target batched-bench
[ 92%] Built target save-load-state
[ 92%] Built target test-tokenizer-0-llama
[ 93%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 93%] Built target lookahead
[ 93%] Built target passkey
[ 94%] Linking CXX executable ../../bin/imatrix
[ 95%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 95%] Built target lookup
[ 95%] Built target llava-cli
[ 96%] Linking CXX executable ../bin/test-backend-ops
[ 96%] Built target parallel
[ 96%] Built target train-text-from-scratch
[ 96%] Built target speculative
[ 96%] Built target finetune
[ 96%] Built target test-backend-ops
[ 96%] Built target infill
[ 96%] Built target main
[ 96%] Built target test-tokenizer-1-llama
[ 96%] Built target test-tokenizer-1-bpe
[ 96%] Built target imatrix
[ 97%] Linking CXX executable ../../bin/perplexity
[ 97%] Built target perplexity
[ 98%] Linking CXX executable ../../bin/llama-bench
[ 98%] Built target llama-bench
[ 99%] Linking CXX executable ../bin/test-llama-grammar
[ 99%] Built target test-llama-grammar
[100%] Linking CXX executable ../../bin/server
[100%] Built target server

real	1m27.316s
user	2m37.632s
sys	0m12.602s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/ctest_debug-ctest.log
+ ctest --output-on-failure -L main -E test-opt
Test project /home/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-quantize-fns
 1/20 Test  #1: test-quantize-fns ...................   Passed    0.81 sec
      Start  2: test-quantize-perf
 2/20 Test  #2: test-quantize-perf ..................   Passed    1.24 sec
      Start  3: test-sampling
 3/20 Test  #3: test-sampling .......................   Passed    0.07 sec
      Start  4: test-tokenizer-0-llama
 4/20 Test  #4: test-tokenizer-0-llama ..............   Passed    0.73 sec
      Start  5: test-tokenizer-0-falcon
 5/20 Test  #5: test-tokenizer-0-falcon .............   Passed    1.80 sec
      Start  6: test-tokenizer-1-llama
 6/20 Test  #6: test-tokenizer-1-llama ..............   Passed   18.87 sec
      Start  7: test-tokenizer-1-baichuan
 7/20 Test  #7: test-tokenizer-1-baichuan ...........   Passed   19.20 sec
      Start  8: test-tokenizer-1-falcon
 8/20 Test  #8: test-tokenizer-1-falcon .............   Passed   16.31 sec
      Start  9: test-tokenizer-1-aquila
 9/20 Test  #9: test-tokenizer-1-aquila .............   Passed   19.44 sec
      Start 10: test-tokenizer-1-mpt
10/20 Test #10: test-tokenizer-1-mpt ................   Passed   14.99 sec
      Start 11: test-tokenizer-1-stablelm-3b-4e1t
11/20 Test #11: test-tokenizer-1-stablelm-3b-4e1t ...   Passed   14.75 sec
      Start 12: test-tokenizer-1-gpt-neox
12/20 Test #12: test-tokenizer-1-gpt-neox ...........   Passed   14.79 sec
      Start 13: test-tokenizer-1-refact
13/20 Test #13: test-tokenizer-1-refact .............   Passed   14.47 sec
      Start 14: test-tokenizer-1-starcoder
14/20 Test #14: test-tokenizer-1-starcoder ..........   Passed   14.45 sec
      Start 15: test-tokenizer-1-gpt2
15/20 Test #15: test-tokenizer-1-gpt2 ...............   Passed   14.82 sec
      Start 16: test-grammar-parser
16/20 Test #16: test-grammar-parser .................   Passed    0.00 sec
      Start 17: test-llama-grammar
17/20 Test #17: test-llama-grammar ..................   Passed    0.04 sec
      Start 18: test-grad0
18/20 Test #18: test-grad0 ..........................   Passed    4.72 sec
      Start 19: test-backend-ops
19/20 Test #19: test-backend-ops ....................   Passed  246.66 sec
      Start 20: test-rope
20/20 Test #20: test-rope ...........................   Passed    0.64 sec

100% tests passed, 0 tests failed out of 20

Label Time Summary:
main    = 418.79 sec*proc (20 tests)

Total Test time (real) = 418.80 sec

real	6m58.837s
user	16m17.653s
sys	0m26.385s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_release
+ cd /home/ggml/work/llama.cpp
+ tee /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/ctest_release.log
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/ctest_release-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_CUBLAS=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.140") 
-- cuBLAS found
-- The CUDA compiler identification is NVIDIA 12.2.140
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Using CUDA architectures: 52;61;70
-- CUDA host compiler is GNU 11.4.0

-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (3.0s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m3.259s
user	0m2.408s
sys	0m0.858s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/ctest_release-make.log
+ make -j
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  4%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o
[  5%] Built target build_info
/home/ggml/work/llama.cpp/ggml-cuda.cu(645): warning #177-D: function "warp_reduce_sum(half2)" was declared but never referenced
                                   half2 warp_reduce_sum(half2 a) {
                                         ^

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/home/ggml/work/llama.cpp/ggml-cuda.cu(666): warning #177-D: function "warp_reduce_max(half2)" was declared but never referenced
                                   half2 warp_reduce_max(half2 x) {
                                         ^

/home/ggml/work/llama.cpp/ggml-cuda.cu(1696): warning #177-D: variable "ksigns64" was declared but never referenced
                         uint64_t ksigns64[128] = {
                                  ^

[  5%] Built target ggml
[  6%] Linking CUDA static library libggml_static.a
[  7%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  7%] Built target ggml_static
[  8%] Linking CXX static library libllama.a
[  8%] Built target llama
[  9%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 10%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 11%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 12%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 15%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 15%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 16%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 17%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 18%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 19%] Linking CXX executable ../bin/test-c
[ 19%] Built target test-c
/home/ggml/work/llama.cpp/common/sampling.cpp: In function ‘void sampler_queue(llama_context*, const llama_sampling_params&, llama_token_data_array&, size_t&)’:
/home/ggml/work/llama.cpp/common/sampling.cpp:130:15: warning: unused variable ‘n_vocab’ [-Wunused-variable]
  130 |     const int n_vocab = llama_n_vocab(llama_get_model(ctx_main));
      |               ^~~~~~~
[ 20%] Linking CXX executable ../../bin/benchmark
[ 20%] Built target benchmark
[ 21%] Linking CXX executable ../../bin/quantize
[ 21%] Built target quantize
[ 22%] Linking CXX executable ../../bin/quantize-stats
[ 22%] Built target quantize-stats
[ 22%] Linking CXX static library libcommon.a
[ 22%] Built target common
[ 23%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/get-model.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 43%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 44%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 45%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 46%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 47%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 48%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 48%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 49%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 50%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 51%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 51%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 51%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 52%] Linking CXX executable ../bin/test-model-load-cancel
[ 53%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 53%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 54%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 55%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 55%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 56%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 57%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 58%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 59%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 60%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 60%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 61%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 61%] Built target test-model-load-cancel
[ 62%] Linking CXX executable ../bin/test-rope
[ 63%] Linking CXX executable ../bin/test-autorelease
[ 64%] Linking CXX executable ../bin/test-quantize-fns
[ 64%] Built target test-rope
[ 64%] Built target test-autorelease
[ 64%] Built target test-quantize-fns
[ 65%] Linking CXX executable ../bin/test-grammar-parser
[ 65%] Built target test-grammar-parser
[ 66%] Linking CXX executable ../bin/test-sampling
[ 67%] Linking CXX executable ../../bin/tokenize
[ 68%] Linking CXX executable ../../bin/q8dot
[ 69%] Linking CXX executable ../../bin/baby-llama
[ 70%] Linking CXX executable ../../bin/vdot
[ 70%] Built target test-sampling
[ 70%] Built target q8dot
[ 70%] Built target tokenize
[ 71%] Linking CXX executable ../../bin/embedding
[ 72%] Linking CXX executable ../../bin/save-load-state
[ 73%] Linking CXX executable ../../bin/beam-search
[ 73%] Built target baby-llama
[ 73%] Built target vdot
[ 74%] Linking CXX executable ../bin/test-grad0
[ 74%] Built target embedding
[ 74%] Built target save-load-state
[ 74%] Built target beam-search
[ 74%] Built target test-grad0
[ 75%] Linking CXX executable ../../bin/simple
[ 76%] Linking CXX executable ../../bin/batched
[ 76%] Linking CXX executable ../../bin/batched-bench
[ 77%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 78%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 79%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 79%] Built target simple
[ 80%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 80%] Built target batched
[ 81%] Linking CXX executable ../bin/test-quantize-perf
[ 82%] Linking CXX executable ../../bin/passkey
[ 82%] Built target batched-bench
[ 83%] Linking CXX executable ../../bin/lookup
[ 83%] Built target test-tokenizer-1-bpe
[ 83%] Built target test-tokenizer-0-falcon
[ 84%] Linking CXX executable ../../bin/speculative
[ 84%] Linking CXX executable ../../bin/export-lora
[ 84%] Built target test-tokenizer-1-llama
[ 84%] Built target test-tokenizer-0-llama
[ 84%] Built target test-quantize-perf
[ 84%] Built target lookup
[ 84%] Built target passkey
[ 85%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 86%] Linking CXX executable ../../bin/parallel
[ 86%] Linking CXX executable ../../bin/lookahead
[ 86%] Built target speculative
[ 86%] Built target export-lora
[ 86%] Built target parallel
[ 86%] Built target convert-llama2c-to-ggml
[ 87%] Linking CXX executable ../../bin/imatrix
[ 87%] Built target lookahead
[ 88%] Linking CXX executable ../../bin/train-text-from-scratch
[ 88%] Built target imatrix
[ 88%] Built target train-text-from-scratch
[ 89%] Linking CXX executable ../../bin/finetune
[ 90%] Linking CXX executable ../../bin/infill
[ 90%] Built target finetune
[ 90%] Built target infill
[ 91%] Linking CXX executable ../../bin/main
[ 91%] Built target main
[ 92%] Linking CXX executable ../../bin/perplexity
[ 93%] Linking CXX executable ../bin/test-backend-ops
[ 93%] Built target llava
[ 93%] Built target perplexity
[ 94%] Linking CXX static library libllava_static.a
[ 95%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 96%] Built target llava_static
[ 96%] Built target test-backend-ops
[ 97%] Linking CXX executable ../../bin/llava-cli
[ 97%] Built target llava-cli
[ 98%] Linking CXX executable ../../bin/llama-bench
[ 98%] Built target llama-bench
[ 99%] Linking CXX executable ../bin/test-llama-grammar
[ 99%] Built target test-llama-grammar
[100%] Linking CXX executable ../../bin/server
[100%] Built target server

real	1m54.367s
user	3m37.762s
sys	0m9.895s
+ '[' -z ']'
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/ctest_release-ctest.log
+ ctest --output-on-failure -L main
Test project /home/ggml/work/llama.cpp/build-ci-release
      Start  1: test-quantize-fns
 1/20 Test  #1: test-quantize-fns ...................   Passed    0.71 sec
      Start  2: test-quantize-perf
 2/20 Test  #2: test-quantize-perf ..................   Passed    0.78 sec
      Start  3: test-sampling
 3/20 Test  #3: test-sampling .......................   Passed    0.04 sec
      Start  4: test-tokenizer-0-llama
 4/20 Test  #4: test-tokenizer-0-llama ..............   Passed    0.62 sec
      Start  5: test-tokenizer-0-falcon
 5/20 Test  #5: test-tokenizer-0-falcon .............   Passed    0.87 sec
      Start  6: test-tokenizer-1-llama
 6/20 Test  #6: test-tokenizer-1-llama ..............   Passed    2.72 sec
      Start  7: test-tokenizer-1-baichuan
 7/20 Test  #7: test-tokenizer-1-baichuan ...........   Passed    2.79 sec
      Start  8: test-tokenizer-1-falcon
 8/20 Test  #8: test-tokenizer-1-falcon .............   Passed    2.38 sec
      Start  9: test-tokenizer-1-aquila
 9/20 Test  #9: test-tokenizer-1-aquila .............   Passed    3.02 sec
      Start 10: test-tokenizer-1-mpt
10/20 Test #10: test-tokenizer-1-mpt ................   Passed    2.17 sec
      Start 11: test-tokenizer-1-stablelm-3b-4e1t
11/20 Test #11: test-tokenizer-1-stablelm-3b-4e1t ...   Passed    2.18 sec
      Start 12: test-tokenizer-1-gpt-neox
12/20 Test #12: test-tokenizer-1-gpt-neox ...........   Passed    2.19 sec
      Start 13: test-tokenizer-1-refact
13/20 Test #13: test-tokenizer-1-refact .............   Passed    2.16 sec
      Start 14: test-tokenizer-1-starcoder
14/20 Test #14: test-tokenizer-1-starcoder ..........   Passed    2.16 sec
      Start 15: test-tokenizer-1-gpt2
15/20 Test #15: test-tokenizer-1-gpt2 ...............   Passed    2.19 sec
      Start 16: test-grammar-parser
16/20 Test #16: test-grammar-parser .................   Passed    0.00 sec
      Start 17: test-llama-grammar
17/20 Test #17: test-llama-grammar ..................   Passed    0.04 sec
      Start 18: test-grad0
18/20 Test #18: test-grad0 ..........................   Passed    4.60 sec
      Start 19: test-backend-ops
19/20 Test #19: test-backend-ops ....................   Passed   42.56 sec
      Start 20: test-rope
20/20 Test #20: test-rope ...........................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 20

Label Time Summary:
main    =  74.77 sec*proc (20 tests)

Total Test time (real) =  74.78 sec

real	1m14.816s
user	1m7.721s
sys	0m26.318s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_open_llama_7b_v2
+ tee /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2.log
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-02-10 14:45:38 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json [502/502] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/tokenizer.model
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/tokenizer.model
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/tokenizer.model
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2024-02-10 14:45:39 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json [593/593] -> "tokenizer_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2024-02-10 14:45:39 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json [330/330] -> "special_tokens_map.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json
Last-modified header missing -- time-stamps turned off.
2024-02-10 14:45:39 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json [26788/26788] -> "pytorch_model.bin.index.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00001-of-00002.bin
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00001-of-00002.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00001-of-00002.bin
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00002-of-00002.bin
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00002-of-00002.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00002-of-00002.bin
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json
Last-modified header missing -- time-stamps turned off.
2024-02-10 14:45:41 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json [132/132] -> "generation_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/wikitext/ https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip
+ local out=models-mnt/wikitext/
+ local url=https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/wikitext/
+ cd models-mnt/wikitext/
+ wget -nv -N https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip
+ cd /home/ggml/work/llama.cpp
+ unzip -o models-mnt/wikitext/wikitext-2-raw-v1.zip -d models-mnt/wikitext/
Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ path_models=../models-mnt/open-llama/7B-v2
+ path_wiki=../models-mnt/wikitext/wikitext-2-raw
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_CUBLAS=1 -DLLAMA_CUBLAS=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.140") 
-- cuBLAS found
-- The CUDA compiler identification is NVIDIA 12.2.140
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Using CUDA architectures: 52;61;70
-- CUDA host compiler is GNU 11.4.0

-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (3.0s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m3.191s
user	0m2.396s
sys	0m0.803s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-make.log
+ make -j
[  1%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  3%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  5%] Built target build_info
/home/ggml/work/llama.cpp/ggml-cuda.cu(645): warning #177-D: function "warp_reduce_sum(half2)" was declared but never referenced
                                   half2 warp_reduce_sum(half2 a) {
                                         ^

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/home/ggml/work/llama.cpp/ggml-cuda.cu(666): warning #177-D: function "warp_reduce_max(half2)" was declared but never referenced
                                   half2 warp_reduce_max(half2 x) {
                                         ^

/home/ggml/work/llama.cpp/ggml-cuda.cu(1696): warning #177-D: variable "ksigns64" was declared but never referenced
                         uint64_t ksigns64[128] = {
                                  ^

[  5%] Built target ggml
[  6%] Linking CUDA static library libggml_static.a
[  7%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  7%] Built target ggml_static
[  8%] Linking CXX static library libllama.a
[  8%] Built target llama
[  9%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 10%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 11%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 12%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 14%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 15%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 16%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 17%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 18%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 19%] Linking CXX executable ../bin/test-c
[ 19%] Built target test-c
/home/ggml/work/llama.cpp/common/sampling.cpp: In function ‘void sampler_queue(llama_context*, const llama_sampling_params&, llama_token_data_array&, size_t&)’:
/home/ggml/work/llama.cpp/common/sampling.cpp:130:15: warning: unused variable ‘n_vocab’ [-Wunused-variable]
  130 |     const int n_vocab = llama_n_vocab(llama_get_model(ctx_main));
      |               ^~~~~~~
[ 20%] Linking CXX executable ../../bin/benchmark
[ 20%] Built target benchmark
[ 21%] Linking CXX executable ../../bin/quantize
[ 21%] Built target quantize
[ 22%] Linking CXX executable ../../bin/quantize-stats
[ 22%] Built target quantize-stats
[ 22%] Linking CXX static library libcommon.a
[ 22%] Built target common
[ 23%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/get-model.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 42%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 44%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 45%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 46%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 47%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 48%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 49%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 49%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 50%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 51%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 52%] Linking CXX executable ../bin/test-model-load-cancel
[ 52%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 52%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 53%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 53%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 54%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 55%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 55%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 56%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 57%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 58%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 59%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 60%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 60%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 61%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 62%] Linking CXX executable ../bin/test-rope
[ 62%] Built target test-model-load-cancel
[ 63%] Linking CXX executable ../bin/test-quantize-fns
[ 64%] Linking CXX executable ../bin/test-autorelease
[ 64%] Built target test-rope
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-autorelease
[ 65%] Linking CXX executable ../bin/test-grammar-parser
[ 66%] Linking CXX executable ../../bin/q8dot
[ 66%] Built target test-grammar-parser
[ 67%] Linking CXX executable ../bin/test-sampling
[ 68%] Linking CXX executable ../../bin/tokenize
[ 68%] Built target q8dot
[ 69%] Linking CXX executable ../../bin/baby-llama
[ 70%] Linking CXX executable ../../bin/vdot
[ 71%] Linking CXX executable ../../bin/embedding
[ 71%] Built target tokenize
[ 71%] Built target test-sampling
[ 71%] Built target baby-llama
[ 71%] Built target vdot
[ 72%] Linking CXX executable ../../bin/save-load-state
[ 73%] Linking CXX executable ../../bin/beam-search
[ 74%] Linking CXX executable ../bin/test-grad0
[ 75%] Linking CXX executable ../../bin/batched
[ 75%] Linking CXX executable ../../bin/batched-bench
[ 75%] Built target embedding
[ 75%] Built target save-load-state
[ 76%] Linking CXX executable ../../bin/simple
[ 76%] Built target beam-search
[ 77%] Linking CXX executable ../bin/test-quantize-perf
[ 77%] Built target test-grad0
[ 78%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 78%] Built target batched-bench
[ 79%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 79%] Built target batched
[ 80%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 81%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 81%] Built target simple
[ 82%] Linking CXX executable ../../bin/lookup
[ 82%] Built target test-quantize-perf
[ 82%] Built target test-tokenizer-1-bpe
[ 82%] Built target test-tokenizer-0-falcon
[ 83%] Linking CXX executable ../../bin/passkey
[ 83%] Linking CXX executable ../../bin/export-lora
[ 83%] Built target test-tokenizer-0-llama
[ 83%] Built target test-tokenizer-1-llama
[ 83%] Built target lookup
[ 84%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 84%] Built target export-lora
[ 84%] Built target passkey
[ 84%] Linking CXX executable ../../bin/lookahead
[ 85%] Linking CXX executable ../../bin/parallel
[ 86%] Linking CXX executable ../../bin/speculative
[ 86%] Built target convert-llama2c-to-ggml
[ 86%] Built target lookahead
[ 86%] Built target parallel
[ 86%] Built target speculative
[ 87%] Linking CXX executable ../../bin/train-text-from-scratch
[ 88%] Linking CXX executable ../../bin/imatrix
[ 88%] Built target train-text-from-scratch
[ 89%] Linking CXX executable ../../bin/finetune
[ 89%] Built target imatrix
[ 90%] Linking CXX executable ../../bin/infill
[ 90%] Built target finetune
[ 90%] Built target infill
[ 90%] Built target llava
[ 91%] Linking CXX static library libllava_static.a
[ 92%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 93%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 93%] Built target llava_static
[ 94%] Linking CXX executable ../../bin/main
[ 94%] Built target main
[ 95%] Linking CXX executable ../../bin/perplexity
[ 96%] Linking CXX executable ../bin/test-backend-ops
[ 96%] Built target perplexity
[ 96%] Built target test-backend-ops
[ 97%] Linking CXX executable ../../bin/llava-cli
[ 97%] Built target llava-cli
[ 98%] Linking CXX executable ../../bin/llama-bench
[ 98%] Built target llama-bench
[ 99%] Linking CXX executable ../bin/test-llama-grammar
[ 99%] Built target test-llama-grammar
[100%] Linking CXX executable ../../bin/server
[100%] Built target server

real	1m54.675s
user	3m38.073s
sys	0m10.119s
+ python3 ../convert.py ../models-mnt/open-llama/7B-v2
Loading model file ../models-mnt/open-llama/7B-v2/pytorch_model-00001-of-00002.bin
Loading model file ../models-mnt/open-llama/7B-v2/pytorch_model-00001-of-00002.bin
Loading model file ../models-mnt/open-llama/7B-v2/pytorch_model-00002-of-00002.bin
params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=2048, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('../models-mnt/open-llama/7B-v2'))
Found vocab files: {'tokenizer.model': PosixPath('../models-mnt/open-llama/7B-v2/tokenizer.model'), 'vocab.json': None, 'tokenizer.json': None}
Loading vocab file '../models-mnt/open-llama/7B-v2/tokenizer.model', type 'spm'
Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>
Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'pad': 0}, add special tokens {'bos': True, 'eos': False}>
Permuting layer 0
Permuting layer 1
Permuting layer 2
Permuting layer 3
Permuting layer 4
Permuting layer 5
Permuting layer 6
Permuting layer 7
Permuting layer 8
Permuting layer 9
Permuting layer 10
Permuting layer 11
Permuting layer 12
Permuting layer 13
Permuting layer 14
Permuting layer 15
Permuting layer 16
Permuting layer 17
Permuting layer 18
Permuting layer 19
Permuting layer 20
Permuting layer 21
Permuting layer 22
Permuting layer 23
Permuting layer 24
Permuting layer 25
Permuting layer 26
Permuting layer 27
Permuting layer 28
Permuting layer 29
Permuting layer 30
Permuting layer 31
model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 4096]
model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]
model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [4096, 4096]
model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [4096, 4096]
model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.0.attn_rot_embd
model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]
model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]
model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]
model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [4096, 4096]
model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [4096, 4096]
model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.1.attn_rot_embd
model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]
model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]
model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]
model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [4096, 4096]
model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [4096, 4096]
model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.2.attn_rot_embd
model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]
model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]
model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]
model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [4096, 4096]
model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [4096, 4096]
model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.3.attn_rot_embd
model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]
model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]
model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]
model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [4096, 4096]
model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [4096, 4096]
model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.4.attn_rot_embd
model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]
model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]
model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]
model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [4096, 4096]
model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [4096, 4096]
model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.5.attn_rot_embd
model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]
model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]
model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]
model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [4096, 4096]
model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [4096, 4096]
model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.6.attn_rot_embd
model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]
model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]
model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]
model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [4096, 4096]
model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [4096, 4096]
model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.7.attn_rot_embd
model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]
model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]
model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]
model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [4096, 4096]
model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [4096, 4096]
model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.8.attn_rot_embd
model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]
model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]
model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]
model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [4096, 4096]
model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [4096, 4096]
model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.9.attn_rot_embd
model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]
model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]
model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]
model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [4096, 4096]
model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [4096, 4096]
model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.10.attn_rot_embd
model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]
model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]
model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]
model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [4096, 4096]
model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [4096, 4096]
model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.11.attn_rot_embd
model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]
model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]
model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]
model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [4096, 4096]
model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [4096, 4096]
model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.12.attn_rot_embd
model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]
model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]
model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]
model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [4096, 4096]
model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [4096, 4096]
model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.13.attn_rot_embd
model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]
model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]
model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]
model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [4096, 4096]
model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [4096, 4096]
model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.14.attn_rot_embd
model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]
model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]
model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]
model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [4096, 4096]
model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [4096, 4096]
model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.15.attn_rot_embd
model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]
model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]
model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]
model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [4096, 4096]
model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [4096, 4096]
model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.16.attn_rot_embd
model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]
model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]
model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]
model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [4096, 4096]
model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [4096, 4096]
model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.17.attn_rot_embd
model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]
model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]
model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]
model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [4096, 4096]
model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [4096, 4096]
model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.18.attn_rot_embd
model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]
model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]
model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]
model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [4096, 4096]
model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [4096, 4096]
model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.19.attn_rot_embd
model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]
model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]
model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]
model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [4096, 4096]
model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [4096, 4096]
model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.20.attn_rot_embd
model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]
model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]
model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]
model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [4096, 4096]
model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [4096, 4096]
model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.21.attn_rot_embd
model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]
model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]
model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]
model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [4096, 4096]
model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [4096, 4096]
model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.22.attn_rot_embd
model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]
model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]
model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]
model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [4096, 4096]
model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [4096, 4096]
model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.23.attn_rot_embd
model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]
model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]
model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]
model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [4096, 4096]
model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [4096, 4096]
model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.24.attn_rot_embd
model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]
model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]
model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]
model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [4096, 4096]
model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [4096, 4096]
model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.25.attn_rot_embd
model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]
model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]
model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]
model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [4096, 4096]
model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [4096, 4096]
model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.26.attn_rot_embd
model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]
model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]
model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]
model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [4096, 4096]
model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [4096, 4096]
model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.27.attn_rot_embd
model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]
model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]
model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]
model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [4096, 4096]
model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [4096, 4096]
model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.28.attn_rot_embd
model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]
model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]
model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]
model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [4096, 4096]
model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [4096, 4096]
model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.29.attn_rot_embd
model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]
model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]
model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]
model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [4096, 4096]
model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [4096, 4096]
model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.30.attn_rot_embd
model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]
model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]
model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]
model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [4096, 4096]
model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [4096, 4096]
model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.31.attn_rot_embd
model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]
model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]
model.norm.weight                                -> output_norm.weight                       | F16    | [4096]
lm_head.weight                                   -> output.weight                            | F16    | [32000, 4096]
Writing ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf, format 1
Ignoring added_tokens.json since model matches vocab size without it.
gguf: This GGUF file is for Little Endian only
gguf: Setting special token type bos to 1
gguf: Setting special token type eos to 2
gguf: Setting special token type pad to 0
gguf: Setting add_bos_token to True
gguf: Setting add_eos_token to False
[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   1
[  2/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1
[  3/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   3
[  4/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   3
[  5/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3
[  6/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   3
[  7/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3
[  8/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3
[  9/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   3
[ 10/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   3
[ 11/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 12/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 13/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 14/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   4
[ 15/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   5
[ 16/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   5
[ 17/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   5
[ 18/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   6
[ 19/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   6
[ 20/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   7
[ 21/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   7
[ 22/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   7
[ 23/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+   7
[ 24/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   7
[ 25/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   7
[ 26/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   7
[ 27/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   7
[ 28/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   7
[ 29/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   8
[ 30/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   8
[ 31/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   8
[ 32/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+   8
[ 33/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   9
[ 34/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  13
[ 35/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  13
[ 36/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  13
[ 37/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  13
[ 38/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  13
[ 39/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  14
[ 40/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  14
[ 41/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+  20
[ 42/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  20
[ 43/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  20
[ 44/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  20
[ 45/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  20
[ 46/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  20
[ 47/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  20
[ 48/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  20
[ 49/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  20
[ 50/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+  21
[ 51/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  22
[ 52/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  22
[ 53/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  22
[ 54/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  22
[ 55/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  22
[ 56/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  22
[ 57/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  22
[ 58/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  22
[ 59/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+  23
[ 60/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  24
[ 61/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  24
[ 62/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  24
[ 63/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  24
[ 64/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  24
[ 65/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  24
[ 66/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  24
[ 67/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  24
[ 68/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+  24
[ 69/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  25
[ 70/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  25
[ 71/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  26
[ 72/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  26
[ 73/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  26
[ 74/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  26
[ 75/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  26
[ 76/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  26
[ 77/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+  28
[ 78/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  28
[ 79/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  28
[ 80/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  28
[ 81/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  28
[ 82/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  28
[ 83/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  28
[ 84/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  28
[ 85/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  28
[ 86/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+  29
[ 87/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  29
[ 88/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  30
[ 89/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  30
[ 90/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  30
[ 91/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  30
[ 92/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  30
[ 93/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  30
[ 94/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  30
[ 95/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+  31
[ 96/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  31
[ 97/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  31
[ 98/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  31
[ 99/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  31
[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  31
[101/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  32
[102/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  32
[103/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  32
[104/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+  32
[105/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  32
[106/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  33
[107/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  33
[108/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  33
[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  33
[110/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  33
[111/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  34
[112/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  34
[113/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+  34
[114/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  34
[115/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  35
[116/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  35
[117/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  35
[118/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  35
[119/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  35
[120/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  37
[121/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  37
[122/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+  37
[123/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  37
[124/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  37
[125/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  37
[126/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  37
[127/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  37
[128/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  37
[129/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  37
[130/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  38
[131/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+  38
[132/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  38
[133/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  39
[134/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  39
[135/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  39
[136/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  39
[137/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  39
[138/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  39
[139/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  39
[140/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+  39
[141/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  40
[142/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  41
[143/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  41
[144/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  41
[145/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  41
[146/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  41
[147/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  41
[148/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  41
[149/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+  42
[150/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  42
[151/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  42
[152/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  43
[153/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  44
[154/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  44
[155/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  44
[156/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  44
[157/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  44
[158/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+  44
[159/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  44
[160/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  44
[161/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  45
[162/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  45
[163/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  45
[164/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  45
[165/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  45
[166/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  45
[167/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+  46
[168/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  46
[169/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  46
[170/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  47
[171/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  47
[172/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  47
[173/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  47
[174/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  47
[175/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  47
[176/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+  47
[177/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  47
[178/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  48
[179/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  48
[180/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  49
[181/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  49
[182/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  49
[183/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  49
[184/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  49
[185/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+  49
[186/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  49
[187/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  50
[188/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  50
[189/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  50
[190/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  50
[191/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  52
[192/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  52
[193/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  52
[194/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+  53
[195/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  53
[196/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  58
[197/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  58
[198/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  58
[199/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  58
[200/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  61
[201/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  61
[202/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  61
[203/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+  61
[204/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  67
[205/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  69
[206/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  69
[207/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  71
[208/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  71
[209/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  71
[210/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  72
[211/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  78
[212/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+  78
[213/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  78
[214/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  78
[215/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  81
[216/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  81
[217/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+  81
[218/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  81
[219/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  81
[220/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  81
[221/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+  81
[222/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  84
[223/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  91
[224/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  92
[225/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+  92
[226/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+  92
[227/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  92
[228/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  92
[229/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  92
[230/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+  92
[231/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  93
[232/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  93
[233/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  93
[234/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+  93
[235/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+  93
[236/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  93
[237/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  93
[238/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  93
[239/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+  93
[240/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  94
[241/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  95
[242/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  95
[243/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+  95
[244/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+  95
[245/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  95
[246/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  95
[247/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  96
[248/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+  96
[249/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  96
[250/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  98
[251/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  98
[252/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+  98
[253/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+ 109
[254/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 109
[255/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 109
[256/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 109
[257/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 109
[258/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 109
[259/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 109
[260/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 109
[261/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+ 109
[262/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+ 109
[263/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 110
[264/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 113
[265/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 113
[266/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 113
[267/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 113
[268/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 113
[269/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 113
[270/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+ 113
[271/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+ 113
[272/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 113
[273/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 113
[274/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 113
[275/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 113
[276/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 115
[277/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 115
[278/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 115
[279/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 115
[280/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 115
[281/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 115
[282/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 115
[283/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 115
[284/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 115
[285/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 116
[286/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 116
[287/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 124
[288/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 124
[289/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 124
[290/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 124
[291/291] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+ 124
Wrote ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf
+ model_f16=../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf
+ model_q4_0=../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf
+ model_q4_1=../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf
+ model_q5_0=../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf
+ model_q5_1=../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf
+ model_q2_k=../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf
+ model_q3_k=../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf
+ model_q4_k=../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf
+ model_q5_k=../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf
+ model_q6_k=../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf
+ wiki_test=../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf q8_0
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q8_0 .. size =   250.00 MiB ->   132.81 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.025 0.016 0.026 0.041 0.060 0.085 0.113 0.267 0.113 0.085 0.060 0.041 0.026 0.016 0.025 
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.024 0.014 0.023 0.037 0.057 0.084 0.116 0.287 0.117 0.084 0.057 0.037 0.023 0.014 0.024 
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.107 0.240 0.107 0.087 0.065 0.046 0.030 0.019 0.027 
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.026 0.017 0.028 0.043 0.063 0.088 0.112 0.246 0.111 0.088 0.063 0.043 0.028 0.017 0.026 
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.028 0.020 0.032 0.049 0.067 0.088 0.105 0.223 0.105 0.088 0.067 0.049 0.032 0.020 0.028 
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.025 0.015 0.025 0.039 0.059 0.086 0.116 0.272 0.116 0.086 0.059 0.039 0.024 0.015 0.025 
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.025 0.016 0.025 0.040 0.060 0.086 0.115 0.265 0.115 0.087 0.060 0.040 0.025 0.016 0.025 
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.233 0.107 0.087 0.066 0.047 0.031 0.019 0.027 
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.025 0.016 0.026 0.040 0.060 0.086 0.115 0.263 0.115 0.086 0.060 0.040 0.026 0.016 0.025 
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.087 0.106 0.231 0.106 0.087 0.066 0.047 0.031 0.020 0.027 
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.047 0.031 0.019 0.027 
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.107 0.228 0.107 0.088 0.066 0.047 0.031 0.019 0.027 
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.229 0.106 0.088 0.066 0.047 0.031 0.020 0.027 
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.229 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.229 0.106 0.088 0.066 0.047 0.031 0.020 0.027 
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.229 0.106 0.087 0.066 0.048 0.031 0.020 0.027 
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.047 0.031 0.020 0.027 
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.087 0.106 0.229 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.229 0.106 0.088 0.066 0.047 0.031 0.020 0.027 
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.087 0.066 0.047 0.031 0.020 0.027 
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.228 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.229 0.106 0.088 0.066 0.047 0.031 0.020 0.027 
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.087 0.066 0.048 0.031 0.020 0.027 
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.227 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.047 0.031 0.020 0.027 
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.047 0.030 0.019 0.027 
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q8_0 .. size =   250.00 MiB ->   132.81 MiB | hist: 0.000 0.027 0.021 0.033 0.050 0.069 0.089 0.105 0.218 0.103 0.087 0.067 0.049 0.033 0.021 0.028 
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  6828.64 MB
llama_model_quantize_internal: hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 

main: quantize time = 46777.81 ms
main:    total time = 46777.81 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf q4_0
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf' as Q4_0
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   250.00 MiB ->    70.31 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.013 0.020 0.032 0.049 0.071 0.098 0.125 0.142 0.125 0.098 0.072 0.049 0.032 0.020 0.017 
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.035 0.011 0.018 0.030 0.046 0.069 0.098 0.132 0.153 0.132 0.098 0.069 0.046 0.029 0.018 0.015 
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.096 0.115 0.126 0.115 0.096 0.075 0.054 0.037 0.024 0.020 
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.013 0.022 0.035 0.052 0.075 0.099 0.120 0.128 0.120 0.099 0.074 0.052 0.035 0.022 0.018 
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.040 0.026 0.021 
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.035 0.012 0.019 0.031 0.047 0.071 0.099 0.128 0.143 0.129 0.100 0.071 0.048 0.031 0.019 0.016 
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.035 0.012 0.020 0.032 0.049 0.072 0.100 0.126 0.139 0.126 0.099 0.072 0.049 0.032 0.020 0.017 
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.013 0.020 0.032 0.049 0.072 0.099 0.126 0.138 0.126 0.099 0.072 0.049 0.032 0.020 0.017 
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.113 0.120 0.113 0.096 0.076 0.056 0.038 0.025 0.020 
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.113 0.096 0.076 0.056 0.038 0.025 0.021 
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.097 0.076 0.056 0.039 0.025 0.021 
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  3647.87 MB
llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 

main: quantize time = 26482.41 ms
main:    total time = 26482.41 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf q4_1
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf' as Q4_1
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q4_1 .. size =   250.00 MiB ->    78.12 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.024 0.036 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.083 0.068 0.051 0.036 0.024 0.040 
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.024 0.037 0.053 0.068 0.083 0.094 0.101 0.101 0.094 0.083 0.068 0.053 0.037 0.024 0.040 
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.024 0.036 0.050 0.066 0.083 0.097 0.105 0.105 0.097 0.083 0.066 0.050 0.036 0.024 0.040 
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.024 0.036 0.051 0.067 0.083 0.096 0.103 0.104 0.096 0.083 0.067 0.050 0.036 0.024 0.040 
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.101 0.095 0.082 0.067 0.052 0.037 0.025 0.040 
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.039 0.023 0.035 0.050 0.067 0.083 0.096 0.104 0.104 0.097 0.084 0.067 0.051 0.036 0.023 0.039 
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.024 0.036 0.051 0.067 0.083 0.096 0.103 0.104 0.096 0.083 0.067 0.051 0.036 0.024 0.040 
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.036 0.050 0.066 0.083 0.096 0.104 0.104 0.096 0.083 0.066 0.050 0.036 0.025 0.040 
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.023 0.035 0.050 0.067 0.084 0.097 0.104 0.104 0.097 0.084 0.067 0.050 0.035 0.023 0.040 
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.038 0.052 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.067 0.052 0.038 0.026 0.040 
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.067 0.052 0.037 0.025 0.040 
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.036 0.051 0.067 0.083 0.096 0.103 0.103 0.095 0.083 0.067 0.051 0.036 0.025 0.040 
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.036 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.026 0.037 0.052 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.067 0.052 0.038 0.026 0.040 
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.068 0.052 0.037 0.025 0.040 
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.067 0.052 0.037 0.026 0.040 
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.101 0.094 0.082 0.067 0.052 0.037 0.025 0.040 
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.026 0.037 0.052 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.067 0.052 0.038 0.026 0.040 
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.101 0.094 0.082 0.067 0.052 0.037 0.025 0.040 
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.101 0.094 0.082 0.067 0.052 0.037 0.026 0.040 
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.102 0.094 0.082 0.067 0.052 0.037 0.025 0.040 
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.067 0.052 0.037 0.025 0.040 
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.101 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.102 0.094 0.082 0.067 0.051 0.037 0.025 0.040 
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.101 0.095 0.082 0.067 0.052 0.037 0.025 0.040 
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.094 0.101 0.101 0.095 0.082 0.067 0.052 0.037 0.025 0.040 
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.101 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.036 0.051 0.067 0.083 0.096 0.103 0.103 0.096 0.083 0.067 0.050 0.036 0.025 0.040 
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  4041.68 MB
llama_model_quantize_internal: hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 

main: quantize time = 26886.11 ms
main:    total time = 26886.11 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf q5_0
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf' as Q5_0
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q5_0 .. size =   250.00 MiB ->    85.94 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.038 0.014 0.023 0.036 0.054 0.078 0.105 0.131 0.140 0.119 0.091 0.065 0.044 0.029 0.018 0.013 
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.037 0.013 0.021 0.033 0.051 0.075 0.106 0.140 0.151 0.124 0.090 0.062 0.041 0.026 0.016 0.012 
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.039 0.017 0.027 0.041 0.059 0.080 0.101 0.119 0.125 0.111 0.091 0.069 0.050 0.033 0.021 0.015 
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.039 0.015 0.025 0.039 0.057 0.081 0.105 0.123 0.128 0.115 0.093 0.069 0.047 0.031 0.020 0.014 
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.029 0.044 0.062 0.082 0.100 0.112 0.115 0.107 0.092 0.072 0.052 0.036 0.023 0.016 
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.036 0.023 0.016 
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.038 0.013 0.022 0.034 0.053 0.078 0.107 0.134 0.142 0.122 0.092 0.065 0.043 0.027 0.017 0.012 
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.038 0.014 0.022 0.036 0.054 0.078 0.107 0.131 0.138 0.120 0.092 0.066 0.044 0.028 0.018 0.013 
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.039 0.017 0.027 0.042 0.060 0.081 0.101 0.117 0.121 0.110 0.092 0.070 0.050 0.034 0.022 0.016 
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.038 0.014 0.023 0.036 0.054 0.078 0.106 0.131 0.137 0.120 0.092 0.066 0.044 0.029 0.018 0.013 
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.081 0.101 0.116 0.119 0.109 0.091 0.071 0.051 0.034 0.022 0.016 
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.039 0.017 0.027 0.042 0.061 0.081 0.101 0.116 0.119 0.110 0.092 0.071 0.051 0.034 0.022 0.016 
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.039 0.017 0.028 0.042 0.061 0.082 0.101 0.115 0.119 0.109 0.092 0.071 0.051 0.034 0.022 0.016 
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.034 0.022 0.016 
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.108 0.091 0.071 0.051 0.035 0.022 0.016 
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.081 0.101 0.115 0.119 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.109 0.091 0.071 0.051 0.035 0.022 0.016 
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.081 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.081 0.101 0.115 0.119 0.109 0.091 0.071 0.052 0.035 0.022 0.016 
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.023 0.016 
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.081 0.101 0.115 0.118 0.108 0.092 0.071 0.051 0.035 0.022 0.016 
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.081 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.081 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.081 0.101 0.115 0.118 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.051 0.035 0.022 0.016 
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.072 0.052 0.035 0.022 0.016 
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.081 0.101 0.115 0.119 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.081 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.108 0.092 0.071 0.051 0.035 0.022 0.016 
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.100 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.100 0.114 0.118 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.072 0.052 0.035 0.022 0.016 
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.039 0.017 0.027 0.042 0.060 0.081 0.102 0.117 0.120 0.111 0.092 0.071 0.050 0.034 0.022 0.016 
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  4435.49 MB
llama_model_quantize_internal: hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 

main: quantize time = 31143.55 ms
main:    total time = 31143.55 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf q5_1
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf' as Q5_1
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q5_1 .. size =   250.00 MiB ->    93.75 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.044 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.044 
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.044 0.026 0.039 0.053 0.068 0.081 0.092 0.097 0.098 0.092 0.081 0.068 0.053 0.039 0.026 0.044 
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.037 0.050 0.066 0.081 0.094 0.102 0.102 0.094 0.081 0.066 0.050 0.037 0.025 0.045 
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.044 0.025 0.037 0.051 0.067 0.082 0.094 0.100 0.100 0.094 0.082 0.067 0.051 0.037 0.025 0.044 
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.044 0.025 0.037 0.051 0.067 0.082 0.094 0.100 0.101 0.094 0.082 0.067 0.051 0.037 0.025 0.044 
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.044 0.025 0.037 0.051 0.067 0.082 0.094 0.100 0.100 0.094 0.082 0.067 0.051 0.037 0.025 0.044 
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.037 0.051 0.066 0.081 0.093 0.100 0.101 0.093 0.081 0.066 0.051 0.037 0.026 0.045 
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.044 0.025 0.036 0.051 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.067 0.051 0.036 0.025 0.044 
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.039 0.052 0.067 0.081 0.092 0.097 0.098 0.091 0.081 0.067 0.052 0.039 0.027 0.045 
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.051 0.066 0.081 0.093 0.100 0.100 0.093 0.081 0.066 0.051 0.038 0.026 0.045 
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.039 0.027 0.045 
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.066 0.081 0.093 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.039 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.051 0.038 0.026 0.045 
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.051 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.051 0.038 0.026 0.045 
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.051 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.066 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.066 0.052 0.038 0.026 0.045 
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.051 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.051 0.038 0.026 0.045 
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.066 0.051 0.038 0.026 0.045 
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.051 0.066 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.093 0.081 0.067 0.051 0.038 0.026 0.045 
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.051 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.066 0.052 0.038 0.026 0.045 
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.051 0.038 0.026 0.045 
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.066 0.052 0.038 0.026 0.045 
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.051 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.051 0.038 0.026 0.045 
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.066 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.037 0.051 0.066 0.081 0.093 0.100 0.100 0.093 0.081 0.066 0.051 0.037 0.026 0.045 
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  4829.30 MB
llama_model_quantize_internal: hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 

main: quantize time = 33381.37 ms
main:    total time = 33381.37 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf q2_k
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf' as Q2_K
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q2_K .. size =   250.00 MiB ->    41.02 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  2414.82 MB

main: quantize time = 71390.42 ms
main:    total time = 71390.42 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf q3_k
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf' as Q3_K
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q3_K .. size =   250.00 MiB ->    53.71 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  3144.52 MB

main: quantize time = 67862.08 ms
main:    total time = 67862.08 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf q4_k
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf' as Q4_K
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q4_K .. size =   250.00 MiB ->    70.31 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  3891.24 MB

main: quantize time = 122251.63 ms
main:    total time = 122251.63 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf q5_k
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf' as Q5_K
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q5_K .. size =   250.00 MiB ->    85.94 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  4560.87 MB

main: quantize time = 105842.24 ms
main:    total time = 105842.24 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf q6_k
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf' as Q6_K
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  5272.34 MB

main: quantize time = 59907.03 ms
main:    total time = 59907.03 ms
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-f16.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   250.00 MiB
llm_load_tensors:      CUDA0 buffer size = 12603.02 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =     9.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    77.55 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 0


 I believe the meaning of life is to be happy and that there are many different ways in which we can achieve this.
I think it’s important not just to focus on yourself but also others as well, helping each other out wherever possible without expecting anything back from people for what they have done or helped you with (unless requested). This way everyone benefits equally! What would life be like if nobody did anything nice? We would all live in a world where only one person feels good about themselves at any given time.
My main aim is to help others as much possible while also making sure that my own well-being does not suffer too badly – sometimes it’s hard not feeling selfish when trying so desperately for other people but still having enough energy left over after everything else has been dealt with (or maybe even before). If everyone had this attitude, then surely no one would ever feel alone or neglected again?
I think I am a good person because of the way that my family raised me. They taught me about respect and kindness towards others without expecting anything in return; not only that but they also made sure we were always doing something fun together such as going on trips away with friends/family members where there would be plenty more opportunities for interacting with strangers too!
I think it’s
llama_print_timings:        load time =    2341.62 ms
llama_print_timings:      sample time =     116.38 ms /   256 runs   (    0.45 ms per token,  2199.77 tokens per second)
llama_print_timings: prompt eval time =      24.85 ms /     8 tokens (    3.11 ms per token,   321.91 tokens per second)
llama_print_timings:        eval time =    4904.13 ms /   255 runs   (   19.23 ms per token,    52.00 tokens per second)
llama_print_timings:       total time =    5131.91 ms /   263 tokens
Log end

real	0m8.084s
user	0m6.851s
sys	0m1.233s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q8_0.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q8_0:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   132.81 MiB
llm_load_tensors:      CUDA0 buffer size =  6695.84 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =     9.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    77.55 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 0


 I believe the meaning of life is to be happy and that there are many different ways in which we can achieve this.
I think it’s important not just to focus on yourself but also your friends, family or anyone who surround you as they all play an essential role at achieving a successful long-term happiness; afterall “it takes a village”! I believe the secret ingredient is having lots of good times and being surrounded by positive people.
I’ve created this page to inspire others in their own search for inner peace, health & wellbeing through my journey on life as well as help you all achieve it too with some fantastic tools that can be applied straight away into your everyday lives! So whether its nutrition, fitness or mental health – I hope the information within will give you a little more insight and hopefully provide you something to make 2019 even better than this year has been so far.
Happy Holidays & see you in the New Year 🙂 xo KJL xx
Nutrition Tips: The importance of hydration is essential for good health; it also helps reduce cravings, improves digestion and gives a boost to your energy levels! Try keeping water close by at all times during the day (like my personal favourite – Hydroflask bottle), as
llama_print_timings:        load time =    1550.48 ms
llama_print_timings:      sample time =     118.46 ms /   256 runs   (    0.46 ms per token,  2161.10 tokens per second)
llama_print_timings: prompt eval time =      25.37 ms /     8 tokens (    3.17 ms per token,   315.35 tokens per second)
llama_print_timings:        eval time =    3179.37 ms /   255 runs   (   12.47 ms per token,    80.20 tokens per second)
llama_print_timings:       total time =    3410.16 ms /   263 tokens
Log end

real	0m5.557s
user	0m4.374s
sys	0m1.181s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_0.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =     9.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    77.55 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 0


 I believe the meaning of life is to give, share and create.
We are all here for a purpose; we just need to find out what that will be! And once you have found it - Go For It!!! Be passionate about whatever your heart tells you...the possibilities then become endless!! What can YOU do? How can You make the world around us better, more loving and happier???
I am also interested in learning how we all interact. Why don't we talk to each other anymore!?? I know it is because of technology - but that doesn't mean anything has been lost with face-to-face communication....what do you think? How did our grandparents communicate so well...how can WE learn this again???
I believe in a world where people don't judge others by their looks or background, they just appreciate the heart! I also love to see that we are all equal - rich and poor alike. We have more similarities than differences!! Let us try harder at making our communities better places for everyone...for generations yet unborn!!!
What do YOU think??? Please leave a comment on this blog post!!!! :) Thanks in advance....we can learn from each other, so please share your thoughts! I really look forward to reading them all. Blessings and hugs
llama_print_timings:        load time =     864.71 ms
llama_print_timings:      sample time =     124.81 ms /   256 runs   (    0.49 ms per token,  2051.08 tokens per second)
llama_print_timings: prompt eval time =      20.44 ms /     8 tokens (    2.56 ms per token,   391.31 tokens per second)
llama_print_timings:        eval time =    2220.37 ms /   255 runs   (    8.71 ms per token,   114.85 tokens per second)
llama_print_timings:       total time =    2452.19 ms /   263 tokens
Log end

real	0m3.939s
user	0m3.018s
sys	0m0.917s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_1.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 3
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_1:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_1
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.95 GiB (5.03 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    78.12 MiB
llm_load_tensors:      CUDA0 buffer size =  3963.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =     9.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    77.55 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 0


 I believe the meaning of life is to enjoy each moment as much as possible, and that’s why we decided not only make a movie about this very concept but also live it.
In our opinion every person deserves at least one chance in their lifetime to do things they can´t otherwise imagine doing: jump from an airplane, ride on the back of a motorcycle or even skydive for 18 seconds… We are so convinced that you will love these moments once you take part as well.
We think life is meant to be lived and we want to help people live it in such way by giving them all opportunities they need through our company: Krytyka Films Production, or simply put – The Red Bicycle Studio. We believe the world needs a new generation of filmmakers who are willing not only tell stories but also bring emotions into their work with passion and dedication to make every second count during production process as well!
We have been working in this industry for many years now; some would say we’ve learned all there is about filming or producing videos, others may disagree. But one thing is certain: people are our top priority – always will be because it’s the most important part of any project after having decided which topic should be covered by camera lenses in order not
llama_print_timings:        load time =     946.32 ms
llama_print_timings:      sample time =     116.27 ms /   256 runs   (    0.45 ms per token,  2201.83 tokens per second)
llama_print_timings: prompt eval time =      19.43 ms /     8 tokens (    2.43 ms per token,   411.69 tokens per second)
llama_print_timings:        eval time =    2348.34 ms /   255 runs   (    9.21 ms per token,   108.59 tokens per second)
llama_print_timings:       total time =    2568.85 ms /   263 tokens
Log end

real	0m4.104s
user	0m3.199s
sys	0m0.902s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_0.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 8
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q5_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q5_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 4.33 GiB (5.52 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    85.94 MiB
llm_load_tensors:      CUDA0 buffer size =  4349.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =     9.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    77.55 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 0


 I believe the meaning of life is to find and express our passions.
When we are passionate about something, it becomes a part of us like nothing else. It gives us energy and focus in all things that come into contact with those subjects or activities so passionately held as special by ourselves; from family relationships, friendships, work environments, sports teams/leagues to hobbies and more.
Passion is contagious when it’s shared! This explains why passionate people make such great leaders – their leadership comes out of a deep belief in the mission or cause they are helping others get behind themselves (and vice-versa). They believe so deeply that if you have passion, anything can be accomplished; even things beyond what most would consider possible.
And while I’m not saying this is always easy to find and keep doing for long periods of time in our lives – it certainly feels better when we do! It makes us feel alive again like nothing else will ever make us feel because those activities are so deeply held within ourselves that they define who/what “we” really are.
When I was a little girl, my dad had an old saying he liked to share with me: ‘If you have passion in your heart for something then it can be done.’ So much
llama_print_timings:        load time =    1026.87 ms
llama_print_timings:      sample time =     120.73 ms /   256 runs   (    0.47 ms per token,  2120.49 tokens per second)
llama_print_timings: prompt eval time =      32.27 ms /     8 tokens (    4.03 ms per token,   247.94 tokens per second)
llama_print_timings:        eval time =    2511.69 ms /   255 runs   (    9.85 ms per token,   101.53 tokens per second)
llama_print_timings:       total time =    2753.15 ms /   263 tokens
Log end

real	0m4.379s
user	0m3.439s
sys	0m0.938s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_1.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 9
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q5_1:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q5_1
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 4.72 GiB (6.01 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    93.75 MiB
llm_load_tensors:      CUDA0 buffer size =  4735.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =     9.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    77.55 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 0


 I believe the meaning of life is to be happy and have a good time, but that doesn’t mean you don’t work hard.
I know everyone will say this one…but it really does sound like something from an 80s movie..it kind makes sense though! We should all just get along with each other because we are human beings on the same planet as our fellow man, right? I think that is a little naïve and very unrealistic.
I guess if you put it in perspective of what happens around us everyday…you do see some people doing their best to be happy! But for others not so much..or maybe they are just plain miserable because life sucks or something like that, but I’m gonna try my hardest every day and live the good life.
And if you believe it too then please share this with your friends on Facebook!!
Happy Friday everyone…and have a great weekend! 🙂 ❤️🌴⛱🌊⛺💃
~Bree
Advertisements
One thought on “I Believe The Meaning of Life Is To Be Happy and Have A Good Time.”
Love this!! I am sharing it with my friends. Thank you so much for taking
llama_print_timings:        load time =    1126.83 ms
llama_print_timings:      sample time =     122.92 ms /   256 runs   (    0.48 ms per token,  2082.57 tokens per second)
llama_print_timings: prompt eval time =      26.10 ms /     8 tokens (    3.26 ms per token,   306.47 tokens per second)
llama_print_timings:        eval time =    2570.30 ms /   255 runs   (   10.08 ms per token,    99.21 tokens per second)
llama_print_timings:       total time =    2804.56 ms /   263 tokens
Log end

real	0m4.531s
user	0m3.530s
sys	0m0.999s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q2_k.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 10
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q2_K:  129 tensors
llama_model_loader: - type q3_K:   96 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q2_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 2.36 GiB (3.01 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    41.02 MiB
llm_load_tensors:      CUDA0 buffer size =  2373.81 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =     9.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    77.55 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 0


 I believe the meaning of life is to enjoy every single thing and moment that you have.
The more money we make, or how much fame a person receives does not matter at all; what matters most are moments in our lives which will be with us forever! All these good times become memories for people like me who was born on 19th of May , the day to remember is my birthday as it happens every year.
I love food, reading and learning new things about life's secrets that everyone has a right to know; I want all this knowledge from each person or thing we see in our lives whether its good for us bad! If you ask me what was special on the day of May 19th , my birthday is always going be like how every year it happens.
I love food, reading and learning new things about life's secrets that everyone has a right to know; I want all this knowledge from each person or thing we see in our lives whether its good for us bad! If you ask me what was special on the day of May 19th , my birthday is always going be like how every year it happens.
My favorite memories are that same time as when everyone else's birthdays happen, and I really want to know more about
llama_print_timings:        load time =     598.29 ms
llama_print_timings:      sample time =     120.48 ms /   256 runs   (    0.47 ms per token,  2124.76 tokens per second)
llama_print_timings: prompt eval time =      32.53 ms /     8 tokens (    4.07 ms per token,   245.90 tokens per second)
llama_print_timings:        eval time =    2571.29 ms /   255 runs   (   10.08 ms per token,    99.17 tokens per second)
llama_print_timings:       total time =    2808.71 ms /   263 tokens
Log end

real	0m4.005s
user	0m3.252s
sys	0m0.753s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q3_k.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 12
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q3_K:  129 tensors
llama_model_loader: - type q4_K:   92 tensors
llama_model_loader: - type q5_K:    4 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q3_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.07 GiB (3.91 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    53.71 MiB
llm_load_tensors:      CUDA0 buffer size =  3090.81 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =     9.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    77.55 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 0


 I believe the meaning of life is to find a way and mean for living, not just surviving.
I don’t know if you have ever felt this pain before but it was one that would never let go… The kind where every day feels like 264 hours long with no break in sight until your body gives up on itself from exhaustion & the only way to make yourself feel better is to get high.
I remember coming home crying after seeing my friends at school who had a happy life, and I was always thinking about how much happier their lives were compared to mine where there wasn’t any peace of mind or hope for the future because you just couldn’t see past what happened in your teenage years… It is so hard not knowing when it would all end.
I remember being on a date with my then boyfriend & looking at him and thinking “how can I love someone that isn’t there anymore?” And even though he was gone, his presence still lingered around me because of the memories we shared together but without them in your life things just aren’t the same…
I remember being on a date with my then boyfriend & looking at him and thinking “how can I love someone that isn’t there anymore?” And even though he was gone, his
llama_print_timings:        load time =     766.25 ms
llama_print_timings:      sample time =     118.04 ms /   256 runs   (    0.46 ms per token,  2168.68 tokens per second)
llama_print_timings: prompt eval time =      38.75 ms /     8 tokens (    4.84 ms per token,   206.44 tokens per second)
llama_print_timings:        eval time =    2899.86 ms /   255 runs   (   11.37 ms per token,    87.94 tokens per second)
llama_print_timings:       total time =    3141.35 ms /   263 tokens
Log end

real	0m4.516s
user	0m3.691s
sys	0m0.824s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_k.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3820.94 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =     9.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    77.55 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 0


 I believe the meaning of life is to find something you’re passionate about and then do it as long as possible.
—Muhammad Ali (1942-2016) American boxer, philanthropist 👑
When people ask me how my day was today or what I did lately the response that most often comes to mind is: “I didn’t do anything.” And it’s true. As a stay-at-home mom of two little kids (and one on the way), there just aren’t a lot of moments when we can say, without any sort of exaggeration or hyperbole that I accomplished something great today!
As much as those days get old and tiring after awhile it also gets harder to keep myself motivated. It’s not like my husband isn’t working hard every day at his job either; but somehow he manages to find time for hobbies, sports, interests outside of our family life that give him a sense of accomplishment in the formative years we are raising these kids and preparing them for their own lives ahead!
It may sound selfish or even egotistical when I say it out loud—but honestly sometimes there is nothing more rewarding than knowing my husband comes home after working 
llama_print_timings:        load time =     924.72 ms
llama_print_timings:      sample time =     127.71 ms /   256 runs   (    0.50 ms per token,  2004.57 tokens per second)
llama_print_timings: prompt eval time =      22.36 ms /     8 tokens (    2.80 ms per token,   357.72 tokens per second)
llama_print_timings:        eval time =    2357.37 ms /   255 runs   (    9.24 ms per token,   108.17 tokens per second)
llama_print_timings:       total time =    2592.01 ms /   263 tokens
Log end

real	0m4.104s
user	0m3.221s
sys	0m0.880s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_k.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 17
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q5_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q5_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    85.94 MiB
llm_load_tensors:      CUDA0 buffer size =  4474.94 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =     9.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    77.55 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 0


 I believe the meaning of life is to find and be loved, by someone or something.
I am a writer because it’s my passion in this world for whatever reason that may have led me here today with thoughts on love being one such reason but not even close as why exactly I write; at least what brings out those words from the depths of myself when others can only wonder how come they are written so beautifully.
I am a writer because it is an artform to my soul, and if anyone should be lucky enough to read these thoughts then perhaps there will also find something here that sparks inspiration for their own creative ventures? I have no illusions about what kind of success this might lead me towards but neither do I care.
I am a writer because it is who i’ve always been, and though others may think differently to why they are too or not at all…that doesn’t matter either as the important part for them will be whether their own words find expression in whatever form that takes; this world has enough of us taking ourselves far too seriously already so I see no need nor desire myself.
I am a writer because it is who i’ll always be, and if someone should read my thoughts on love then perhaps they might also feel as though some part
llama_print_timings:        load time =    1069.52 ms
llama_print_timings:      sample time =     116.88 ms /   256 runs   (    0.46 ms per token,  2190.37 tokens per second)
llama_print_timings: prompt eval time =      26.45 ms /     8 tokens (    3.31 ms per token,   302.51 tokens per second)
llama_print_timings:        eval time =    2594.18 ms /   255 runs   (   10.17 ms per token,    98.30 tokens per second)
llama_print_timings:       total time =    2828.13 ms /   263 tokens
Log end

real	0m4.488s
user	0m3.512s
sys	0m0.968s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q6_k.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 18
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q6_K:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q6_K
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 5.15 GiB (6.56 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   102.54 MiB
llm_load_tensors:      CUDA0 buffer size =  5169.81 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =     9.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    77.55 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 0


 I believe the meaning of life is to be happy and if you are not doing what makes your heart sing then it’s time for a change.
If at first, no one will hire me or say yes; don’t give up because something better can happen right around that next corner! If there seems like nothing in my path but dead ends…I won’t stop until I get to the end of them all and find what it is I want/need.
Lately, so many things have been coming together for me including this blog which has given new purpose as well as direction at a time when everything feels chaotic! It gives us something we can control over our own lives rather than feeling powerless about the world around you…and that’s why I love it.
You might be wondering what is so great and different here? Why should this one matter more to me then all of those other ones out there who try but fail at making money online because they don’t know how or where their audience lies when creating content for them instead! So let me show you why:
This blog has been created with passion, purposefulness and the goal in mind. It is meant as an outlet to share my thoughts on life topics such as fashion, beauty products (and tips
llama_print_timings:        load time =    1293.42 ms
llama_print_timings:      sample time =     116.92 ms /   256 runs   (    0.46 ms per token,  2189.59 tokens per second)
llama_print_timings: prompt eval time =      28.51 ms /     8 tokens (    3.56 ms per token,   280.63 tokens per second)
llama_print_timings:        eval time =    2867.54 ms /   255 runs   (   11.25 ms per token,    88.93 tokens per second)
llama_print_timings:       total time =    3099.89 ms /   263 tokens
Log end

real	0m4.992s
user	0m3.933s
sys	0m1.056s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-f16.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1707577236
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   250.00 MiB
llm_load_tensors:      CUDA0 buffer size = 12603.02 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    12.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   171.60 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1036.45 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.72 seconds per pass - ETA 0.03 minutes
[1]5.5327,[2]6.8323,[3]7.4355,[4]6.8783,
llama_print_timings:        load time =    2329.35 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    2455.49 ms /  8192 tokens (    0.30 ms per token,  3336.20 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    3799.91 ms /  8193 tokens

Final estimate: PPL = 6.8783 +/- 0.25580

real	0m6.732s
user	0m5.825s
sys	0m1.533s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q8_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1707577243
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q8_0:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   132.81 MiB
llm_load_tensors:      CUDA0 buffer size =  6695.84 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    12.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   171.60 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1027.02 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.82 seconds per pass - ETA 0.05 minutes
[1]5.5319,[2]6.8326,[3]7.4388,[4]6.8806,
llama_print_timings:        load time =    1274.76 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    2851.90 ms /  8192 tokens (    0.35 ms per token,  2872.47 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4189.69 ms /  8193 tokens

Final estimate: PPL = 6.8806 +/- 0.25603

real	0m6.063s
user	0m5.393s
sys	0m1.300s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1707577249
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    12.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   171.60 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1026.19 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.93 seconds per pass - ETA 0.05 minutes
[1]5.6359,[2]6.9332,[3]7.5732,[4]7.0138,
llama_print_timings:        load time =     721.04 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3293.83 ms /  8192 tokens (    0.40 ms per token,  2487.07 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4629.11 ms /  8193 tokens

Final estimate: PPL = 7.0138 +/- 0.26116

real	0m5.970s
user	0m5.371s
sys	0m1.251s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_1.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1707577255
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 3
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_1:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_1
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.95 GiB (5.03 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    78.12 MiB
llm_load_tensors:      CUDA0 buffer size =  3963.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    12.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   171.60 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1005.53 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.92 seconds per pass - ETA 0.05 minutes
[1]5.6920,[2]6.9505,[3]7.5628,[4]7.0158,
llama_print_timings:        load time =     790.45 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3291.25 ms /  8192 tokens (    0.40 ms per token,  2489.02 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4607.75 ms /  8193 tokens

Final estimate: PPL = 7.0158 +/- 0.26045

real	0m6.020s
user	0m5.393s
sys	0m1.269s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1707577261
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 8
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q5_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q5_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 4.33 GiB (5.52 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    85.94 MiB
llm_load_tensors:      CUDA0 buffer size =  4349.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    12.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   171.60 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1010.97 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.82 seconds per pass - ETA 0.05 minutes
[1]5.5344,[2]6.8544,[3]7.4728,[4]6.9230,
llama_print_timings:        load time =     846.30 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    2852.59 ms /  8192 tokens (    0.35 ms per token,  2871.78 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4183.27 ms /  8193 tokens

Final estimate: PPL = 6.9230 +/- 0.25774

real	0m5.627s
user	0m5.036s
sys	0m1.248s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_1.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1707577267
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 9
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q5_1:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q5_1
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 4.72 GiB (6.01 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    93.75 MiB
llm_load_tensors:      CUDA0 buffer size =  4735.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    12.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   171.60 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1025.64 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.82 seconds per pass - ETA 0.05 minutes
[1]5.5648,[2]6.8468,[3]7.4553,[4]6.9070,
llama_print_timings:        load time =     922.82 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    2851.04 ms /  8192 tokens (    0.35 ms per token,  2873.34 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4189.18 ms /  8193 tokens

Final estimate: PPL = 6.9070 +/- 0.25705

real	0m5.711s
user	0m5.085s
sys	0m1.264s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q2_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1707577272
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 10
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q2_K:  129 tensors
llama_model_loader: - type q3_K:   96 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q2_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 2.36 GiB (3.01 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    41.02 MiB
llm_load_tensors:      CUDA0 buffer size =  2373.81 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    12.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   171.60 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1017.19 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.89 seconds per pass - ETA 0.05 minutes
[1]6.5021,[2]7.9561,[3]8.6949,[4]8.2070,
llama_print_timings:        load time =     509.02 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3150.45 ms /  8192 tokens (    0.38 ms per token,  2600.26 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4478.41 ms /  8193 tokens

Final estimate: PPL = 8.2070 +/- 0.30935

real	0m5.590s
user	0m5.091s
sys	0m1.136s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q3_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1707577278
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 12
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q3_K:  129 tensors
llama_model_loader: - type q4_K:   92 tensors
llama_model_loader: - type q5_K:    4 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q3_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.07 GiB (3.91 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    53.71 MiB
llm_load_tensors:      CUDA0 buffer size =  3090.81 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    12.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   171.60 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1038.18 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.92 seconds per pass - ETA 0.05 minutes
[1]5.6684,[2]7.0210,[3]7.6408,[4]7.1090,
llama_print_timings:        load time =     630.24 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3275.86 ms /  8192 tokens (    0.40 ms per token,  2500.72 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4621.74 ms /  8193 tokens

Final estimate: PPL = 7.1090 +/- 0.26581

real	0m5.850s
user	0m5.242s
sys	0m1.245s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1707577284
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3820.94 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    12.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   171.60 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1039.58 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.92 seconds per pass - ETA 0.05 minutes
[1]5.5834,[2]6.8660,[3]7.4748,[4]6.9337,
llama_print_timings:        load time =     763.09 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3260.59 ms /  8192 tokens (    0.40 ms per token,  2512.43 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4610.86 ms /  8193 tokens

Final estimate: PPL = 6.9337 +/- 0.25755

real	0m5.997s
user	0m5.296s
sys	0m1.339s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1707577290
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 17
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q5_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q5_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    85.94 MiB
llm_load_tensors:      CUDA0 buffer size =  4474.94 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    12.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   171.60 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1026.09 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.87 seconds per pass - ETA 0.05 minutes
[1]5.5164,[2]6.8153,[3]7.4342,[4]6.8905,
llama_print_timings:        load time =     872.75 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3088.01 ms /  8192 tokens (    0.38 ms per token,  2652.84 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4421.51 ms /  8193 tokens

Final estimate: PPL = 6.8905 +/- 0.25642

real	0m5.894s
user	0m5.253s
sys	0m1.278s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q6_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1707577296
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 18
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q6_K:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q6_K
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 5.15 GiB (6.56 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   102.54 MiB
llm_load_tensors:      CUDA0 buffer size =  5169.81 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    12.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   171.60 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1023.66 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.89 seconds per pass - ETA 0.05 minutes
[1]5.5585,[2]6.8409,[3]7.4450,[4]6.8939,
llama_print_timings:        load time =    1017.17 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3117.28 ms /  8192 tokens (    0.38 ms per token,  2627.93 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4448.84 ms /  8193 tokens

Final estimate: PPL = 6.8939 +/- 0.25650

real	0m6.072s
user	0m5.327s
sys	0m1.382s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-imatrix.log
+ ./bin/imatrix --model ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1707577302
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   250.00 MiB
llm_load_tensors:      CUDA0 buffer size = 12603.02 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    12.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   171.60 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
compute_imatrix: tokenizing the input ..
compute_imatrix: tokenization took 1027.34 ms
compute_imatrix: computing over 4 chunks with batch_size 512
compute_imatrix: 3.21 seconds per pass - ETA 0.20 minutes
[1]5.5314,[2]6.8315,
save_imatrix: stored collected data after 10 chunks in imatrix.dat
[3]7.4349,[4]6.8779,
save_imatrix: stored collected data after 16 chunks in imatrix.dat

llama_print_timings:        load time =    4073.05 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   11837.83 ms /  8192 tokens (    1.45 ms per token,   692.02 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   15990.45 ms /  8193 tokens

Final estimate: PPL = 6.8779 +/- 0.25577

real	0m16.612s
user	0m15.500s
sys	0m1.739s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-save-load-state.log
+ ./bin/save-load-state --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.11 MiB
llm_load_tensors: offloading 0 repeating layers to GPU
llm_load_tensors: offloaded 0/33 layers to GPU
llm_load_tensors:        CPU buffer size =  3647.87 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =     9.01 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    77.55 MiB
llama_new_context_with_model: graph splits (measure): 1
main : serialized state into 2762342 out of a maximum of 334037028 bytes
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =     9.01 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    77.55 MiB
llama_new_context_with_model: graph splits (measure): 1
main : deserialized state from 2762342 out of a maximum of 334037028 bytes

main : success

first run: The quick brown fox jumps over the lazy dog.
Today’s (Tuesday, 


second run: The quick brown fox jumps over the lazy dog.
Today’s (Tuesday, 

real	0m10.256s
user	0m26.033s
sys	0m1.325s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-f16.log
++ grep '^\[1\]'
+ check_ppl f16 '[1]5.5327,[2]6.8323,[3]7.4355,[4]6.8783,'
+ qnt=f16
++ echo '[1]5.5327,[2]6.8323,[3]7.4355,[4]6.8783,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.8783
++ echo '6.8783 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' f16 6.8783
+ return 0
  - f16 @ 6.8783 OK
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q8_0.log
++ grep '^\[1\]'
+ check_ppl q8_0 '[1]5.5319,[2]6.8326,[3]7.4388,[4]6.8806,'
+ qnt=q8_0
++ echo '[1]5.5319,[2]6.8326,[3]7.4388,[4]6.8806,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.8806
++ echo '6.8806 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q8_0 6.8806
+ return 0
  - q8_0 @ 6.8806 OK
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_0.log
++ grep '^\[1\]'
+ check_ppl q4_0 '[1]5.6359,[2]6.9332,[3]7.5732,[4]7.0138,'
+ qnt=q4_0
++ echo '[1]5.6359,[2]6.9332,[3]7.5732,[4]7.0138,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=7.0138
++ echo '7.0138 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_0 7.0138
+ return 0
  - q4_0 @ 7.0138 OK
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_1.log
++ grep '^\[1\]'
+ check_ppl q4_1 '[1]5.6920,[2]6.9505,[3]7.5628,[4]7.0158,'
+ qnt=q4_1
++ echo '[1]5.6920,[2]6.9505,[3]7.5628,[4]7.0158,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=7.0158
++ echo '7.0158 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_1 7.0158
+ return 0
  - q4_1 @ 7.0158 OK
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_0.log
++ grep '^\[1\]'
+ check_ppl q5_0 '[1]5.5344,[2]6.8544,[3]7.4728,[4]6.9230,'
+ qnt=q5_0
++ echo '[1]5.5344,[2]6.8544,[3]7.4728,[4]6.9230,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.9230
++ echo '6.9230 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_0 6.9230
+ return 0
  - q5_0 @ 6.9230 OK
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_1.log
++ grep '^\[1\]'
+ check_ppl q5_1 '[1]5.5648,[2]6.8468,[3]7.4553,[4]6.9070,'
+ qnt=q5_1
++ echo '[1]5.5648,[2]6.8468,[3]7.4553,[4]6.9070,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.9070
++ echo '6.9070 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_1 6.9070
+ return 0
  - q5_1 @ 6.9070 OK
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q2_k.log
++ grep '^\[1\]'
+ check_ppl q2_k '[1]6.5021,[2]7.9561,[3]8.6949,[4]8.2070,'
+ qnt=q2_k
++ echo '[1]6.5021,[2]7.9561,[3]8.6949,[4]8.2070,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=8.2070
++ echo '8.2070 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q2_k 8.2070
+ return 0
  - q2_k @ 8.2070 OK
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q3_k.log
++ grep '^\[1\]'
+ check_ppl q3_k '[1]5.6684,[2]7.0210,[3]7.6408,[4]7.1090,'
+ qnt=q3_k
++ echo '[1]5.6684,[2]7.0210,[3]7.6408,[4]7.1090,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=7.1090
++ echo '7.1090 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q3_k 7.1090
+ return 0
  - q3_k @ 7.1090 OK
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_k.log
++ grep '^\[1\]'
+ check_ppl q4_k '[1]5.5834,[2]6.8660,[3]7.4748,[4]6.9337,'
+ qnt=q4_k
++ echo '[1]5.5834,[2]6.8660,[3]7.4748,[4]6.9337,'
++ tail -n 1
++ grep -oE '[0-9]+\.[0-9]+'
+ ppl=6.9337
++ echo '6.9337 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_k 6.9337
+ return 0
  - q4_k @ 6.9337 OK
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_k.log
++ grep '^\[1\]'
+ check_ppl q5_k '[1]5.5164,[2]6.8153,[3]7.4342,[4]6.8905,'
+ qnt=q5_k
++ echo '[1]5.5164,[2]6.8153,[3]7.4342,[4]6.8905,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.8905
++ echo '6.8905 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_k 6.8905
+ return 0
  - q5_k @ 6.8905 OK
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q6_k.log
++ grep '^\[1\]'
+ check_ppl q6_k '[1]5.5585,[2]6.8409,[3]7.4450,[4]6.8939,'
+ qnt=q6_k
++ echo '[1]5.5585,[2]6.8409,[3]7.4450,[4]6.8939,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.8939
++ echo '6.8939 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q6_k 6.8939
+ return 0
  - q6_k @ 6.8939 OK
+ cat /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-imatrix.log
+ grep Final
+ path_lora=../models-mnt/open-llama/7B-v2/lora
+ path_shakespeare=../models-mnt/shakespeare
+ shakespeare=../models-mnt/shakespeare/shakespeare.txt
+ lora_shakespeare=../models-mnt/open-llama/7B-v2/lora/ggml-adapter-model.bin
+ gg_wget ../models-mnt/open-llama/7B-v2/lora https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_config.json
+ local out=../models-mnt/open-llama/7B-v2/lora
+ local url=https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp/build-ci-release
+ mkdir -p ../models-mnt/open-llama/7B-v2/lora
+ cd ../models-mnt/open-llama/7B-v2/lora
+ wget -nv -N https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_config.json
Last-modified header missing -- time-stamps turned off.
2024-02-10 15:02:09 URL:https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_config.json [457/457] -> "adapter_config.json" [1]
+ cd /home/ggml/work/llama.cpp/build-ci-release
+ gg_wget ../models-mnt/open-llama/7B-v2/lora https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_model.bin
+ local out=../models-mnt/open-llama/7B-v2/lora
+ local url=https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_model.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp/build-ci-release
+ mkdir -p ../models-mnt/open-llama/7B-v2/lora
+ cd ../models-mnt/open-llama/7B-v2/lora
+ wget -nv -N https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_model.bin
+ cd /home/ggml/work/llama.cpp/build-ci-release
+ gg_wget ../models-mnt/shakespeare https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/shakespeare.txt
+ local out=../models-mnt/shakespeare
+ local url=https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/shakespeare.txt
++ pwd
+ local cwd=/home/ggml/work/llama.cpp/build-ci-release
+ mkdir -p ../models-mnt/shakespeare
+ cd ../models-mnt/shakespeare
+ wget -nv -N https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/shakespeare.txt
Last-modified header missing -- time-stamps turned off.
2024-02-10 15:02:10 URL:https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/shakespeare.txt [94275/94275] -> "shakespeare.txt" [1]
+ cd /home/ggml/work/llama.cpp/build-ci-release
+ python3 ../convert-lora-to-ggml.py ../models-mnt/open-llama/7B-v2/lora
model.layers.0.self_attn.q_proj => blk.0.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.0.self_attn.q_proj => blk.0.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.0.self_attn.v_proj => blk.0.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.0.self_attn.v_proj => blk.0.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.1.self_attn.q_proj => blk.1.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.1.self_attn.q_proj => blk.1.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.1.self_attn.v_proj => blk.1.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.1.self_attn.v_proj => blk.1.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.2.self_attn.q_proj => blk.2.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.2.self_attn.q_proj => blk.2.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.2.self_attn.v_proj => blk.2.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.2.self_attn.v_proj => blk.2.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.3.self_attn.q_proj => blk.3.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.3.self_attn.q_proj => blk.3.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.3.self_attn.v_proj => blk.3.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.3.self_attn.v_proj => blk.3.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.4.self_attn.q_proj => blk.4.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.4.self_attn.q_proj => blk.4.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.4.self_attn.v_proj => blk.4.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.4.self_attn.v_proj => blk.4.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.5.self_attn.q_proj => blk.5.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.5.self_attn.q_proj => blk.5.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.5.self_attn.v_proj => blk.5.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.5.self_attn.v_proj => blk.5.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.6.self_attn.q_proj => blk.6.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.6.self_attn.q_proj => blk.6.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.6.self_attn.v_proj => blk.6.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.6.self_attn.v_proj => blk.6.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.7.self_attn.q_proj => blk.7.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.7.self_attn.q_proj => blk.7.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.7.self_attn.v_proj => blk.7.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.7.self_attn.v_proj => blk.7.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.8.self_attn.q_proj => blk.8.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.8.self_attn.q_proj => blk.8.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.8.self_attn.v_proj => blk.8.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.8.self_attn.v_proj => blk.8.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.9.self_attn.q_proj => blk.9.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.9.self_attn.q_proj => blk.9.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.9.self_attn.v_proj => blk.9.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.9.self_attn.v_proj => blk.9.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.10.self_attn.q_proj => blk.10.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.10.self_attn.q_proj => blk.10.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.10.self_attn.v_proj => blk.10.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.10.self_attn.v_proj => blk.10.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.11.self_attn.q_proj => blk.11.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.11.self_attn.q_proj => blk.11.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.11.self_attn.v_proj => blk.11.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.11.self_attn.v_proj => blk.11.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.12.self_attn.q_proj => blk.12.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.12.self_attn.q_proj => blk.12.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.12.self_attn.v_proj => blk.12.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.12.self_attn.v_proj => blk.12.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.13.self_attn.q_proj => blk.13.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.13.self_attn.q_proj => blk.13.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.13.self_attn.v_proj => blk.13.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.13.self_attn.v_proj => blk.13.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.14.self_attn.q_proj => blk.14.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.14.self_attn.q_proj => blk.14.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.14.self_attn.v_proj => blk.14.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.14.self_attn.v_proj => blk.14.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.15.self_attn.q_proj => blk.15.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.15.self_attn.q_proj => blk.15.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.15.self_attn.v_proj => blk.15.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.15.self_attn.v_proj => blk.15.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.16.self_attn.q_proj => blk.16.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.16.self_attn.q_proj => blk.16.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.16.self_attn.v_proj => blk.16.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.16.self_attn.v_proj => blk.16.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.17.self_attn.q_proj => blk.17.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.17.self_attn.q_proj => blk.17.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.17.self_attn.v_proj => blk.17.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.17.self_attn.v_proj => blk.17.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.18.self_attn.q_proj => blk.18.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.18.self_attn.q_proj => blk.18.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.18.self_attn.v_proj => blk.18.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.18.self_attn.v_proj => blk.18.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.19.self_attn.q_proj => blk.19.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.19.self_attn.q_proj => blk.19.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.19.self_attn.v_proj => blk.19.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.19.self_attn.v_proj => blk.19.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.20.self_attn.q_proj => blk.20.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.20.self_attn.q_proj => blk.20.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.20.self_attn.v_proj => blk.20.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.20.self_attn.v_proj => blk.20.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.21.self_attn.q_proj => blk.21.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.21.self_attn.q_proj => blk.21.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.21.self_attn.v_proj => blk.21.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.21.self_attn.v_proj => blk.21.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.22.self_attn.q_proj => blk.22.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.22.self_attn.q_proj => blk.22.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.22.self_attn.v_proj => blk.22.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.22.self_attn.v_proj => blk.22.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.23.self_attn.q_proj => blk.23.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.23.self_attn.q_proj => blk.23.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.23.self_attn.v_proj => blk.23.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.23.self_attn.v_proj => blk.23.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.24.self_attn.q_proj => blk.24.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.24.self_attn.q_proj => blk.24.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.24.self_attn.v_proj => blk.24.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.24.self_attn.v_proj => blk.24.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.25.self_attn.q_proj => blk.25.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.25.self_attn.q_proj => blk.25.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.25.self_attn.v_proj => blk.25.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.25.self_attn.v_proj => blk.25.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.26.self_attn.q_proj => blk.26.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.26.self_attn.q_proj => blk.26.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.26.self_attn.v_proj => blk.26.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.26.self_attn.v_proj => blk.26.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.27.self_attn.q_proj => blk.27.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.27.self_attn.q_proj => blk.27.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.27.self_attn.v_proj => blk.27.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.27.self_attn.v_proj => blk.27.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.28.self_attn.q_proj => blk.28.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.28.self_attn.q_proj => blk.28.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.28.self_attn.v_proj => blk.28.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.28.self_attn.v_proj => blk.28.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.29.self_attn.q_proj => blk.29.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.29.self_attn.q_proj => blk.29.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.29.self_attn.v_proj => blk.29.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.29.self_attn.v_proj => blk.29.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.30.self_attn.q_proj => blk.30.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.30.self_attn.q_proj => blk.30.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.30.self_attn.v_proj => blk.30.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.30.self_attn.v_proj => blk.30.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.31.self_attn.q_proj => blk.31.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.31.self_attn.q_proj => blk.31.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.31.self_attn.v_proj => blk.31.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.31.self_attn.v_proj => blk.31.attn_v.weight.loraB (4096, 64) float32 1.00MB
Converted ../models-mnt/open-llama/7B-v2/lora/adapter_config.json and ../models-mnt/open-llama/7B-v2/lora/adapter_model.bin to ../models-mnt/open-llama/7B-v2/lora/ggml-adapter-model.bin
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl-shakespeare-f16.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf -f ../models-mnt/shakespeare/shakespeare.txt -t 1 -ngl 999 -c 2048 -b 512 --chunks 3
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1707577334
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   250.00 MiB
llm_load_tensors:      CUDA0 buffer size = 12603.02 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    12.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   171.60 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 65.872 ms
perplexity: calculating perplexity over 3 chunks, batch_size=512
perplexity: 0.71 seconds per pass - ETA 0.03 minutes
[1]14.1490,[2]11.8090,[3]11.2211,
llama_print_timings:        load time =    2340.07 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    1841.46 ms /  6144 tokens (    0.30 ms per token,  3336.48 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    2158.83 ms /  6145 tokens

Final estimate: PPL = 11.2211 +/- 0.52122

real	0m5.119s
user	0m4.095s
sys	0m1.492s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl-shakespeare-lora-f16.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf -f ../models-mnt/shakespeare/shakespeare.txt --lora ../models-mnt/open-llama/7B-v2/lora/ggml-adapter-model.bin -t 1 -ngl 999 -c 2048 -b 512 --chunks 3
main: build = 2116 (f026f812)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1707577339
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:  CUDA_Host buffer size =   250.00 MiB
llm_load_tensors:      CUDA0 buffer size = 12603.02 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    12.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   171.60 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB
llama_new_context_with_model: graph splits (measure): 3
llama_apply_lora_from_file_internal: applying lora adapter from '../models-mnt/open-llama/7B-v2/lora/ggml-adapter-model.bin' - please wait ...
llama_apply_lora_from_file_internal: r = 64, alpha = 128, scaling = 2.00
................ done (6009.33 ms)

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 60.913 ms
perplexity: calculating perplexity over 3 chunks, batch_size=512
perplexity: 0.71 seconds per pass - ETA 0.03 minutes
[1]11.5881,[2]9.1780,[3]8.7622,
llama_print_timings:        load time =   11050.78 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    1835.65 ms /  6144 tokens (    0.30 ms per token,  3347.05 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    2144.13 ms /  6145 tokens

Final estimate: PPL = 8.7622 +/- 0.39497

real	0m13.833s
user	0m7.269s
sys	0m7.054s
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-lora-ppl.log
++ cat /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl-shakespeare-f16.log
++ grep '^\[1\]'
++ cat /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl-shakespeare-lora-f16.log
++ grep '^\[1\]'
+ compare_ppl 'f16 shakespeare' '[1]14.1490,[2]11.8090,[3]11.2211,' '[1]11.5881,[2]9.1780,[3]8.7622,'
+ qnt='f16 shakespeare'
++ echo '[1]14.1490,[2]11.8090,[3]11.2211,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl1=11.2211
++ echo '[1]11.5881,[2]9.1780,[3]8.7622,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl2=8.7622
++ echo '11.2211 < 8.7622'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s %s OK\n' 'f16 shakespeare' 11.2211 8.7622
+ return 0
  - f16 shakespeare @ 11.2211 8.7622 OK
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_debug
+ cd /home/ggml/work/llama.cpp
+ tee /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/ctest_with_model_debug.log
+ local model
++ gg_get_model
++ local gguf_3b=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
++ local gguf_7b=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf ]]
++ [[ -s /mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf ]]
++ echo -n /mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
+ model=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
+ cd build-ci-debug
+ set -e
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/ctest_with_model_debug-ctest.log
+ LLAMACPP_TEST_MODELFILE=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /home/ggml/work/llama.cpp/build-ci-debug
    Start 21: test-model-load-cancel
1/2 Test #21: test-model-load-cancel ...........   Passed    8.40 sec
    Start 22: test-autorelease
2/2 Test #22: test-autorelease .................   Passed    1.69 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =  10.09 sec*proc (2 tests)

Total Test time (real) =  10.10 sec
0.53user 9.58system 0:10.12elapsed 99%CPU (0avgtext+0avgdata 13775228maxresident)k
0inputs+40outputs (0major+3609376minor)pagefaults 0swaps
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_release
+ cd /home/ggml/work/llama.cpp
+ local model
+ tee /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/ctest_with_model_release.log
++ gg_get_model
++ local gguf_3b=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
++ local gguf_7b=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf ]]
++ [[ -s /mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf ]]
++ echo -n /mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
+ model=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/f0/26f8120f97090d34a52b3dc023c82e0ede3f7d/ggml-4-x86-cuda-v100/ctest_with_model_release-ctest.log
+ LLAMACPP_TEST_MODELFILE=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /home/ggml/work/llama.cpp/build-ci-release
    Start 21: test-model-load-cancel
1/2 Test #21: test-model-load-cancel ...........   Passed    8.30 sec
    Start 22: test-autorelease
2/2 Test #22: test-autorelease .................   Passed    1.54 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   9.84 sec*proc (2 tests)

Total Test time (real) =   9.84 sec
0.26user 9.60system 0:09.87elapsed 99%CPU (0avgtext+0avgdata 13774632maxresident)k
0inputs+40outputs (0major+3609362minor)pagefaults 0swaps
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
