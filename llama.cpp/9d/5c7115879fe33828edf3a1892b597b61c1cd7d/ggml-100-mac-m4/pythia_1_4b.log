Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.601s
user	0m0.884s
sys	0m1.259s
++ nproc
+ make -j10
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  6%] Built target sha1
[  6%] Built target xxhash
[  6%] Built target sha256
[  6%] Built target build_info
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 18%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 19%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 21%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama
[ 22%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 23%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 24%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple-chat
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Built target llava
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Linking CXX static library libcommon.a
[ 31%] Linking CXX shared library libllava_shared.dylib
[ 32%] Linking CXX static library libllava_static.a
[ 32%] Built target test-c
[ 32%] Built target llama-quantize-stats
[ 32%] Built target llama-simple-chat
[ 32%] Built target llama-simple
[ 32%] Built target llava_static
[ 32%] Built target common
[ 32%] Built target llava_shared
[ 33%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-0
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 40%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-log
[ 46%] Linking CXX executable ../bin/test-arg-parser
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Built target test-tokenizer-1-spm
[ 46%] Built target test-tokenizer-0
[ 46%] Built target test-tokenizer-1-bpe
[ 46%] Built target test-json-schema-to-grammar
[ 46%] Built target test-sampling
[ 46%] Built target test-log
[ 46%] Built target test-grammar-integration
[ 46%] Built target test-grammar-parser
[ 46%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 47%] Built target test-llama-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 50%] Built target test-arg-parser
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 51%] Linking CXX executable ../bin/test-chat-template
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 52%] Linking CXX executable ../bin/test-gguf
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-backend-ops
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-barrier
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Built target test-gguf
[ 58%] Built target test-chat-template
[ 58%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 58%] Linking CXX executable ../bin/test-quantize-fns
[ 58%] Built target test-backend-ops
[ 58%] Built target test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-quantize-perf
[ 59%] Built target test-autorelease
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 61%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 61%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Built target test-barrier
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Linking CXX executable ../bin/test-rope
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-batched
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Built target test-quantize-fns
[ 67%] Built target test-quantize-perf
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Linking CXX executable ../../bin/llama-gguf-split
[ 67%] Built target llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-gritlm
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Built target test-rope
[ 68%] Built target llama-batched
[ 68%] Built target llama-embedding
[ 69%] Linking CXX executable ../../bin/llama-infill
[ 69%] Built target llama-eval-callback
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target llama-gbnf-validator
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Built target llama-gguf-split
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 71%] Built target llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-bench
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 73%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Linking CXX executable ../../bin/llama-lookup-create
[ 75%] Built target llama-imatrix
[ 75%] Built target llama-infill
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Linking CXX executable ../../bin/llama-cli
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Built target llama-lookahead
[ 78%] Built target llama-bench
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Built target llama-lookup-create
[ 79%] Built target llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookup-merge
[ 80%] Generating loading.html.hpp
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookup-stats
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Built target llama-cli
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Generating index.html.gz.hpp
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Built target llama-parallel
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Linking CXX executable ../../bin/llama-quantize
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Built target llama-passkey
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Linking CXX executable ../../bin/llama-run
[ 86%] Built target llama-perplexity
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Built target llama-quantize
[ 90%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Built target llama-save-load-state
[ 90%] Built target llama-run
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-speculative
[ 92%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Built target llama-tokenize
[ 95%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Built target llama-tts
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Built target llama-gen-docs
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.836s
user	0m5.976s
sys	0m9.427s

main: quantize time =  2293.65 ms
main:    total time =  2293.65 ms

main: quantize time =  1345.15 ms
main:    total time =  1345.15 ms

main: quantize time =  2103.09 ms
main:    total time =  2103.09 ms

main: quantize time =  2306.95 ms
main:    total time =  2306.95 ms

main: quantize time =  2031.99 ms
main:    total time =  2032.00 ms

main: quantize time =  4985.62 ms
main:    total time =  4985.62 ms

main: quantize time =  5908.80 ms
main:    total time =  5908.80 ms

main: quantize time =  6931.05 ms
main:    total time =  6931.05 ms

main: quantize time =  6026.27 ms
main:    total time =  6026.27 ms

main: quantize time =  4518.44 ms
main:    total time =  4518.44 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.105 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.230 I main: llama backend init
0.00.000.240 I main: load the model and apply lora adapter, if any
0.00.030.634 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.041.415 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.432 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.436 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.437 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.438 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.438 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.439 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.441 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.441 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.442 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.443 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.444 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.444 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.445 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.454 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.455 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.455 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.377 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.583 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.876 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.059.885 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.885 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.886 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.886 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.888 I llama_model_loader: - type  f32:  194 tensors
0.00.059.888 I llama_model_loader: - type  f16:   98 tensors
0.00.093.291 I llm_load_vocab: special tokens cache size = 25
0.00.100.520 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.100.523 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.100.524 I llm_load_print_meta: arch             = gptneox
0.00.100.524 I llm_load_print_meta: vocab type       = BPE
0.00.100.524 I llm_load_print_meta: n_vocab          = 50304
0.00.100.524 I llm_load_print_meta: n_merges         = 50009
0.00.100.524 I llm_load_print_meta: vocab_only       = 0
0.00.100.525 I llm_load_print_meta: n_ctx_train      = 2048
0.00.100.525 I llm_load_print_meta: n_embd           = 2048
0.00.100.525 I llm_load_print_meta: n_layer          = 24
0.00.100.529 I llm_load_print_meta: n_head           = 16
0.00.100.530 I llm_load_print_meta: n_head_kv        = 16
0.00.100.530 I llm_load_print_meta: n_rot            = 32
0.00.100.530 I llm_load_print_meta: n_swa            = 0
0.00.100.530 I llm_load_print_meta: n_embd_head_k    = 128
0.00.100.530 I llm_load_print_meta: n_embd_head_v    = 128
0.00.100.531 I llm_load_print_meta: n_gqa            = 1
0.00.100.532 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.100.532 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.100.533 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.100.533 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.100.534 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.100.534 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.100.534 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.100.535 I llm_load_print_meta: n_ff             = 8192
0.00.100.535 I llm_load_print_meta: n_expert         = 0
0.00.100.535 I llm_load_print_meta: n_expert_used    = 0
0.00.100.535 I llm_load_print_meta: causal attn      = 1
0.00.100.535 I llm_load_print_meta: pooling type     = 0
0.00.100.535 I llm_load_print_meta: rope type        = 2
0.00.100.535 I llm_load_print_meta: rope scaling     = linear
0.00.100.536 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.100.536 I llm_load_print_meta: freq_scale_train = 1
0.00.100.536 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.100.536 I llm_load_print_meta: rope_finetuned   = unknown
0.00.100.537 I llm_load_print_meta: ssm_d_conv       = 0
0.00.100.537 I llm_load_print_meta: ssm_d_inner      = 0
0.00.100.538 I llm_load_print_meta: ssm_d_state      = 0
0.00.100.538 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.100.538 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.100.538 I llm_load_print_meta: model type       = 1.4B
0.00.100.539 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.100.539 I llm_load_print_meta: model params     = 1.41 B
0.00.100.540 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.100.540 I llm_load_print_meta: general.name     = 1.4B
0.00.100.540 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.100.540 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.100.541 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.100.541 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.100.541 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.100.542 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.100.542 I llm_load_print_meta: max token length = 1024
0.00.103.154 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.103.155 I llm_load_tensors: offloading output layer to GPU
0.00.103.155 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.103.174 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.103.175 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.104.165 I llama_new_context_with_model: n_seq_max     = 1
0.00.104.166 I llama_new_context_with_model: n_ctx         = 2048
0.00.104.167 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.104.167 I llama_new_context_with_model: n_batch       = 2048
0.00.104.167 I llama_new_context_with_model: n_ubatch      = 512
0.00.104.167 I llama_new_context_with_model: flash_attn    = 0
0.00.104.168 I llama_new_context_with_model: freq_base     = 10000.0
0.00.104.168 I llama_new_context_with_model: freq_scale    = 1
0.00.104.169 I ggml_metal_init: allocating
0.00.104.178 I ggml_metal_init: found device: Apple M4
0.00.104.181 I ggml_metal_init: picking default device: Apple M4
0.00.104.906 I ggml_metal_init: using embedded metal library
0.00.116.661 I ggml_metal_init: GPU name:   Apple M4
0.00.116.663 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.116.663 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.116.664 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.116.664 I ggml_metal_init: simdgroup reduction   = true
0.00.116.664 I ggml_metal_init: simdgroup matrix mul. = true
0.00.116.664 I ggml_metal_init: has bfloat            = true
0.00.116.664 I ggml_metal_init: use bfloat            = true
0.00.116.665 I ggml_metal_init: hasUnifiedMemory      = true
0.00.116.665 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.140.942 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.163.276 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.163.284 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.163.307 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.164.342 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.164.344 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.164.345 I llama_new_context_with_model: graph nodes  = 967
0.00.164.345 I llama_new_context_with_model: graph splits = 2
0.00.164.371 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.164.491 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.164.491 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.249.061 I main: llama threadpool init, n_threads = 4
0.00.249.089 I 
0.00.249.124 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.249.126 I 
0.00.249.201 I sampler seed: 1234
0.00.249.205 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.249.239 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.249.241 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.249.241 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.089.376 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56573.71 tokens per second)
0.02.089.376 I llama_perf_context_print:        load time =     218.41 ms
0.02.089.377 I llama_perf_context_print: prompt eval time =      43.70 ms /     7 tokens (    6.24 ms per token,   160.18 tokens per second)
0.02.089.378 I llama_perf_context_print:        eval time =    1793.50 ms /    63 runs   (   28.47 ms per token,    35.13 tokens per second)
0.02.089.378 I llama_perf_context_print:       total time =    1840.32 ms /    70 tokens
0.02.089.595 I ggml_metal_free: deallocating

real	0m2.378s
user	0m0.147s
sys	0m0.106s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.805 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.666 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.670 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.672 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.673 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.673 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.673 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.674 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.675 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.675 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.675 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.676 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.676 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.676 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.677 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.681 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.681 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.681 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.663 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.737 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.765 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.767 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.767 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.767 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.768 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.768 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.769 I llama_model_loader: - type  f32:  194 tensors
0.00.027.770 I llama_model_loader: - type q8_0:   98 tensors
0.00.050.262 I llm_load_vocab: special tokens cache size = 25
0.00.056.144 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.149 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.150 I llm_load_print_meta: arch             = gptneox
0.00.056.152 I llm_load_print_meta: vocab type       = BPE
0.00.056.152 I llm_load_print_meta: n_vocab          = 50304
0.00.056.152 I llm_load_print_meta: n_merges         = 50009
0.00.056.153 I llm_load_print_meta: vocab_only       = 0
0.00.056.153 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.153 I llm_load_print_meta: n_embd           = 2048
0.00.056.153 I llm_load_print_meta: n_layer          = 24
0.00.056.159 I llm_load_print_meta: n_head           = 16
0.00.056.160 I llm_load_print_meta: n_head_kv        = 16
0.00.056.160 I llm_load_print_meta: n_rot            = 32
0.00.056.160 I llm_load_print_meta: n_swa            = 0
0.00.056.161 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.161 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.161 I llm_load_print_meta: n_gqa            = 1
0.00.056.162 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.163 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.164 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.165 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.165 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.165 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.165 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.166 I llm_load_print_meta: n_ff             = 8192
0.00.056.166 I llm_load_print_meta: n_expert         = 0
0.00.056.166 I llm_load_print_meta: n_expert_used    = 0
0.00.056.166 I llm_load_print_meta: causal attn      = 1
0.00.056.166 I llm_load_print_meta: pooling type     = 0
0.00.056.167 I llm_load_print_meta: rope type        = 2
0.00.056.167 I llm_load_print_meta: rope scaling     = linear
0.00.056.167 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.168 I llm_load_print_meta: freq_scale_train = 1
0.00.056.168 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.169 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.169 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.169 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.169 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.170 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.170 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.170 I llm_load_print_meta: model type       = 1.4B
0.00.056.170 I llm_load_print_meta: model ftype      = Q8_0
0.00.056.171 I llm_load_print_meta: model params     = 1.41 B
0.00.056.171 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.056.171 I llm_load_print_meta: general.name     = 1.4B
0.00.056.172 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.172 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.172 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.172 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.172 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.056.173 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.173 I llm_load_print_meta: max token length = 1024
0.00.058.686 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.686 I llm_load_tensors: offloading output layer to GPU
0.00.058.686 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.698 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.058.699 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.059.697 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.698 I llama_new_context_with_model: n_ctx         = 2048
0.00.059.698 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.059.698 I llama_new_context_with_model: n_batch       = 2048
0.00.059.699 I llama_new_context_with_model: n_ubatch      = 512
0.00.059.699 I llama_new_context_with_model: flash_attn    = 0
0.00.059.699 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.700 I llama_new_context_with_model: freq_scale    = 1
0.00.059.700 I ggml_metal_init: allocating
0.00.059.710 I ggml_metal_init: found device: Apple M4
0.00.059.714 I ggml_metal_init: picking default device: Apple M4
0.00.060.493 I ggml_metal_init: using embedded metal library
0.00.063.049 I ggml_metal_init: GPU name:   Apple M4
0.00.063.051 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.052 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.052 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.052 I ggml_metal_init: simdgroup reduction   = true
0.00.063.052 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.053 I ggml_metal_init: has bfloat            = true
0.00.063.053 I ggml_metal_init: use bfloat            = true
0.00.063.053 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.054 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.640 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.097.931 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.939 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.964 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.998 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.099.001 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.099.001 I llama_new_context_with_model: graph nodes  = 967
0.00.099.002 I llama_new_context_with_model: graph splits = 2
0.00.099.021 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.164 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.165 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.350.968 I main: llama threadpool init, n_threads = 4
0.01.351.014 I 
0.01.351.047 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.351.047 I 
0.01.351.277 I sampler seed: 1234
0.01.351.282 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.351.321 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.351.321 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.351.321 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.443.902 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56259.90 tokens per second)
0.02.443.902 I llama_perf_context_print:        load time =    1341.16 ms
0.02.443.903 I llama_perf_context_print: prompt eval time =      42.84 ms /     7 tokens (    6.12 ms per token,   163.41 tokens per second)
0.02.443.904 I llama_perf_context_print:        eval time =    1046.91 ms /    63 runs   (   16.62 ms per token,    60.18 tokens per second)
0.02.443.905 I llama_perf_context_print:       total time =    1092.94 ms /    70 tokens
0.02.444.110 I ggml_metal_free: deallocating

real	0m2.461s
user	0m0.114s
sys	0m0.227s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.013.675 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.941 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.030.947 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.949 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.949 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.950 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.954 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.954 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.955 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.956 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.956 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.956 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.956 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.957 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.957 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.960 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.960 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.960 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.065 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.177 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.339 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.341 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.341 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.341 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.342 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.346 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.347 I llama_model_loader: - type  f32:  194 tensors
0.00.040.347 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.347 I llama_model_loader: - type q6_K:    1 tensors
0.00.062.422 I llm_load_vocab: special tokens cache size = 25
0.00.068.507 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.511 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.512 I llm_load_print_meta: arch             = gptneox
0.00.068.512 I llm_load_print_meta: vocab type       = BPE
0.00.068.512 I llm_load_print_meta: n_vocab          = 50304
0.00.068.513 I llm_load_print_meta: n_merges         = 50009
0.00.068.514 I llm_load_print_meta: vocab_only       = 0
0.00.068.518 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.518 I llm_load_print_meta: n_embd           = 2048
0.00.068.518 I llm_load_print_meta: n_layer          = 24
0.00.068.522 I llm_load_print_meta: n_head           = 16
0.00.068.523 I llm_load_print_meta: n_head_kv        = 16
0.00.068.523 I llm_load_print_meta: n_rot            = 32
0.00.068.523 I llm_load_print_meta: n_swa            = 0
0.00.068.523 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.524 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.524 I llm_load_print_meta: n_gqa            = 1
0.00.068.525 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.526 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.526 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.529 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.530 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.531 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.531 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.531 I llm_load_print_meta: n_ff             = 8192
0.00.068.532 I llm_load_print_meta: n_expert         = 0
0.00.068.532 I llm_load_print_meta: n_expert_used    = 0
0.00.068.532 I llm_load_print_meta: causal attn      = 1
0.00.068.532 I llm_load_print_meta: pooling type     = 0
0.00.068.532 I llm_load_print_meta: rope type        = 2
0.00.068.533 I llm_load_print_meta: rope scaling     = linear
0.00.068.533 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.533 I llm_load_print_meta: freq_scale_train = 1
0.00.068.534 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.535 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.535 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.535 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.535 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.535 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.535 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.535 I llm_load_print_meta: model type       = 1.4B
0.00.068.536 I llm_load_print_meta: model ftype      = Q4_0
0.00.068.536 I llm_load_print_meta: model params     = 1.41 B
0.00.068.537 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.068.537 I llm_load_print_meta: general.name     = 1.4B
0.00.068.537 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.537 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.537 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.537 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.538 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.068.538 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.538 I llm_load_print_meta: max token length = 1024
0.00.070.496 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.497 I llm_load_tensors: offloading output layer to GPU
0.00.070.497 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.508 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.070.509 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.071.418 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.418 I llama_new_context_with_model: n_ctx         = 2048
0.00.071.419 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.071.419 I llama_new_context_with_model: n_batch       = 2048
0.00.071.419 I llama_new_context_with_model: n_ubatch      = 512
0.00.071.419 I llama_new_context_with_model: flash_attn    = 0
0.00.071.420 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.420 I llama_new_context_with_model: freq_scale    = 1
0.00.071.420 I ggml_metal_init: allocating
0.00.071.425 I ggml_metal_init: found device: Apple M4
0.00.071.429 I ggml_metal_init: picking default device: Apple M4
0.00.072.099 I ggml_metal_init: using embedded metal library
0.00.074.630 I ggml_metal_init: GPU name:   Apple M4
0.00.074.632 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.632 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.632 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.633 I ggml_metal_init: simdgroup reduction   = true
0.00.074.633 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.633 I ggml_metal_init: has bfloat            = true
0.00.074.633 I ggml_metal_init: use bfloat            = true
0.00.074.634 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.635 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.850 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.108.930 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.108.935 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.108.957 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.885 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.109.886 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.109.887 I llama_new_context_with_model: graph nodes  = 967
0.00.109.887 I llama_new_context_with_model: graph splits = 2
0.00.109.903 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.110.030 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.110.031 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.884.408 I main: llama threadpool init, n_threads = 4
0.00.884.448 I 
0.00.884.482 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.884.482 I 
0.00.884.701 I sampler seed: 1234
0.00.884.705 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.884.720 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.884.722 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.884.722 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.564.631 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51005.75 tokens per second)
0.01.564.632 I llama_perf_context_print:        load time =     870.73 ms
0.01.564.633 I llama_perf_context_print: prompt eval time =      43.75 ms /     7 tokens (    6.25 ms per token,   160.00 tokens per second)
0.01.564.634 I llama_perf_context_print:        eval time =     633.28 ms /    63 runs   (   10.05 ms per token,    99.48 tokens per second)
0.01.564.634 I llama_perf_context_print:       total time =     680.23 ms /    70 tokens
0.01.564.818 I ggml_metal_free: deallocating

real	0m1.587s
user	0m0.114s
sys	0m0.170s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.148 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.706 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.710 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.711 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.717 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.717 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.718 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.718 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.719 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.719 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.719 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.720 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.720 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.721 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.721 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.723 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.723 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.723 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.717 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.814 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.895 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.896 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.896 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.897 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.897 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.897 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.898 I llama_model_loader: - type  f32:  194 tensors
0.00.024.898 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.898 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.346 I llm_load_vocab: special tokens cache size = 25
0.00.051.376 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.378 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.379 I llm_load_print_meta: arch             = gptneox
0.00.051.379 I llm_load_print_meta: vocab type       = BPE
0.00.051.379 I llm_load_print_meta: n_vocab          = 50304
0.00.051.380 I llm_load_print_meta: n_merges         = 50009
0.00.051.380 I llm_load_print_meta: vocab_only       = 0
0.00.051.380 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.380 I llm_load_print_meta: n_embd           = 2048
0.00.051.380 I llm_load_print_meta: n_layer          = 24
0.00.051.383 I llm_load_print_meta: n_head           = 16
0.00.051.383 I llm_load_print_meta: n_head_kv        = 16
0.00.051.383 I llm_load_print_meta: n_rot            = 32
0.00.051.386 I llm_load_print_meta: n_swa            = 0
0.00.051.386 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.386 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.387 I llm_load_print_meta: n_gqa            = 1
0.00.051.387 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.388 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.389 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.389 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.389 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.389 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.389 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.390 I llm_load_print_meta: n_ff             = 8192
0.00.051.390 I llm_load_print_meta: n_expert         = 0
0.00.051.390 I llm_load_print_meta: n_expert_used    = 0
0.00.051.391 I llm_load_print_meta: causal attn      = 1
0.00.051.391 I llm_load_print_meta: pooling type     = 0
0.00.051.391 I llm_load_print_meta: rope type        = 2
0.00.051.391 I llm_load_print_meta: rope scaling     = linear
0.00.051.391 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.392 I llm_load_print_meta: freq_scale_train = 1
0.00.051.392 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.392 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.392 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.392 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.393 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.393 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.393 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.393 I llm_load_print_meta: model type       = 1.4B
0.00.051.393 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.394 I llm_load_print_meta: model params     = 1.41 B
0.00.051.394 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.394 I llm_load_print_meta: general.name     = 1.4B
0.00.051.395 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.395 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.395 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.395 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.396 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.396 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.396 I llm_load_print_meta: max token length = 1024
0.00.053.245 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.246 I llm_load_tensors: offloading output layer to GPU
0.00.053.246 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.252 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.252 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.160 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.161 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.161 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.161 I llama_new_context_with_model: n_batch       = 2048
0.00.054.162 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.162 I llama_new_context_with_model: flash_attn    = 0
0.00.054.162 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.162 I llama_new_context_with_model: freq_scale    = 1
0.00.054.163 I ggml_metal_init: allocating
0.00.054.166 I ggml_metal_init: found device: Apple M4
0.00.054.168 I ggml_metal_init: picking default device: Apple M4
0.00.054.775 I ggml_metal_init: using embedded metal library
0.00.057.090 I ggml_metal_init: GPU name:   Apple M4
0.00.057.091 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.092 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.092 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.092 I ggml_metal_init: simdgroup reduction   = true
0.00.057.092 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.092 I ggml_metal_init: has bfloat            = true
0.00.057.093 I ggml_metal_init: use bfloat            = true
0.00.057.093 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.094 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.942 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.690 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.697 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.718 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.726 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.728 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.728 I llama_new_context_with_model: graph nodes  = 967
0.00.087.728 I llama_new_context_with_model: graph splits = 2
0.00.087.744 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.887 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.888 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.273 I main: llama threadpool init, n_threads = 4
0.00.717.314 I 
0.00.717.347 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.717.349 I 
0.00.717.581 I sampler seed: 1234
0.00.717.587 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.717.603 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.717.605 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.717.605 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.448.842 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57723.58 tokens per second)
0.01.448.842 I llama_perf_context_print:        load time =     708.12 ms
0.01.448.844 I llama_perf_context_print: prompt eval time =      43.90 ms /     7 tokens (    6.27 ms per token,   159.47 tokens per second)
0.01.448.845 I llama_perf_context_print:        eval time =     684.45 ms /    63 runs   (   10.86 ms per token,    92.05 tokens per second)
0.01.448.845 I llama_perf_context_print:       total time =     731.57 ms /    70 tokens
0.01.449.073 I ggml_metal_free: deallocating

real	0m1.465s
user	0m0.110s
sys	0m0.145s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.011.308 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.841 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.845 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.847 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.848 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.848 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.852 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.853 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.855 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.855 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.856 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.856 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.856 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.857 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.860 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.863 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.863 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.863 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.780 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.820 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.749 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.750 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.750 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.751 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.751 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.751 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.752 I llama_model_loader: - type  f32:  194 tensors
0.00.026.752 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.752 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.319 I llm_load_vocab: special tokens cache size = 25
0.00.053.106 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.109 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.109 I llm_load_print_meta: arch             = gptneox
0.00.053.109 I llm_load_print_meta: vocab type       = BPE
0.00.053.109 I llm_load_print_meta: n_vocab          = 50304
0.00.053.110 I llm_load_print_meta: n_merges         = 50009
0.00.053.110 I llm_load_print_meta: vocab_only       = 0
0.00.053.110 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.110 I llm_load_print_meta: n_embd           = 2048
0.00.053.110 I llm_load_print_meta: n_layer          = 24
0.00.053.113 I llm_load_print_meta: n_head           = 16
0.00.053.114 I llm_load_print_meta: n_head_kv        = 16
0.00.053.114 I llm_load_print_meta: n_rot            = 32
0.00.053.114 I llm_load_print_meta: n_swa            = 0
0.00.053.114 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.115 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.115 I llm_load_print_meta: n_gqa            = 1
0.00.053.116 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.117 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.117 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.118 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.118 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.118 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.118 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.119 I llm_load_print_meta: n_ff             = 8192
0.00.053.119 I llm_load_print_meta: n_expert         = 0
0.00.053.119 I llm_load_print_meta: n_expert_used    = 0
0.00.053.121 I llm_load_print_meta: causal attn      = 1
0.00.053.123 I llm_load_print_meta: pooling type     = 0
0.00.053.123 I llm_load_print_meta: rope type        = 2
0.00.053.123 I llm_load_print_meta: rope scaling     = linear
0.00.053.124 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.124 I llm_load_print_meta: freq_scale_train = 1
0.00.053.124 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.124 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.124 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.124 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.125 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.125 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.125 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.125 I llm_load_print_meta: model type       = 1.4B
0.00.053.125 I llm_load_print_meta: model ftype      = Q5_0
0.00.053.126 I llm_load_print_meta: model params     = 1.41 B
0.00.053.126 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.127 I llm_load_print_meta: general.name     = 1.4B
0.00.053.127 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.127 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.127 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.127 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.129 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.129 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.130 I llm_load_print_meta: max token length = 1024
0.00.055.104 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.104 I llm_load_tensors: offloading output layer to GPU
0.00.055.104 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.115 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.116 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.056.049 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.050 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.050 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.051 I llama_new_context_with_model: n_batch       = 2048
0.00.056.051 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.051 I llama_new_context_with_model: flash_attn    = 0
0.00.056.051 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.052 I llama_new_context_with_model: freq_scale    = 1
0.00.056.052 I ggml_metal_init: allocating
0.00.056.056 I ggml_metal_init: found device: Apple M4
0.00.056.058 I ggml_metal_init: picking default device: Apple M4
0.00.056.662 I ggml_metal_init: using embedded metal library
0.00.058.969 I ggml_metal_init: GPU name:   Apple M4
0.00.058.970 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.971 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.971 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.971 I ggml_metal_init: simdgroup reduction   = true
0.00.058.971 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.971 I ggml_metal_init: has bfloat            = true
0.00.058.972 I ggml_metal_init: use bfloat            = true
0.00.058.972 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.974 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.800 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.090.232 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.241 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.262 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.298 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.299 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.300 I llama_new_context_with_model: graph nodes  = 967
0.00.091.300 I llama_new_context_with_model: graph splits = 2
0.00.091.316 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.459 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.460 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.356 I main: llama threadpool init, n_threads = 4
0.00.768.393 I 
0.00.768.423 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.768.423 I 
0.00.768.655 I sampler seed: 1234
0.00.768.659 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.768.675 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.768.677 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.768.677 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.554.510 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.01.554.510 I llama_perf_context_print:        load time =     757.04 ms
0.01.554.511 I llama_perf_context_print: prompt eval time =      43.10 ms /     7 tokens (    6.16 ms per token,   162.40 tokens per second)
0.01.554.512 I llama_perf_context_print:        eval time =     739.79 ms /    63 runs   (   11.74 ms per token,    85.16 tokens per second)
0.01.554.512 I llama_perf_context_print:       total time =     786.16 ms /    70 tokens
0.01.554.689 I ggml_metal_free: deallocating

real	0m1.573s
user	0m0.110s
sys	0m0.164s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.804 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.600 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.604 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.606 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.611 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.611 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.612 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.612 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.614 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.614 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.614 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.615 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.615 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.615 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.616 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.617 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.618 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.618 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.574 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.645 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.472 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.473 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.474 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.474 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.474 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.475 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.475 I llama_model_loader: - type  f32:  194 tensors
0.00.024.476 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.476 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.018 I llm_load_vocab: special tokens cache size = 25
0.00.051.068 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.071 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.072 I llm_load_print_meta: arch             = gptneox
0.00.051.072 I llm_load_print_meta: vocab type       = BPE
0.00.051.072 I llm_load_print_meta: n_vocab          = 50304
0.00.051.072 I llm_load_print_meta: n_merges         = 50009
0.00.051.073 I llm_load_print_meta: vocab_only       = 0
0.00.051.073 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.073 I llm_load_print_meta: n_embd           = 2048
0.00.051.073 I llm_load_print_meta: n_layer          = 24
0.00.051.075 I llm_load_print_meta: n_head           = 16
0.00.051.076 I llm_load_print_meta: n_head_kv        = 16
0.00.051.076 I llm_load_print_meta: n_rot            = 32
0.00.051.077 I llm_load_print_meta: n_swa            = 0
0.00.051.077 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.077 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.078 I llm_load_print_meta: n_gqa            = 1
0.00.051.079 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.079 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.080 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.080 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.080 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.081 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.081 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.081 I llm_load_print_meta: n_ff             = 8192
0.00.051.082 I llm_load_print_meta: n_expert         = 0
0.00.051.082 I llm_load_print_meta: n_expert_used    = 0
0.00.051.082 I llm_load_print_meta: causal attn      = 1
0.00.051.082 I llm_load_print_meta: pooling type     = 0
0.00.051.082 I llm_load_print_meta: rope type        = 2
0.00.051.083 I llm_load_print_meta: rope scaling     = linear
0.00.051.083 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.083 I llm_load_print_meta: freq_scale_train = 1
0.00.051.084 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.084 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.084 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.084 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.084 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.084 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.084 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.085 I llm_load_print_meta: model type       = 1.4B
0.00.051.085 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.086 I llm_load_print_meta: model params     = 1.41 B
0.00.051.086 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.086 I llm_load_print_meta: general.name     = 1.4B
0.00.051.087 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.087 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.087 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.089 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.089 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.090 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.090 I llm_load_print_meta: max token length = 1024
0.00.052.868 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.868 I llm_load_tensors: offloading output layer to GPU
0.00.052.869 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.874 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.874 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.787 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.788 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.788 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.788 I llama_new_context_with_model: n_batch       = 2048
0.00.053.788 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.789 I llama_new_context_with_model: flash_attn    = 0
0.00.053.789 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.789 I llama_new_context_with_model: freq_scale    = 1
0.00.053.790 I ggml_metal_init: allocating
0.00.053.794 I ggml_metal_init: found device: Apple M4
0.00.053.797 I ggml_metal_init: picking default device: Apple M4
0.00.054.400 I ggml_metal_init: using embedded metal library
0.00.056.718 I ggml_metal_init: GPU name:   Apple M4
0.00.056.720 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.720 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.720 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.721 I ggml_metal_init: simdgroup reduction   = true
0.00.056.721 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.721 I ggml_metal_init: has bfloat            = true
0.00.056.721 I ggml_metal_init: use bfloat            = true
0.00.056.722 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.722 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.448 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.960 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.969 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.988 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.960 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.961 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.961 I llama_new_context_with_model: graph nodes  = 967
0.00.086.962 I llama_new_context_with_model: graph splits = 2
0.00.086.971 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.112 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.112 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.132 I main: llama threadpool init, n_threads = 4
0.00.703.171 I 
0.00.703.218 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.219 I 
0.00.703.439 I sampler seed: 1234
0.00.703.443 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.703.459 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.703.460 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.703.460 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.545.889 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55081.46 tokens per second)
0.01.545.889 I llama_perf_context_print:        load time =     694.32 ms
0.01.545.890 I llama_perf_context_print: prompt eval time =      45.90 ms /     7 tokens (    6.56 ms per token,   152.50 tokens per second)
0.01.545.892 I llama_perf_context_print:        eval time =     793.46 ms /    63 runs   (   12.59 ms per token,    79.40 tokens per second)
0.01.545.892 I llama_perf_context_print:       total time =     842.76 ms /    70 tokens
0.01.546.063 I ggml_metal_free: deallocating

real	0m1.562s
user	0m0.109s
sys	0m0.160s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.103 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.565 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.570 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.572 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.573 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.573 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.573 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.573 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.574 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.575 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.575 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.576 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.576 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.576 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.578 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.580 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.580 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.581 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.470 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.507 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.439 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.441 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.441 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.441 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.442 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.442 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.443 I llama_model_loader: - type  f32:  194 tensors
0.00.023.443 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.443 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.443 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.287 I llm_load_vocab: special tokens cache size = 25
0.00.050.324 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.327 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.327 I llm_load_print_meta: arch             = gptneox
0.00.050.328 I llm_load_print_meta: vocab type       = BPE
0.00.050.328 I llm_load_print_meta: n_vocab          = 50304
0.00.050.328 I llm_load_print_meta: n_merges         = 50009
0.00.050.328 I llm_load_print_meta: vocab_only       = 0
0.00.050.328 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.328 I llm_load_print_meta: n_embd           = 2048
0.00.050.329 I llm_load_print_meta: n_layer          = 24
0.00.050.331 I llm_load_print_meta: n_head           = 16
0.00.050.332 I llm_load_print_meta: n_head_kv        = 16
0.00.050.332 I llm_load_print_meta: n_rot            = 32
0.00.050.332 I llm_load_print_meta: n_swa            = 0
0.00.050.333 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.335 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.335 I llm_load_print_meta: n_gqa            = 1
0.00.050.336 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.337 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.338 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.338 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.338 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.339 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.339 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.340 I llm_load_print_meta: n_ff             = 8192
0.00.050.340 I llm_load_print_meta: n_expert         = 0
0.00.050.340 I llm_load_print_meta: n_expert_used    = 0
0.00.050.340 I llm_load_print_meta: causal attn      = 1
0.00.050.341 I llm_load_print_meta: pooling type     = 0
0.00.050.342 I llm_load_print_meta: rope type        = 2
0.00.050.342 I llm_load_print_meta: rope scaling     = linear
0.00.050.342 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.342 I llm_load_print_meta: freq_scale_train = 1
0.00.050.342 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.343 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.343 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.343 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.343 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.343 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.343 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.343 I llm_load_print_meta: model type       = 1.4B
0.00.050.344 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.344 I llm_load_print_meta: model params     = 1.41 B
0.00.050.345 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.345 I llm_load_print_meta: general.name     = 1.4B
0.00.050.345 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.345 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.346 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.346 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.347 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.348 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.348 I llm_load_print_meta: max token length = 1024
0.00.051.992 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.993 I llm_load_tensors: offloading output layer to GPU
0.00.051.993 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.002 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.004 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.844 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.845 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.845 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.845 I llama_new_context_with_model: n_batch       = 2048
0.00.052.845 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.845 I llama_new_context_with_model: flash_attn    = 0
0.00.052.846 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.846 I llama_new_context_with_model: freq_scale    = 1
0.00.052.847 I ggml_metal_init: allocating
0.00.052.853 I ggml_metal_init: found device: Apple M4
0.00.052.855 I ggml_metal_init: picking default device: Apple M4
0.00.053.421 I ggml_metal_init: using embedded metal library
0.00.055.757 I ggml_metal_init: GPU name:   Apple M4
0.00.055.758 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.758 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.759 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.759 I ggml_metal_init: simdgroup reduction   = true
0.00.055.759 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.759 I ggml_metal_init: has bfloat            = true
0.00.055.760 I ggml_metal_init: use bfloat            = true
0.00.055.760 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.760 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.512 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.893 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.899 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.918 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.899 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.901 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.901 I llama_new_context_with_model: graph nodes  = 967
0.00.086.901 I llama_new_context_with_model: graph splits = 2
0.00.086.916 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.050 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.051 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.442.916 I main: llama threadpool init, n_threads = 4
0.00.442.948 I 
0.00.442.977 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.442.977 I 
0.00.443.204 I sampler seed: 1234
0.00.443.208 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.443.252 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.443.256 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.443.256 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.113.460 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50497.87 tokens per second)
0.01.113.461 I llama_perf_context_print:        load time =     433.81 ms
0.01.113.462 I llama_perf_context_print: prompt eval time =      35.81 ms /     7 tokens (    5.12 ms per token,   195.47 tokens per second)
0.01.113.462 I llama_perf_context_print:        eval time =     631.74 ms /    63 runs   (   10.03 ms per token,    99.73 tokens per second)
0.01.113.463 I llama_perf_context_print:       total time =     670.55 ms /    70 tokens
0.01.113.637 I ggml_metal_free: deallocating

real	0m1.133s
user	0m0.109s
sys	0m0.114s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.011.946 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.383 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.388 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.389 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.390 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.390 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.395 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.395 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.398 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.398 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.398 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.399 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.399 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.399 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.400 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.402 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.403 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.403 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.334 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.447 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.347 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.348 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.348 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.348 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.349 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.349 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.349 I llama_model_loader: - type  f32:  194 tensors
0.00.027.350 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.350 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.350 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.350 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.877 I llm_load_vocab: special tokens cache size = 25
0.00.053.900 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.903 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.903 I llm_load_print_meta: arch             = gptneox
0.00.053.904 I llm_load_print_meta: vocab type       = BPE
0.00.053.904 I llm_load_print_meta: n_vocab          = 50304
0.00.053.904 I llm_load_print_meta: n_merges         = 50009
0.00.053.904 I llm_load_print_meta: vocab_only       = 0
0.00.053.904 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.905 I llm_load_print_meta: n_embd           = 2048
0.00.053.905 I llm_load_print_meta: n_layer          = 24
0.00.053.907 I llm_load_print_meta: n_head           = 16
0.00.053.908 I llm_load_print_meta: n_head_kv        = 16
0.00.053.910 I llm_load_print_meta: n_rot            = 32
0.00.053.910 I llm_load_print_meta: n_swa            = 0
0.00.053.910 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.910 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.911 I llm_load_print_meta: n_gqa            = 1
0.00.053.912 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.912 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.913 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.913 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.913 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.914 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.914 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.914 I llm_load_print_meta: n_ff             = 8192
0.00.053.916 I llm_load_print_meta: n_expert         = 0
0.00.053.917 I llm_load_print_meta: n_expert_used    = 0
0.00.053.917 I llm_load_print_meta: causal attn      = 1
0.00.053.917 I llm_load_print_meta: pooling type     = 0
0.00.053.917 I llm_load_print_meta: rope type        = 2
0.00.053.917 I llm_load_print_meta: rope scaling     = linear
0.00.053.918 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.918 I llm_load_print_meta: freq_scale_train = 1
0.00.053.918 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.918 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.918 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.919 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.919 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.919 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.919 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.919 I llm_load_print_meta: model type       = 1.4B
0.00.053.919 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.053.920 I llm_load_print_meta: model params     = 1.41 B
0.00.053.924 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.053.924 I llm_load_print_meta: general.name     = 1.4B
0.00.053.925 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.925 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.925 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.925 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.925 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.929 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.929 I llm_load_print_meta: max token length = 1024
0.00.055.907 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.908 I llm_load_tensors: offloading output layer to GPU
0.00.055.908 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.919 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.055.920 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.056.900 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.900 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.901 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.901 I llama_new_context_with_model: n_batch       = 2048
0.00.056.901 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.901 I llama_new_context_with_model: flash_attn    = 0
0.00.056.902 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.902 I llama_new_context_with_model: freq_scale    = 1
0.00.056.903 I ggml_metal_init: allocating
0.00.056.910 I ggml_metal_init: found device: Apple M4
0.00.056.912 I ggml_metal_init: picking default device: Apple M4
0.00.057.537 I ggml_metal_init: using embedded metal library
0.00.059.878 I ggml_metal_init: GPU name:   Apple M4
0.00.059.881 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.881 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.882 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.882 I ggml_metal_init: simdgroup reduction   = true
0.00.059.882 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.882 I ggml_metal_init: has bfloat            = true
0.00.059.882 I ggml_metal_init: use bfloat            = true
0.00.059.883 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.884 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.741 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.685 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.690 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.708 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.687 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.688 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.688 I llama_new_context_with_model: graph nodes  = 967
0.00.090.688 I llama_new_context_with_model: graph splits = 2
0.00.090.703 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.844 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.844 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.538.568 I main: llama threadpool init, n_threads = 4
0.00.538.604 I 
0.00.538.649 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.538.651 I 
0.00.538.876 I sampler seed: 1234
0.00.538.881 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.538.913 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.538.915 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.538.915 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.289.440 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60787.67 tokens per second)
0.01.289.441 I llama_perf_context_print:        load time =     526.62 ms
0.01.289.442 I llama_perf_context_print: prompt eval time =      44.38 ms /     7 tokens (    6.34 ms per token,   157.74 tokens per second)
0.01.289.442 I llama_perf_context_print:        eval time =     703.23 ms /    63 runs   (   11.16 ms per token,    89.59 tokens per second)
0.01.289.443 I llama_perf_context_print:       total time =     750.87 ms /    70 tokens
0.01.289.631 I ggml_metal_free: deallocating

real	0m1.306s
user	0m0.110s
sys	0m0.123s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.737 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.935 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.940 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.942 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.948 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.948 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.948 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.949 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.949 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.950 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.953 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.953 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.953 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.954 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.954 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.956 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.956 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.956 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.757 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.747 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.445 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.446 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.446 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.446 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.447 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.447 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.448 I llama_model_loader: - type  f32:  194 tensors
0.00.024.448 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.448 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.448 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.991 I llm_load_vocab: special tokens cache size = 25
0.00.051.011 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.014 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.014 I llm_load_print_meta: arch             = gptneox
0.00.051.015 I llm_load_print_meta: vocab type       = BPE
0.00.051.015 I llm_load_print_meta: n_vocab          = 50304
0.00.051.015 I llm_load_print_meta: n_merges         = 50009
0.00.051.015 I llm_load_print_meta: vocab_only       = 0
0.00.051.015 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.015 I llm_load_print_meta: n_embd           = 2048
0.00.051.016 I llm_load_print_meta: n_layer          = 24
0.00.051.018 I llm_load_print_meta: n_head           = 16
0.00.051.019 I llm_load_print_meta: n_head_kv        = 16
0.00.051.019 I llm_load_print_meta: n_rot            = 32
0.00.051.019 I llm_load_print_meta: n_swa            = 0
0.00.051.019 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.020 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.020 I llm_load_print_meta: n_gqa            = 1
0.00.051.021 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.024 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.024 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.024 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.025 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.026 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.026 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.027 I llm_load_print_meta: n_ff             = 8192
0.00.051.027 I llm_load_print_meta: n_expert         = 0
0.00.051.027 I llm_load_print_meta: n_expert_used    = 0
0.00.051.027 I llm_load_print_meta: causal attn      = 1
0.00.051.028 I llm_load_print_meta: pooling type     = 0
0.00.051.028 I llm_load_print_meta: rope type        = 2
0.00.051.028 I llm_load_print_meta: rope scaling     = linear
0.00.051.029 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.030 I llm_load_print_meta: freq_scale_train = 1
0.00.051.030 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.030 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.030 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.030 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.030 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.030 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.032 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.033 I llm_load_print_meta: model type       = 1.4B
0.00.051.033 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.033 I llm_load_print_meta: model params     = 1.41 B
0.00.051.034 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.034 I llm_load_print_meta: general.name     = 1.4B
0.00.051.034 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.035 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.035 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.035 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.035 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.036 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.036 I llm_load_print_meta: max token length = 1024
0.00.052.954 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.955 I llm_load_tensors: offloading output layer to GPU
0.00.052.955 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.965 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.966 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.909 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.909 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.910 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.910 I llama_new_context_with_model: n_batch       = 2048
0.00.053.910 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.910 I llama_new_context_with_model: flash_attn    = 0
0.00.053.911 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.911 I llama_new_context_with_model: freq_scale    = 1
0.00.053.911 I ggml_metal_init: allocating
0.00.053.915 I ggml_metal_init: found device: Apple M4
0.00.053.917 I ggml_metal_init: picking default device: Apple M4
0.00.054.506 I ggml_metal_init: using embedded metal library
0.00.056.815 I ggml_metal_init: GPU name:   Apple M4
0.00.056.816 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.817 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.817 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.817 I ggml_metal_init: simdgroup reduction   = true
0.00.056.818 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.818 I ggml_metal_init: has bfloat            = true
0.00.056.818 I ggml_metal_init: use bfloat            = true
0.00.056.818 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.819 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.587 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.892 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.898 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.918 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.938 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.940 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.940 I llama_new_context_with_model: graph nodes  = 967
0.00.087.940 I llama_new_context_with_model: graph splits = 2
0.00.087.956 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.086 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.087 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.389 I main: llama threadpool init, n_threads = 4
0.00.617.428 I 
0.00.617.463 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.617.464 I 
0.00.617.695 I sampler seed: 1234
0.00.617.699 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.617.743 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.617.747 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.617.748 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.376.502 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58532.56 tokens per second)
0.01.376.502 I llama_perf_context_print:        load time =     607.65 ms
0.01.376.504 I llama_perf_context_print: prompt eval time =      47.29 ms /     7 tokens (    6.76 ms per token,   148.04 tokens per second)
0.01.376.505 I llama_perf_context_print:        eval time =     708.45 ms /    63 runs   (   11.25 ms per token,    88.93 tokens per second)
0.01.376.505 I llama_perf_context_print:       total time =     759.12 ms /    70 tokens
0.01.376.687 I ggml_metal_free: deallocating

real	0m1.393s
user	0m0.110s
sys	0m0.144s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.614 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.798 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.803 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.805 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.806 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.807 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.807 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.807 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.809 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.809 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.809 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.810 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.810 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.811 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.812 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.813 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.814 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.814 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.764 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.872 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.783 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.784 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.784 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.785 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.785 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.785 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.786 I llama_model_loader: - type  f32:  194 tensors
0.00.024.786 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.787 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.023 I llm_load_vocab: special tokens cache size = 25
0.00.051.923 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.925 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.926 I llm_load_print_meta: arch             = gptneox
0.00.051.926 I llm_load_print_meta: vocab type       = BPE
0.00.051.926 I llm_load_print_meta: n_vocab          = 50304
0.00.051.926 I llm_load_print_meta: n_merges         = 50009
0.00.051.927 I llm_load_print_meta: vocab_only       = 0
0.00.051.927 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.927 I llm_load_print_meta: n_embd           = 2048
0.00.051.927 I llm_load_print_meta: n_layer          = 24
0.00.051.930 I llm_load_print_meta: n_head           = 16
0.00.051.930 I llm_load_print_meta: n_head_kv        = 16
0.00.051.930 I llm_load_print_meta: n_rot            = 32
0.00.051.931 I llm_load_print_meta: n_swa            = 0
0.00.051.931 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.931 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.932 I llm_load_print_meta: n_gqa            = 1
0.00.051.933 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.933 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.934 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.934 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.935 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.935 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.935 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.936 I llm_load_print_meta: n_ff             = 8192
0.00.051.936 I llm_load_print_meta: n_expert         = 0
0.00.051.936 I llm_load_print_meta: n_expert_used    = 0
0.00.051.936 I llm_load_print_meta: causal attn      = 1
0.00.051.936 I llm_load_print_meta: pooling type     = 0
0.00.051.936 I llm_load_print_meta: rope type        = 2
0.00.051.937 I llm_load_print_meta: rope scaling     = linear
0.00.051.937 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.937 I llm_load_print_meta: freq_scale_train = 1
0.00.051.938 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.938 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.940 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.940 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.940 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.940 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.941 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.941 I llm_load_print_meta: model type       = 1.4B
0.00.051.941 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.942 I llm_load_print_meta: model params     = 1.41 B
0.00.051.943 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.943 I llm_load_print_meta: general.name     = 1.4B
0.00.051.943 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.943 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.943 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.944 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.949 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.949 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.951 I llm_load_print_meta: max token length = 1024
0.00.054.016 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.016 I llm_load_tensors: offloading output layer to GPU
0.00.054.017 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.027 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.028 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.920 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.920 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.921 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.921 I llama_new_context_with_model: n_batch       = 2048
0.00.054.921 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.921 I llama_new_context_with_model: flash_attn    = 0
0.00.054.922 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.922 I llama_new_context_with_model: freq_scale    = 1
0.00.054.923 I ggml_metal_init: allocating
0.00.054.930 I ggml_metal_init: found device: Apple M4
0.00.054.932 I ggml_metal_init: picking default device: Apple M4
0.00.055.531 I ggml_metal_init: using embedded metal library
0.00.057.858 I ggml_metal_init: GPU name:   Apple M4
0.00.057.859 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.860 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.860 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.860 I ggml_metal_init: simdgroup reduction   = true
0.00.057.860 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.861 I ggml_metal_init: has bfloat            = true
0.00.057.861 I ggml_metal_init: use bfloat            = true
0.00.057.861 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.862 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.743 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.616 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.622 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.650 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.594 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.595 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.596 I llama_new_context_with_model: graph nodes  = 967
0.00.089.596 I llama_new_context_with_model: graph splits = 2
0.00.089.607 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.750 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.751 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.697.507 I main: llama threadpool init, n_threads = 4
0.00.697.553 I 
0.00.697.613 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.614 I 
0.00.697.855 I sampler seed: 1234
0.00.697.860 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.697.893 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.697.894 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.697.894 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.546.972 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59513.83 tokens per second)
0.01.546.973 I llama_perf_context_print:        load time =     688.88 ms
0.01.546.973 I llama_perf_context_print: prompt eval time =      51.54 ms /     7 tokens (    7.36 ms per token,   135.82 tokens per second)
0.01.546.974 I llama_perf_context_print:        eval time =     794.53 ms /    63 runs   (   12.61 ms per token,    79.29 tokens per second)
0.01.546.974 I llama_perf_context_print:       total time =     849.47 ms /    70 tokens
0.01.547.167 I ggml_metal_free: deallocating

real	0m1.563s
user	0m0.111s
sys	0m0.157s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.571 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.329 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.333 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.335 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.339 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.340 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.340 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.340 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.343 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.343 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.343 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.344 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.344 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.345 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.346 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.348 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.348 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.348 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.325 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.418 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.331 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.332 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.332 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.332 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.333 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.333 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.333 I llama_model_loader: - type  f32:  194 tensors
0.00.025.334 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.846 I llm_load_vocab: special tokens cache size = 25
0.00.051.765 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.768 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.768 I llm_load_print_meta: arch             = gptneox
0.00.051.769 I llm_load_print_meta: vocab type       = BPE
0.00.051.769 I llm_load_print_meta: n_vocab          = 50304
0.00.051.769 I llm_load_print_meta: n_merges         = 50009
0.00.051.769 I llm_load_print_meta: vocab_only       = 0
0.00.051.770 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.770 I llm_load_print_meta: n_embd           = 2048
0.00.051.770 I llm_load_print_meta: n_layer          = 24
0.00.051.773 I llm_load_print_meta: n_head           = 16
0.00.051.773 I llm_load_print_meta: n_head_kv        = 16
0.00.051.773 I llm_load_print_meta: n_rot            = 32
0.00.051.774 I llm_load_print_meta: n_swa            = 0
0.00.051.774 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.774 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.775 I llm_load_print_meta: n_gqa            = 1
0.00.051.776 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.776 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.777 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.777 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.777 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.777 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.778 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.779 I llm_load_print_meta: n_ff             = 8192
0.00.051.779 I llm_load_print_meta: n_expert         = 0
0.00.051.779 I llm_load_print_meta: n_expert_used    = 0
0.00.051.779 I llm_load_print_meta: causal attn      = 1
0.00.051.780 I llm_load_print_meta: pooling type     = 0
0.00.051.780 I llm_load_print_meta: rope type        = 2
0.00.051.780 I llm_load_print_meta: rope scaling     = linear
0.00.051.780 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.781 I llm_load_print_meta: freq_scale_train = 1
0.00.051.781 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.781 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.781 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.781 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.782 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.782 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.782 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.782 I llm_load_print_meta: model type       = 1.4B
0.00.051.783 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.783 I llm_load_print_meta: model params     = 1.41 B
0.00.051.785 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.785 I llm_load_print_meta: general.name     = 1.4B
0.00.051.785 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.785 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.786 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.786 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.786 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.786 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.786 I llm_load_print_meta: max token length = 1024
0.00.053.822 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.823 I llm_load_tensors: offloading output layer to GPU
0.00.053.823 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.833 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.834 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.734 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.735 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.735 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.735 I llama_new_context_with_model: n_batch       = 2048
0.00.054.736 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.736 I llama_new_context_with_model: flash_attn    = 0
0.00.054.736 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.736 I llama_new_context_with_model: freq_scale    = 1
0.00.054.737 I ggml_metal_init: allocating
0.00.054.740 I ggml_metal_init: found device: Apple M4
0.00.054.742 I ggml_metal_init: picking default device: Apple M4
0.00.055.332 I ggml_metal_init: using embedded metal library
0.00.057.677 I ggml_metal_init: GPU name:   Apple M4
0.00.057.679 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.679 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.679 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.679 I ggml_metal_init: simdgroup reduction   = true
0.00.057.680 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.680 I ggml_metal_init: has bfloat            = true
0.00.057.680 I ggml_metal_init: use bfloat            = true
0.00.057.680 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.681 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.552 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.410 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.417 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.438 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.480 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.481 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.481 I llama_new_context_with_model: graph nodes  = 967
0.00.088.482 I llama_new_context_with_model: graph splits = 2
0.00.088.496 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.631 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.632 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.931 I main: llama threadpool init, n_threads = 4
0.00.750.968 I 
0.00.751.028 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.030 I 
0.00.751.255 I sampler seed: 1234
0.00.751.263 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.305 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.309 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.310 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.634.187 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50070.52 tokens per second)
0.01.634.188 I llama_perf_context_print:        load time =     741.36 ms
0.01.634.188 I llama_perf_context_print: prompt eval time =      54.43 ms /     7 tokens (    7.78 ms per token,   128.61 tokens per second)
0.01.634.190 I llama_perf_context_print:        eval time =     825.90 ms /    63 runs   (   13.11 ms per token,    76.28 tokens per second)
0.01.634.190 I llama_perf_context_print:       total time =     883.26 ms /    70 tokens
0.01.634.430 I ggml_metal_free: deallocating

real	0m1.652s
user	0m0.111s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.539 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.112 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.839 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.852 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.855 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.857 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.857 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.858 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.859 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.861 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.861 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.862 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.863 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.863 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.864 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.865 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.870 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.870 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.871 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.996 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.428 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.973 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.975 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.975 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.976 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.976 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.977 I llama_model_loader: - type  f32:  194 tensors
0.00.053.978 I llama_model_loader: - type  f16:   98 tensors
0.00.084.976 I llm_load_vocab: special tokens cache size = 25
0.00.091.804 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.807 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.808 I llm_load_print_meta: arch             = gptneox
0.00.091.808 I llm_load_print_meta: vocab type       = BPE
0.00.091.808 I llm_load_print_meta: n_vocab          = 50304
0.00.091.808 I llm_load_print_meta: n_merges         = 50009
0.00.091.808 I llm_load_print_meta: vocab_only       = 0
0.00.091.809 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.809 I llm_load_print_meta: n_embd           = 2048
0.00.091.809 I llm_load_print_meta: n_layer          = 24
0.00.091.812 I llm_load_print_meta: n_head           = 16
0.00.091.813 I llm_load_print_meta: n_head_kv        = 16
0.00.091.813 I llm_load_print_meta: n_rot            = 32
0.00.091.813 I llm_load_print_meta: n_swa            = 0
0.00.091.813 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.813 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.814 I llm_load_print_meta: n_gqa            = 1
0.00.091.815 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.815 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.816 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.819 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.819 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.819 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.819 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.820 I llm_load_print_meta: n_ff             = 8192
0.00.091.820 I llm_load_print_meta: n_expert         = 0
0.00.091.820 I llm_load_print_meta: n_expert_used    = 0
0.00.091.820 I llm_load_print_meta: causal attn      = 1
0.00.091.820 I llm_load_print_meta: pooling type     = 0
0.00.091.821 I llm_load_print_meta: rope type        = 2
0.00.091.821 I llm_load_print_meta: rope scaling     = linear
0.00.091.821 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.821 I llm_load_print_meta: freq_scale_train = 1
0.00.091.822 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.822 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.822 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.822 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.822 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.822 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.822 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.826 I llm_load_print_meta: model type       = 1.4B
0.00.091.826 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.091.827 I llm_load_print_meta: model params     = 1.41 B
0.00.091.827 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.827 I llm_load_print_meta: general.name     = 1.4B
0.00.091.828 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.828 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.829 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.830 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.830 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.091.830 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.830 I llm_load_print_meta: max token length = 1024
0.00.094.412 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.413 I llm_load_tensors: offloading output layer to GPU
0.00.094.413 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.423 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.424 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.095.359 I llama_new_context_with_model: n_seq_max     = 1
0.00.095.360 I llama_new_context_with_model: n_ctx         = 128
0.00.095.361 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.095.361 I llama_new_context_with_model: n_batch       = 128
0.00.095.361 I llama_new_context_with_model: n_ubatch      = 128
0.00.095.361 I llama_new_context_with_model: flash_attn    = 0
0.00.095.361 I llama_new_context_with_model: freq_base     = 10000.0
0.00.095.362 I llama_new_context_with_model: freq_scale    = 1
0.00.095.362 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.362 I ggml_metal_init: allocating
0.00.095.369 I ggml_metal_init: found device: Apple M4
0.00.095.373 I ggml_metal_init: picking default device: Apple M4
0.00.095.995 I ggml_metal_init: using embedded metal library
0.00.098.574 I ggml_metal_init: GPU name:   Apple M4
0.00.098.576 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.576 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.576 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.577 I ggml_metal_init: simdgroup reduction   = true
0.00.098.577 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.577 I ggml_metal_init: has bfloat            = true
0.00.098.577 I ggml_metal_init: use bfloat            = true
0.00.098.577 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.578 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.123 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.109.455 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.460 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.474 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.396 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.110.397 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.110.398 I llama_new_context_with_model: graph nodes  = 967
0.00.110.398 I llama_new_context_with_model: graph splits = 2
0.00.110.410 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.110.411 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.165.487 I 
0.01.165.581 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.165.638 I perplexity: tokenizing the input ..
0.01.178.507 I perplexity: tokenization took 12.868 ms
0.01.178.517 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.299.829 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.301.676 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.301.703 I llama_perf_context_print:        load time =    1142.36 ms
0.01.301.707 I llama_perf_context_print: prompt eval time =     121.00 ms /   128 tokens (    0.95 ms per token,  1057.89 tokens per second)
0.01.301.708 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.301.709 I llama_perf_context_print:       total time =     136.22 ms /   129 tokens
0.01.302.493 I ggml_metal_free: deallocating

real	0m1.498s
user	0m0.126s
sys	0m0.214s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.140 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.697 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.817 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.824 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.832 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.833 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.833 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.834 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.834 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.835 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.836 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.836 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.837 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.837 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.838 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.838 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.840 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.841 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.841 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.474 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.303 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.844 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.846 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.846 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.847 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.847 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.847 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.848 I llama_model_loader: - type  f32:  194 tensors
0.00.037.848 I llama_model_loader: - type q8_0:   98 tensors
0.00.066.074 I llm_load_vocab: special tokens cache size = 25
0.00.072.369 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.372 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.372 I llm_load_print_meta: arch             = gptneox
0.00.072.372 I llm_load_print_meta: vocab type       = BPE
0.00.072.373 I llm_load_print_meta: n_vocab          = 50304
0.00.072.373 I llm_load_print_meta: n_merges         = 50009
0.00.072.373 I llm_load_print_meta: vocab_only       = 0
0.00.072.373 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.373 I llm_load_print_meta: n_embd           = 2048
0.00.072.373 I llm_load_print_meta: n_layer          = 24
0.00.072.377 I llm_load_print_meta: n_head           = 16
0.00.072.378 I llm_load_print_meta: n_head_kv        = 16
0.00.072.378 I llm_load_print_meta: n_rot            = 32
0.00.072.378 I llm_load_print_meta: n_swa            = 0
0.00.072.378 I llm_load_print_meta: n_embd_head_k    = 128
0.00.072.378 I llm_load_print_meta: n_embd_head_v    = 128
0.00.072.379 I llm_load_print_meta: n_gqa            = 1
0.00.072.380 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.072.380 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.072.381 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.072.381 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.072.381 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.072.381 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.072.382 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.072.382 I llm_load_print_meta: n_ff             = 8192
0.00.072.382 I llm_load_print_meta: n_expert         = 0
0.00.072.382 I llm_load_print_meta: n_expert_used    = 0
0.00.072.383 I llm_load_print_meta: causal attn      = 1
0.00.072.383 I llm_load_print_meta: pooling type     = 0
0.00.072.383 I llm_load_print_meta: rope type        = 2
0.00.072.383 I llm_load_print_meta: rope scaling     = linear
0.00.072.386 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.072.386 I llm_load_print_meta: freq_scale_train = 1
0.00.072.386 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.072.386 I llm_load_print_meta: rope_finetuned   = unknown
0.00.072.386 I llm_load_print_meta: ssm_d_conv       = 0
0.00.072.386 I llm_load_print_meta: ssm_d_inner      = 0
0.00.072.387 I llm_load_print_meta: ssm_d_state      = 0
0.00.072.387 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.072.387 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.072.387 I llm_load_print_meta: model type       = 1.4B
0.00.072.387 I llm_load_print_meta: model ftype      = Q8_0
0.00.072.388 I llm_load_print_meta: model params     = 1.41 B
0.00.072.388 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.072.388 I llm_load_print_meta: general.name     = 1.4B
0.00.072.388 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.072.389 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.072.389 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.072.389 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.072.393 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.072.393 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.072.393 I llm_load_print_meta: max token length = 1024
0.00.074.848 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.074.848 I llm_load_tensors: offloading output layer to GPU
0.00.074.849 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.074.860 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.074.861 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.075.810 I llama_new_context_with_model: n_seq_max     = 1
0.00.075.811 I llama_new_context_with_model: n_ctx         = 128
0.00.075.811 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.075.812 I llama_new_context_with_model: n_batch       = 128
0.00.075.812 I llama_new_context_with_model: n_ubatch      = 128
0.00.075.812 I llama_new_context_with_model: flash_attn    = 0
0.00.075.812 I llama_new_context_with_model: freq_base     = 10000.0
0.00.075.812 I llama_new_context_with_model: freq_scale    = 1
0.00.075.813 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.075.813 I ggml_metal_init: allocating
0.00.075.817 I ggml_metal_init: found device: Apple M4
0.00.075.819 I ggml_metal_init: picking default device: Apple M4
0.00.076.514 I ggml_metal_init: using embedded metal library
0.00.079.132 I ggml_metal_init: GPU name:   Apple M4
0.00.079.133 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.134 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.134 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.134 I ggml_metal_init: simdgroup reduction   = true
0.00.079.134 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.135 I ggml_metal_init: has bfloat            = true
0.00.079.135 I ggml_metal_init: use bfloat            = true
0.00.079.135 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.136 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.663 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.090.032 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.090.035 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.090.050 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.002 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.091.003 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.091.004 I llama_new_context_with_model: graph nodes  = 967
0.00.091.004 I llama_new_context_with_model: graph splits = 2
0.00.091.017 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.091.018 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.942.427 I 
0.00.942.468 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.942.497 I perplexity: tokenizing the input ..
0.00.951.561 I perplexity: tokenization took 9.063 ms
0.00.951.565 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.076.810 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.078.052 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.078.072 I llama_perf_context_print:        load time =     929.72 ms
0.01.078.078 I llama_perf_context_print: prompt eval time =     124.99 ms /   128 tokens (    0.98 ms per token,  1024.08 tokens per second)
0.01.078.079 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.078.079 I llama_perf_context_print:       total time =     135.65 ms /   129 tokens
0.01.078.528 I ggml_metal_free: deallocating

real	0m1.098s
user	0m0.101s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.193 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.168 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.172 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.174 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.174 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.178 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.178 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.179 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.180 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.180 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.180 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.181 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.181 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.181 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.182 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.183 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.183 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.184 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.981 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.011 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.897 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.898 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.898 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.899 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.899 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.899 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.900 I llama_model_loader: - type  f32:  194 tensors
0.00.024.900 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.900 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.318 I llm_load_vocab: special tokens cache size = 25
0.00.051.221 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.225 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.225 I llm_load_print_meta: arch             = gptneox
0.00.051.226 I llm_load_print_meta: vocab type       = BPE
0.00.051.226 I llm_load_print_meta: n_vocab          = 50304
0.00.051.226 I llm_load_print_meta: n_merges         = 50009
0.00.051.230 I llm_load_print_meta: vocab_only       = 0
0.00.051.231 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.231 I llm_load_print_meta: n_embd           = 2048
0.00.051.231 I llm_load_print_meta: n_layer          = 24
0.00.051.234 I llm_load_print_meta: n_head           = 16
0.00.051.236 I llm_load_print_meta: n_head_kv        = 16
0.00.051.237 I llm_load_print_meta: n_rot            = 32
0.00.051.237 I llm_load_print_meta: n_swa            = 0
0.00.051.237 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.237 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.238 I llm_load_print_meta: n_gqa            = 1
0.00.051.239 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.239 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.240 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.241 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.241 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.241 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.241 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.242 I llm_load_print_meta: n_ff             = 8192
0.00.051.242 I llm_load_print_meta: n_expert         = 0
0.00.051.242 I llm_load_print_meta: n_expert_used    = 0
0.00.051.242 I llm_load_print_meta: causal attn      = 1
0.00.051.242 I llm_load_print_meta: pooling type     = 0
0.00.051.243 I llm_load_print_meta: rope type        = 2
0.00.051.243 I llm_load_print_meta: rope scaling     = linear
0.00.051.243 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.244 I llm_load_print_meta: freq_scale_train = 1
0.00.051.244 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.244 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.244 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.244 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.245 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.245 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.245 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.245 I llm_load_print_meta: model type       = 1.4B
0.00.051.245 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.246 I llm_load_print_meta: model params     = 1.41 B
0.00.051.246 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.247 I llm_load_print_meta: general.name     = 1.4B
0.00.051.247 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.247 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.247 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.247 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.247 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.248 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.248 I llm_load_print_meta: max token length = 1024
0.00.053.276 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.277 I llm_load_tensors: offloading output layer to GPU
0.00.053.277 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.287 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.288 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.190 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.190 I llama_new_context_with_model: n_ctx         = 128
0.00.054.191 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.191 I llama_new_context_with_model: n_batch       = 128
0.00.054.191 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.191 I llama_new_context_with_model: flash_attn    = 0
0.00.054.191 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.192 I llama_new_context_with_model: freq_scale    = 1
0.00.054.192 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.193 I ggml_metal_init: allocating
0.00.054.196 I ggml_metal_init: found device: Apple M4
0.00.054.198 I ggml_metal_init: picking default device: Apple M4
0.00.054.778 I ggml_metal_init: using embedded metal library
0.00.057.111 I ggml_metal_init: GPU name:   Apple M4
0.00.057.112 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.113 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.113 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.113 I ggml_metal_init: simdgroup reduction   = true
0.00.057.114 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.114 I ggml_metal_init: has bfloat            = true
0.00.057.114 I ggml_metal_init: use bfloat            = true
0.00.057.114 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.115 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.165 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.507 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.511 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.528 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.426 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.427 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.428 I llama_new_context_with_model: graph nodes  = 967
0.00.069.428 I llama_new_context_with_model: graph splits = 2
0.00.069.440 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.441 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.933 I 
0.00.615.972 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.985 I perplexity: tokenizing the input ..
0.00.624.085 I perplexity: tokenization took 8.099 ms
0.00.624.088 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.746.369 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.747.530 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.747.541 I llama_perf_context_print:        load time =     605.73 ms
0.00.747.542 I llama_perf_context_print: prompt eval time =     122.05 ms /   128 tokens (    0.95 ms per token,  1048.78 tokens per second)
0.00.747.543 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.747.543 I llama_perf_context_print:       total time =     131.61 ms /   129 tokens
0.00.748.011 I ggml_metal_free: deallocating

real	0m0.763s
user	0m0.078s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.842 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.483 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.487 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.489 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.490 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.490 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.490 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.491 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.492 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.492 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.492 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.493 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.493 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.493 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.494 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.498 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.498 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.498 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.351 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.438 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.292 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.294 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.294 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.294 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.295 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.295 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.295 I llama_model_loader: - type  f32:  194 tensors
0.00.023.296 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.296 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.677 I llm_load_vocab: special tokens cache size = 25
0.00.049.536 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.539 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.539 I llm_load_print_meta: arch             = gptneox
0.00.049.540 I llm_load_print_meta: vocab type       = BPE
0.00.049.540 I llm_load_print_meta: n_vocab          = 50304
0.00.049.540 I llm_load_print_meta: n_merges         = 50009
0.00.049.540 I llm_load_print_meta: vocab_only       = 0
0.00.049.540 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.541 I llm_load_print_meta: n_embd           = 2048
0.00.049.541 I llm_load_print_meta: n_layer          = 24
0.00.049.543 I llm_load_print_meta: n_head           = 16
0.00.049.544 I llm_load_print_meta: n_head_kv        = 16
0.00.049.544 I llm_load_print_meta: n_rot            = 32
0.00.049.545 I llm_load_print_meta: n_swa            = 0
0.00.049.545 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.547 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.548 I llm_load_print_meta: n_gqa            = 1
0.00.049.549 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.549 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.550 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.550 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.551 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.551 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.551 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.553 I llm_load_print_meta: n_ff             = 8192
0.00.049.553 I llm_load_print_meta: n_expert         = 0
0.00.049.553 I llm_load_print_meta: n_expert_used    = 0
0.00.049.553 I llm_load_print_meta: causal attn      = 1
0.00.049.553 I llm_load_print_meta: pooling type     = 0
0.00.049.553 I llm_load_print_meta: rope type        = 2
0.00.049.554 I llm_load_print_meta: rope scaling     = linear
0.00.049.556 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.556 I llm_load_print_meta: freq_scale_train = 1
0.00.049.556 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.556 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.557 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.557 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.558 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.558 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.558 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.558 I llm_load_print_meta: model type       = 1.4B
0.00.049.558 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.559 I llm_load_print_meta: model params     = 1.41 B
0.00.049.563 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.563 I llm_load_print_meta: general.name     = 1.4B
0.00.049.563 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.564 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.564 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.564 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.565 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.565 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.565 I llm_load_print_meta: max token length = 1024
0.00.051.537 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.538 I llm_load_tensors: offloading output layer to GPU
0.00.051.538 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.548 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.549 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.437 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.438 I llama_new_context_with_model: n_ctx         = 128
0.00.052.438 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.438 I llama_new_context_with_model: n_batch       = 128
0.00.052.438 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.438 I llama_new_context_with_model: flash_attn    = 0
0.00.052.439 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.439 I llama_new_context_with_model: freq_scale    = 1
0.00.052.439 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.440 I ggml_metal_init: allocating
0.00.052.443 I ggml_metal_init: found device: Apple M4
0.00.052.445 I ggml_metal_init: picking default device: Apple M4
0.00.053.020 I ggml_metal_init: using embedded metal library
0.00.055.361 I ggml_metal_init: GPU name:   Apple M4
0.00.055.362 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.362 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.363 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.363 I ggml_metal_init: simdgroup reduction   = true
0.00.055.363 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.363 I ggml_metal_init: has bfloat            = true
0.00.055.364 I ggml_metal_init: use bfloat            = true
0.00.055.364 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.365 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.116 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.353 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.356 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.370 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.261 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.262 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.262 I llama_new_context_with_model: graph nodes  = 967
0.00.067.262 I llama_new_context_with_model: graph splits = 2
0.00.067.276 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.277 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.444 I 
0.00.680.488 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.499 I perplexity: tokenizing the input ..
0.00.688.505 I perplexity: tokenization took 8.004 ms
0.00.688.508 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.811.212 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.812.387 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.812.406 I llama_perf_context_print:        load time =     671.60 ms
0.00.812.407 I llama_perf_context_print: prompt eval time =     122.48 ms /   128 tokens (    0.96 ms per token,  1045.08 tokens per second)
0.00.812.408 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.812.409 I llama_perf_context_print:       total time =     131.96 ms /   129 tokens
0.00.812.928 I ggml_metal_free: deallocating

real	0m0.827s
user	0m0.078s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.240 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.254 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.258 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.260 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.261 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.261 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.261 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.262 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.263 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.263 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.263 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.264 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.264 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.264 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.265 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.267 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.268 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.268 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.246 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.325 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.301 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.303 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.303 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.303 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.304 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.304 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.305 I llama_model_loader: - type  f32:  194 tensors
0.00.024.305 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.305 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.519 I llm_load_vocab: special tokens cache size = 25
0.00.051.538 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.541 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.541 I llm_load_print_meta: arch             = gptneox
0.00.051.542 I llm_load_print_meta: vocab type       = BPE
0.00.051.542 I llm_load_print_meta: n_vocab          = 50304
0.00.051.542 I llm_load_print_meta: n_merges         = 50009
0.00.051.542 I llm_load_print_meta: vocab_only       = 0
0.00.051.542 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.543 I llm_load_print_meta: n_embd           = 2048
0.00.051.543 I llm_load_print_meta: n_layer          = 24
0.00.051.546 I llm_load_print_meta: n_head           = 16
0.00.051.547 I llm_load_print_meta: n_head_kv        = 16
0.00.051.547 I llm_load_print_meta: n_rot            = 32
0.00.051.547 I llm_load_print_meta: n_swa            = 0
0.00.051.547 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.547 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.548 I llm_load_print_meta: n_gqa            = 1
0.00.051.549 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.551 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.552 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.552 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.552 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.553 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.553 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.554 I llm_load_print_meta: n_ff             = 8192
0.00.051.554 I llm_load_print_meta: n_expert         = 0
0.00.051.554 I llm_load_print_meta: n_expert_used    = 0
0.00.051.554 I llm_load_print_meta: causal attn      = 1
0.00.051.554 I llm_load_print_meta: pooling type     = 0
0.00.051.554 I llm_load_print_meta: rope type        = 2
0.00.051.555 I llm_load_print_meta: rope scaling     = linear
0.00.051.555 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.555 I llm_load_print_meta: freq_scale_train = 1
0.00.051.555 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.556 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.556 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.556 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.561 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.561 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.561 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.561 I llm_load_print_meta: model type       = 1.4B
0.00.051.561 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.562 I llm_load_print_meta: model params     = 1.41 B
0.00.051.564 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.564 I llm_load_print_meta: general.name     = 1.4B
0.00.051.564 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.564 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.565 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.565 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.565 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.565 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.566 I llm_load_print_meta: max token length = 1024
0.00.053.589 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.590 I llm_load_tensors: offloading output layer to GPU
0.00.053.590 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.601 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.602 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.493 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.494 I llama_new_context_with_model: n_ctx         = 128
0.00.054.494 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.494 I llama_new_context_with_model: n_batch       = 128
0.00.054.495 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.495 I llama_new_context_with_model: flash_attn    = 0
0.00.054.495 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.495 I llama_new_context_with_model: freq_scale    = 1
0.00.054.496 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.496 I ggml_metal_init: allocating
0.00.054.502 I ggml_metal_init: found device: Apple M4
0.00.054.504 I ggml_metal_init: picking default device: Apple M4
0.00.055.048 I ggml_metal_init: using embedded metal library
0.00.057.402 I ggml_metal_init: GPU name:   Apple M4
0.00.057.403 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.403 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.404 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.404 I ggml_metal_init: simdgroup reduction   = true
0.00.057.404 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.404 I ggml_metal_init: has bfloat            = true
0.00.057.404 I ggml_metal_init: use bfloat            = true
0.00.057.405 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.405 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.867 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.141 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.148 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.166 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.047 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.048 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.048 I llama_new_context_with_model: graph nodes  = 967
0.00.069.048 I llama_new_context_with_model: graph splits = 2
0.00.069.061 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.377 I 
0.00.690.409 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.419 I perplexity: tokenizing the input ..
0.00.697.915 I perplexity: tokenization took 7.494 ms
0.00.697.919 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.833.399 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.834.686 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.834.701 I llama_perf_context_print:        load time =     681.13 ms
0.00.834.702 I llama_perf_context_print: prompt eval time =     135.25 ms /   128 tokens (    1.06 ms per token,   946.40 tokens per second)
0.00.834.703 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.834.703 I llama_perf_context_print:       total time =     144.33 ms /   129 tokens
0.00.835.013 I ggml_metal_free: deallocating

real	0m0.850s
user	0m0.079s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.257 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.991 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.996 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.997 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.997 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.998 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.998 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.998 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.999 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.000 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.000 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.000 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.001 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.001 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.001 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.005 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.005 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.006 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.962 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.042 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.964 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.965 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.965 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.965 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.966 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.966 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.967 I llama_model_loader: - type  f32:  194 tensors
0.00.023.967 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.967 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.113 I llm_load_vocab: special tokens cache size = 25
0.00.050.983 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.986 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.986 I llm_load_print_meta: arch             = gptneox
0.00.050.987 I llm_load_print_meta: vocab type       = BPE
0.00.050.987 I llm_load_print_meta: n_vocab          = 50304
0.00.050.987 I llm_load_print_meta: n_merges         = 50009
0.00.050.987 I llm_load_print_meta: vocab_only       = 0
0.00.050.987 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.987 I llm_load_print_meta: n_embd           = 2048
0.00.050.988 I llm_load_print_meta: n_layer          = 24
0.00.050.991 I llm_load_print_meta: n_head           = 16
0.00.050.992 I llm_load_print_meta: n_head_kv        = 16
0.00.050.992 I llm_load_print_meta: n_rot            = 32
0.00.050.994 I llm_load_print_meta: n_swa            = 0
0.00.050.994 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.995 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.995 I llm_load_print_meta: n_gqa            = 1
0.00.050.996 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.997 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.999 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.999 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.999 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.999 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.999 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.000 I llm_load_print_meta: n_ff             = 8192
0.00.051.000 I llm_load_print_meta: n_expert         = 0
0.00.051.000 I llm_load_print_meta: n_expert_used    = 0
0.00.051.001 I llm_load_print_meta: causal attn      = 1
0.00.051.001 I llm_load_print_meta: pooling type     = 0
0.00.051.001 I llm_load_print_meta: rope type        = 2
0.00.051.001 I llm_load_print_meta: rope scaling     = linear
0.00.051.003 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.007 I llm_load_print_meta: freq_scale_train = 1
0.00.051.007 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.008 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.008 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.008 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.008 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.008 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.008 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.009 I llm_load_print_meta: model type       = 1.4B
0.00.051.009 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.009 I llm_load_print_meta: model params     = 1.41 B
0.00.051.010 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.010 I llm_load_print_meta: general.name     = 1.4B
0.00.051.010 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.010 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.010 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.011 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.011 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.011 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.011 I llm_load_print_meta: max token length = 1024
0.00.053.145 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.146 I llm_load_tensors: offloading output layer to GPU
0.00.053.146 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.157 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.158 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.148 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.149 I llama_new_context_with_model: n_ctx         = 128
0.00.054.149 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.149 I llama_new_context_with_model: n_batch       = 128
0.00.054.149 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.149 I llama_new_context_with_model: flash_attn    = 0
0.00.054.150 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.150 I llama_new_context_with_model: freq_scale    = 1
0.00.054.150 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.151 I ggml_metal_init: allocating
0.00.054.154 I ggml_metal_init: found device: Apple M4
0.00.054.156 I ggml_metal_init: picking default device: Apple M4
0.00.054.736 I ggml_metal_init: using embedded metal library
0.00.057.094 I ggml_metal_init: GPU name:   Apple M4
0.00.057.096 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.096 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.096 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.096 I ggml_metal_init: simdgroup reduction   = true
0.00.057.097 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.097 I ggml_metal_init: has bfloat            = true
0.00.057.097 I ggml_metal_init: use bfloat            = true
0.00.057.099 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.100 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.116 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.375 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.377 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.392 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.305 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.306 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.306 I llama_new_context_with_model: graph nodes  = 967
0.00.069.306 I llama_new_context_with_model: graph splits = 2
0.00.069.319 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.320 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.550 I 
0.00.648.632 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.648.655 I perplexity: tokenizing the input ..
0.00.656.861 I perplexity: tokenization took 8.204 ms
0.00.656.866 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.791.302 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.792.492 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.792.510 I llama_perf_context_print:        load time =     639.28 ms
0.00.792.510 I llama_perf_context_print: prompt eval time =     134.21 ms /   128 tokens (    1.05 ms per token,   953.73 tokens per second)
0.00.792.511 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.792.512 I llama_perf_context_print:       total time =     143.96 ms /   129 tokens
0.00.792.879 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.080s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.808 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.521 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.526 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.527 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.528 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.528 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.528 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.529 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.530 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.530 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.530 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.531 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.531 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.531 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.532 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.534 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.535 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.535 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.391 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.505 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.434 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.435 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.436 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.436 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.436 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.437 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.437 I llama_model_loader: - type  f32:  194 tensors
0.00.024.438 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.438 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.438 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.647 I llm_load_vocab: special tokens cache size = 25
0.00.051.407 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.410 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.410 I llm_load_print_meta: arch             = gptneox
0.00.051.411 I llm_load_print_meta: vocab type       = BPE
0.00.051.411 I llm_load_print_meta: n_vocab          = 50304
0.00.051.411 I llm_load_print_meta: n_merges         = 50009
0.00.051.411 I llm_load_print_meta: vocab_only       = 0
0.00.051.411 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.411 I llm_load_print_meta: n_embd           = 2048
0.00.051.412 I llm_load_print_meta: n_layer          = 24
0.00.051.415 I llm_load_print_meta: n_head           = 16
0.00.051.416 I llm_load_print_meta: n_head_kv        = 16
0.00.051.416 I llm_load_print_meta: n_rot            = 32
0.00.051.417 I llm_load_print_meta: n_swa            = 0
0.00.051.417 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.417 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.418 I llm_load_print_meta: n_gqa            = 1
0.00.051.419 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.419 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.420 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.420 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.420 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.420 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.421 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.423 I llm_load_print_meta: n_ff             = 8192
0.00.051.423 I llm_load_print_meta: n_expert         = 0
0.00.051.423 I llm_load_print_meta: n_expert_used    = 0
0.00.051.423 I llm_load_print_meta: causal attn      = 1
0.00.051.423 I llm_load_print_meta: pooling type     = 0
0.00.051.423 I llm_load_print_meta: rope type        = 2
0.00.051.424 I llm_load_print_meta: rope scaling     = linear
0.00.051.424 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.425 I llm_load_print_meta: freq_scale_train = 1
0.00.051.425 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.426 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.426 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.426 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.426 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.426 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.426 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.427 I llm_load_print_meta: model type       = 1.4B
0.00.051.427 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.427 I llm_load_print_meta: model params     = 1.41 B
0.00.051.432 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.432 I llm_load_print_meta: general.name     = 1.4B
0.00.051.432 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.433 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.433 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.433 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.434 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.434 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.434 I llm_load_print_meta: max token length = 1024
0.00.053.380 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.381 I llm_load_tensors: offloading output layer to GPU
0.00.053.381 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.392 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.393 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.296 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.297 I llama_new_context_with_model: n_ctx         = 128
0.00.054.297 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.298 I llama_new_context_with_model: n_batch       = 128
0.00.054.298 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.298 I llama_new_context_with_model: flash_attn    = 0
0.00.054.298 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.299 I llama_new_context_with_model: freq_scale    = 1
0.00.054.299 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.300 I ggml_metal_init: allocating
0.00.054.307 I ggml_metal_init: found device: Apple M4
0.00.054.309 I ggml_metal_init: picking default device: Apple M4
0.00.054.864 I ggml_metal_init: using embedded metal library
0.00.057.178 I ggml_metal_init: GPU name:   Apple M4
0.00.057.179 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.179 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.180 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.180 I ggml_metal_init: simdgroup reduction   = true
0.00.057.180 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.180 I ggml_metal_init: has bfloat            = true
0.00.057.180 I ggml_metal_init: use bfloat            = true
0.00.057.181 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.181 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.703 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.035 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.040 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.055 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.923 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.924 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.925 I llama_new_context_with_model: graph nodes  = 967
0.00.068.925 I llama_new_context_with_model: graph splits = 2
0.00.068.937 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.938 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.387.664 I 
0.00.387.704 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.387.716 I perplexity: tokenizing the input ..
0.00.395.609 I perplexity: tokenization took 7.892 ms
0.00.395.612 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.528.440 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.529.695 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.529.709 I llama_perf_context_print:        load time =     377.85 ms
0.00.529.710 I llama_perf_context_print: prompt eval time =     132.59 ms /   128 tokens (    1.04 ms per token,   965.39 tokens per second)
0.00.529.710 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.529.711 I llama_perf_context_print:       total time =     142.05 ms /   129 tokens
0.00.530.241 I ggml_metal_free: deallocating

real	0m0.546s
user	0m0.079s
sys	0m0.073s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.778 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.771 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.776 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.778 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.778 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.778 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.779 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.779 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.780 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.780 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.781 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.781 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.781 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.782 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.782 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.784 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.784 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.784 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.646 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.768 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.607 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.608 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.608 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.609 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.609 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.609 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.610 I llama_model_loader: - type  f32:  194 tensors
0.00.023.610 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.610 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.611 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.611 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.225 I llm_load_vocab: special tokens cache size = 25
0.00.051.231 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.236 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.236 I llm_load_print_meta: arch             = gptneox
0.00.051.236 I llm_load_print_meta: vocab type       = BPE
0.00.051.237 I llm_load_print_meta: n_vocab          = 50304
0.00.051.239 I llm_load_print_meta: n_merges         = 50009
0.00.051.240 I llm_load_print_meta: vocab_only       = 0
0.00.051.240 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.240 I llm_load_print_meta: n_embd           = 2048
0.00.051.240 I llm_load_print_meta: n_layer          = 24
0.00.051.244 I llm_load_print_meta: n_head           = 16
0.00.051.245 I llm_load_print_meta: n_head_kv        = 16
0.00.051.246 I llm_load_print_meta: n_rot            = 32
0.00.051.247 I llm_load_print_meta: n_swa            = 0
0.00.051.247 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.247 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.248 I llm_load_print_meta: n_gqa            = 1
0.00.051.249 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.250 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.250 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.251 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.251 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.251 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.251 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.252 I llm_load_print_meta: n_ff             = 8192
0.00.051.252 I llm_load_print_meta: n_expert         = 0
0.00.051.252 I llm_load_print_meta: n_expert_used    = 0
0.00.051.252 I llm_load_print_meta: causal attn      = 1
0.00.051.252 I llm_load_print_meta: pooling type     = 0
0.00.051.252 I llm_load_print_meta: rope type        = 2
0.00.051.253 I llm_load_print_meta: rope scaling     = linear
0.00.051.253 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.253 I llm_load_print_meta: freq_scale_train = 1
0.00.051.253 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.254 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.254 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.254 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.254 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.254 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.254 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.254 I llm_load_print_meta: model type       = 1.4B
0.00.051.255 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.255 I llm_load_print_meta: model params     = 1.41 B
0.00.051.256 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.257 I llm_load_print_meta: general.name     = 1.4B
0.00.051.257 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.257 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.257 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.258 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.258 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.258 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.258 I llm_load_print_meta: max token length = 1024
0.00.053.098 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.099 I llm_load_tensors: offloading output layer to GPU
0.00.053.099 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.110 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.112 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.972 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.973 I llama_new_context_with_model: n_ctx         = 128
0.00.053.973 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.973 I llama_new_context_with_model: n_batch       = 128
0.00.053.974 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.974 I llama_new_context_with_model: flash_attn    = 0
0.00.053.974 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.975 I llama_new_context_with_model: freq_scale    = 1
0.00.053.975 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.976 I ggml_metal_init: allocating
0.00.053.984 I ggml_metal_init: found device: Apple M4
0.00.053.986 I ggml_metal_init: picking default device: Apple M4
0.00.054.605 I ggml_metal_init: using embedded metal library
0.00.057.006 I ggml_metal_init: GPU name:   Apple M4
0.00.057.007 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.008 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.008 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.009 I ggml_metal_init: simdgroup reduction   = true
0.00.057.009 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.009 I ggml_metal_init: has bfloat            = true
0.00.057.009 I ggml_metal_init: use bfloat            = true
0.00.057.010 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.010 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.031 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.305 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.310 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.325 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.237 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.238 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.239 I llama_new_context_with_model: graph nodes  = 967
0.00.069.239 I llama_new_context_with_model: graph splits = 2
0.00.069.252 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.252 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.459.494 I 
0.00.459.528 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.459.538 I perplexity: tokenizing the input ..
0.00.466.860 I perplexity: tokenization took 7.32 ms
0.00.466.866 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.598.374 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.599.673 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.599.689 I llama_perf_context_print:        load time =     450.71 ms
0.00.599.691 I llama_perf_context_print: prompt eval time =     131.28 ms /   128 tokens (    1.03 ms per token,   975.05 tokens per second)
0.00.599.692 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.599.692 I llama_perf_context_print:       total time =     140.20 ms /   129 tokens
0.00.600.091 I ggml_metal_free: deallocating

real	0m0.614s
user	0m0.079s
sys	0m0.069s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.279 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.343 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.347 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.352 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.353 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.353 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.354 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.354 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.355 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.355 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.355 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.356 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.356 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.356 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.357 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.358 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.358 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.359 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.127 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.248 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.105 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.106 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.106 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.107 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.107 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.107 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.108 I llama_model_loader: - type  f32:  194 tensors
0.00.024.108 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.108 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.109 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.457 I llm_load_vocab: special tokens cache size = 25
0.00.051.392 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.398 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.398 I llm_load_print_meta: arch             = gptneox
0.00.051.399 I llm_load_print_meta: vocab type       = BPE
0.00.051.399 I llm_load_print_meta: n_vocab          = 50304
0.00.051.399 I llm_load_print_meta: n_merges         = 50009
0.00.051.399 I llm_load_print_meta: vocab_only       = 0
0.00.051.401 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.401 I llm_load_print_meta: n_embd           = 2048
0.00.051.401 I llm_load_print_meta: n_layer          = 24
0.00.051.405 I llm_load_print_meta: n_head           = 16
0.00.051.405 I llm_load_print_meta: n_head_kv        = 16
0.00.051.405 I llm_load_print_meta: n_rot            = 32
0.00.051.406 I llm_load_print_meta: n_swa            = 0
0.00.051.408 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.408 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.408 I llm_load_print_meta: n_gqa            = 1
0.00.051.410 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.410 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.411 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.412 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.412 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.412 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.412 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.413 I llm_load_print_meta: n_ff             = 8192
0.00.051.413 I llm_load_print_meta: n_expert         = 0
0.00.051.413 I llm_load_print_meta: n_expert_used    = 0
0.00.051.414 I llm_load_print_meta: causal attn      = 1
0.00.051.414 I llm_load_print_meta: pooling type     = 0
0.00.051.414 I llm_load_print_meta: rope type        = 2
0.00.051.414 I llm_load_print_meta: rope scaling     = linear
0.00.051.414 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.415 I llm_load_print_meta: freq_scale_train = 1
0.00.051.415 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.415 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.415 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.416 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.416 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.417 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.417 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.417 I llm_load_print_meta: model type       = 1.4B
0.00.051.417 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.418 I llm_load_print_meta: model params     = 1.41 B
0.00.051.418 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.418 I llm_load_print_meta: general.name     = 1.4B
0.00.051.419 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.419 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.419 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.419 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.419 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.420 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.420 I llm_load_print_meta: max token length = 1024
0.00.053.531 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.531 I llm_load_tensors: offloading output layer to GPU
0.00.053.532 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.542 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.543 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.466 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.467 I llama_new_context_with_model: n_ctx         = 128
0.00.054.467 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.467 I llama_new_context_with_model: n_batch       = 128
0.00.054.468 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.468 I llama_new_context_with_model: flash_attn    = 0
0.00.054.468 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.469 I llama_new_context_with_model: freq_scale    = 1
0.00.054.469 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.470 I ggml_metal_init: allocating
0.00.054.474 I ggml_metal_init: found device: Apple M4
0.00.054.476 I ggml_metal_init: picking default device: Apple M4
0.00.055.056 I ggml_metal_init: using embedded metal library
0.00.057.425 I ggml_metal_init: GPU name:   Apple M4
0.00.057.427 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.427 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.428 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.428 I ggml_metal_init: simdgroup reduction   = true
0.00.057.428 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.428 I ggml_metal_init: has bfloat            = true
0.00.057.429 I ggml_metal_init: use bfloat            = true
0.00.057.429 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.430 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.679 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.982 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.986 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.000 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.852 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.853 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.853 I llama_new_context_with_model: graph nodes  = 967
0.00.069.854 I llama_new_context_with_model: graph splits = 2
0.00.069.867 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.867 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.574.356 I 
0.00.574.408 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.574.429 I perplexity: tokenizing the input ..
0.00.582.495 I perplexity: tokenization took 8.064 ms
0.00.582.499 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.717.058 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.718.219 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.718.232 I llama_perf_context_print:        load time =     565.07 ms
0.00.718.234 I llama_perf_context_print: prompt eval time =     134.33 ms /   128 tokens (    1.05 ms per token,   952.86 tokens per second)
0.00.718.234 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.718.235 I llama_perf_context_print:       total time =     143.88 ms /   129 tokens
0.00.718.718 I ggml_metal_free: deallocating

real	0m0.732s
user	0m0.080s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.538 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.438 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.443 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.449 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.449 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.450 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.450 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.450 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.451 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.453 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.454 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.454 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.454 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.455 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.455 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.457 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.457 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.457 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.247 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.266 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.042 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.044 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.044 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.044 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.045 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.045 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.045 I llama_model_loader: - type  f32:  194 tensors
0.00.024.046 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.046 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.236 I llm_load_vocab: special tokens cache size = 25
0.00.051.119 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.122 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.122 I llm_load_print_meta: arch             = gptneox
0.00.051.122 I llm_load_print_meta: vocab type       = BPE
0.00.051.123 I llm_load_print_meta: n_vocab          = 50304
0.00.051.123 I llm_load_print_meta: n_merges         = 50009
0.00.051.123 I llm_load_print_meta: vocab_only       = 0
0.00.051.123 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.123 I llm_load_print_meta: n_embd           = 2048
0.00.051.123 I llm_load_print_meta: n_layer          = 24
0.00.051.126 I llm_load_print_meta: n_head           = 16
0.00.051.127 I llm_load_print_meta: n_head_kv        = 16
0.00.051.127 I llm_load_print_meta: n_rot            = 32
0.00.051.127 I llm_load_print_meta: n_swa            = 0
0.00.051.127 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.127 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.128 I llm_load_print_meta: n_gqa            = 1
0.00.051.129 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.130 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.130 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.131 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.131 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.131 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.131 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.132 I llm_load_print_meta: n_ff             = 8192
0.00.051.132 I llm_load_print_meta: n_expert         = 0
0.00.051.132 I llm_load_print_meta: n_expert_used    = 0
0.00.051.132 I llm_load_print_meta: causal attn      = 1
0.00.051.132 I llm_load_print_meta: pooling type     = 0
0.00.051.132 I llm_load_print_meta: rope type        = 2
0.00.051.133 I llm_load_print_meta: rope scaling     = linear
0.00.051.133 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.136 I llm_load_print_meta: freq_scale_train = 1
0.00.051.136 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.136 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.136 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.136 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.136 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.137 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.137 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.137 I llm_load_print_meta: model type       = 1.4B
0.00.051.137 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.138 I llm_load_print_meta: model params     = 1.41 B
0.00.051.139 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.139 I llm_load_print_meta: general.name     = 1.4B
0.00.051.139 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.139 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.139 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.140 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.140 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.140 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.140 I llm_load_print_meta: max token length = 1024
0.00.053.156 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.156 I llm_load_tensors: offloading output layer to GPU
0.00.053.157 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.167 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.168 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.110 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.111 I llama_new_context_with_model: n_ctx         = 128
0.00.054.111 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.111 I llama_new_context_with_model: n_batch       = 128
0.00.054.111 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.111 I llama_new_context_with_model: flash_attn    = 0
0.00.054.112 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.112 I llama_new_context_with_model: freq_scale    = 1
0.00.054.112 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.113 I ggml_metal_init: allocating
0.00.054.116 I ggml_metal_init: found device: Apple M4
0.00.054.118 I ggml_metal_init: picking default device: Apple M4
0.00.054.695 I ggml_metal_init: using embedded metal library
0.00.057.100 I ggml_metal_init: GPU name:   Apple M4
0.00.057.101 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.102 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.102 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.102 I ggml_metal_init: simdgroup reduction   = true
0.00.057.103 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.103 I ggml_metal_init: has bfloat            = true
0.00.057.103 I ggml_metal_init: use bfloat            = true
0.00.057.103 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.104 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.091 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.392 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.394 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.407 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.340 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.341 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.341 I llama_new_context_with_model: graph nodes  = 967
0.00.069.342 I llama_new_context_with_model: graph splits = 2
0.00.069.354 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.355 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.809 I 
0.00.642.846 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.856 I perplexity: tokenizing the input ..
0.00.650.560 I perplexity: tokenization took 7.703 ms
0.00.650.564 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.791.158 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.792.324 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.792.336 I llama_perf_context_print:        load time =     633.27 ms
0.00.792.337 I llama_perf_context_print: prompt eval time =     140.34 ms /   128 tokens (    1.10 ms per token,   912.08 tokens per second)
0.00.792.338 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.792.338 I llama_perf_context_print:       total time =     149.53 ms /   129 tokens
0.00.792.707 I ggml_metal_free: deallocating

real	0m0.808s
user	0m0.080s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.159 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.940 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.944 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.946 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.946 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.947 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.947 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.947 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.948 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.949 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.950 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.950 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.951 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.951 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.951 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.953 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.953 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.953 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.666 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.711 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.485 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.486 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.486 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.486 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.487 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.487 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.488 I llama_model_loader: - type  f32:  194 tensors
0.00.024.488 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.021 I llm_load_vocab: special tokens cache size = 25
0.00.050.778 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.780 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.781 I llm_load_print_meta: arch             = gptneox
0.00.050.781 I llm_load_print_meta: vocab type       = BPE
0.00.050.781 I llm_load_print_meta: n_vocab          = 50304
0.00.050.781 I llm_load_print_meta: n_merges         = 50009
0.00.050.782 I llm_load_print_meta: vocab_only       = 0
0.00.050.782 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.782 I llm_load_print_meta: n_embd           = 2048
0.00.050.782 I llm_load_print_meta: n_layer          = 24
0.00.050.785 I llm_load_print_meta: n_head           = 16
0.00.050.786 I llm_load_print_meta: n_head_kv        = 16
0.00.050.787 I llm_load_print_meta: n_rot            = 32
0.00.050.787 I llm_load_print_meta: n_swa            = 0
0.00.050.787 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.788 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.788 I llm_load_print_meta: n_gqa            = 1
0.00.050.789 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.790 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.790 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.791 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.791 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.791 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.791 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.792 I llm_load_print_meta: n_ff             = 8192
0.00.050.792 I llm_load_print_meta: n_expert         = 0
0.00.050.793 I llm_load_print_meta: n_expert_used    = 0
0.00.050.794 I llm_load_print_meta: causal attn      = 1
0.00.050.794 I llm_load_print_meta: pooling type     = 0
0.00.050.794 I llm_load_print_meta: rope type        = 2
0.00.050.794 I llm_load_print_meta: rope scaling     = linear
0.00.050.794 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.795 I llm_load_print_meta: freq_scale_train = 1
0.00.050.795 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.795 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.795 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.797 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.797 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.798 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.798 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.798 I llm_load_print_meta: model type       = 1.4B
0.00.050.798 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.799 I llm_load_print_meta: model params     = 1.41 B
0.00.050.799 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.799 I llm_load_print_meta: general.name     = 1.4B
0.00.050.800 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.800 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.800 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.800 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.801 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.804 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.804 I llm_load_print_meta: max token length = 1024
0.00.052.496 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.497 I llm_load_tensors: offloading output layer to GPU
0.00.052.497 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.507 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.508 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.346 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.347 I llama_new_context_with_model: n_ctx         = 128
0.00.053.347 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.347 I llama_new_context_with_model: n_batch       = 128
0.00.053.348 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.348 I llama_new_context_with_model: flash_attn    = 0
0.00.053.348 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.349 I llama_new_context_with_model: freq_scale    = 1
0.00.053.349 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.349 I ggml_metal_init: allocating
0.00.053.355 I ggml_metal_init: found device: Apple M4
0.00.053.357 I ggml_metal_init: picking default device: Apple M4
0.00.053.918 I ggml_metal_init: using embedded metal library
0.00.056.287 I ggml_metal_init: GPU name:   Apple M4
0.00.056.289 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.289 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.290 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.290 I ggml_metal_init: simdgroup reduction   = true
0.00.056.290 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.290 I ggml_metal_init: has bfloat            = true
0.00.056.290 I ggml_metal_init: use bfloat            = true
0.00.056.291 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.291 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.846 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.170 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.173 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.195 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.028 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.029 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.030 I llama_new_context_with_model: graph nodes  = 967
0.00.068.030 I llama_new_context_with_model: graph splits = 2
0.00.068.042 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.042 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.423.933 I 
0.00.423.977 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.423.988 I perplexity: tokenizing the input ..
0.00.431.803 I perplexity: tokenization took 7.813 ms
0.00.431.807 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.571.912 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.573.076 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.573.097 I llama_perf_context_print:        load time =     413.77 ms
0.00.573.098 I llama_perf_context_print: prompt eval time =     139.85 ms /   128 tokens (    1.09 ms per token,   915.28 tokens per second)
0.00.573.099 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.573.099 I llama_perf_context_print:       total time =     149.16 ms /   129 tokens
0.00.573.533 I ggml_metal_free: deallocating

real	0m0.586s
user	0m0.079s
sys	0m0.087s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.277 I build: 4375 (9d5c7115) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.112 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.784 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.791 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.794 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.795 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.795 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.796 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.797 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.801 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.801 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.802 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.803 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.803 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.804 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.804 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.807 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.808 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.808 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.383 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.305 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.193 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.195 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.196 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.196 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.197 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.198 I llama_model_loader: - type  f32:  194 tensors
0.00.053.198 I llama_model_loader: - type  f16:   98 tensors
0.00.081.341 I llm_load_vocab: special tokens cache size = 25
0.00.087.841 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.087.845 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.087.845 I llm_load_print_meta: arch             = gptneox
0.00.087.846 I llm_load_print_meta: vocab type       = BPE
0.00.087.846 I llm_load_print_meta: n_vocab          = 50304
0.00.087.846 I llm_load_print_meta: n_merges         = 50009
0.00.087.846 I llm_load_print_meta: vocab_only       = 0
0.00.087.846 I llm_load_print_meta: n_ctx_train      = 2048
0.00.087.846 I llm_load_print_meta: n_embd           = 2048
0.00.087.846 I llm_load_print_meta: n_layer          = 24
0.00.087.850 I llm_load_print_meta: n_head           = 16
0.00.087.851 I llm_load_print_meta: n_head_kv        = 16
0.00.087.851 I llm_load_print_meta: n_rot            = 32
0.00.087.851 I llm_load_print_meta: n_swa            = 0
0.00.087.851 I llm_load_print_meta: n_embd_head_k    = 128
0.00.087.852 I llm_load_print_meta: n_embd_head_v    = 128
0.00.087.852 I llm_load_print_meta: n_gqa            = 1
0.00.087.853 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.087.854 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.087.856 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.087.856 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.087.856 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.087.856 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.087.857 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.087.857 I llm_load_print_meta: n_ff             = 8192
0.00.087.857 I llm_load_print_meta: n_expert         = 0
0.00.087.858 I llm_load_print_meta: n_expert_used    = 0
0.00.087.858 I llm_load_print_meta: causal attn      = 1
0.00.087.858 I llm_load_print_meta: pooling type     = 0
0.00.087.859 I llm_load_print_meta: rope type        = 2
0.00.087.859 I llm_load_print_meta: rope scaling     = linear
0.00.087.860 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.087.860 I llm_load_print_meta: freq_scale_train = 1
0.00.087.861 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.087.861 I llm_load_print_meta: rope_finetuned   = unknown
0.00.087.861 I llm_load_print_meta: ssm_d_conv       = 0
0.00.087.861 I llm_load_print_meta: ssm_d_inner      = 0
0.00.087.861 I llm_load_print_meta: ssm_d_state      = 0
0.00.087.861 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.087.861 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.087.862 I llm_load_print_meta: model type       = 1.4B
0.00.087.862 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.087.862 I llm_load_print_meta: model params     = 1.41 B
0.00.087.863 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.087.863 I llm_load_print_meta: general.name     = 1.4B
0.00.087.863 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.087.864 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.087.864 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.087.864 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.087.864 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.087.864 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.087.865 I llm_load_print_meta: max token length = 1024
0.00.089.939 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.089.940 I llm_load_tensors: offloading output layer to GPU
0.00.089.940 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.089.950 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.951 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.090.847 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.847 I llama_new_context_with_model: n_ctx         = 128
0.00.090.848 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.090.848 I llama_new_context_with_model: n_batch       = 128
0.00.090.848 I llama_new_context_with_model: n_ubatch      = 128
0.00.090.848 I llama_new_context_with_model: flash_attn    = 0
0.00.090.849 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.849 I llama_new_context_with_model: freq_scale    = 1
0.00.090.849 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.850 I ggml_metal_init: allocating
0.00.090.859 I ggml_metal_init: found device: Apple M4
0.00.090.862 I ggml_metal_init: picking default device: Apple M4
0.00.091.514 I ggml_metal_init: using embedded metal library
0.00.094.091 I ggml_metal_init: GPU name:   Apple M4
0.00.094.093 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.094 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.094 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.094 I ggml_metal_init: simdgroup reduction   = true
0.00.094.094 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.095 I ggml_metal_init: has bfloat            = true
0.00.094.095 I ggml_metal_init: use bfloat            = true
0.00.094.095 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.096 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.260 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.104.666 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.104.669 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.104.684 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.533 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.105.534 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.105.534 I llama_new_context_with_model: graph nodes  = 967
0.00.105.535 I llama_new_context_with_model: graph splits = 2
0.00.105.548 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.105.549 I 
0.00.105.583 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.105.584 I compute_imatrix: tokenizing the input ..
0.00.112.503 I compute_imatrix: tokenization took 6.918 ms
0.00.112.505 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.731.054 I compute_imatrix: 1.62 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.733.512 I llama_perf_context_print:        load time =    1708.94 ms
0.01.733.514 I llama_perf_context_print: prompt eval time =    1617.96 ms /   128 tokens (   12.64 ms per token,    79.11 tokens per second)
0.01.733.515 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.733.515 I llama_perf_context_print:       total time =    1711.39 ms /   129 tokens
0.01.734.089 I ggml_metal_free: deallocating

real	0m1.923s
user	0m0.171s
sys	0m0.265s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4375 (9d5c7115)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10520a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10520a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10520ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10520b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10520b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10520bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10520c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10520caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10520d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10520d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10520daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10520dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10520eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10520f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10520fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1052101a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1052108c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x105210fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x105211700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x105211ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1052125f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x105212d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x105213430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x105213cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1052143f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1052146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x105214cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x105215930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x105215e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x105216130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1052165d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x105216890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105217120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105217660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105217920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105217dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105218260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x105218700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105218ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105219040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1052194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x105219980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105219e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10521a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10521a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10521ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10521b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10521bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10521c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10521c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10521ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10521d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10521d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10521df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10521e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10521ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10521f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10521f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10521f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105220110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1052203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x105220870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105220d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1052211b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x105221650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x105221af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x105221f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x105222430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1052228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x105222d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x105223210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1052236b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x105223b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1052240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1052245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x105224b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x105225090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1052255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x105225b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x105226080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1052265d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x105226b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x105227070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1052275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x105227b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x105228060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1052285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x105228b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x105229050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1052295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x105229af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10522a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10522a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10522aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10522b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10522b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10522bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10521b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10522bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10522c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10522cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10522d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10522d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10522dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10522e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10522e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10522ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10522f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10522f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10522fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x105230160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1052306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x105230c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1052310a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105231540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1052319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x105231e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x105232320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1052327c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105232c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x105233100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1052335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x105233a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x105233ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x105234380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x105234820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x105234cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x105235160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x105235600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x105235aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x105235f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1052363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x105236880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x105236d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1052371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x105237660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x105237b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x105237fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x105238440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1052388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x105238d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x105239220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1052396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x105239b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10523a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10523a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10523a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10523ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10523b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10523b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10523bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10523c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10523c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10523c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10523ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10523d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10523d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10523dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10523e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10523e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10523ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10523eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10523f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10523f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10523fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105240120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1052405c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x105240a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x105240f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1052413a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x105241840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105241ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x105242180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105242620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105242ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105242f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105243400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1052438a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x105243d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1052441e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x105244680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x105244b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x105244fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x105245460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x105245900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x105245da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x105246240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1052466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x105246b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x105247020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1052474c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x105247960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x105247e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x105248350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1052488a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x105248df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x105249340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x105249600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x105249c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10524a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10524a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10524b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10524b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10524b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10524bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10524c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10524cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10524d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10524d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10524d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10524e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10524e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10524ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10524f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10524f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10524fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x105250100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x105250650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x105250ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1052510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x105251640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x105251b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1052520e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x105252630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x105252b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1052530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105253620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x105253b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1052540c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x105254610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x105254b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1052550b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x105255600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x105255b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1052560a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1052565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x105256b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x105257090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1052575e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x105257b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x105258080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1052585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x105258b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x105259070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1052595c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x105259b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10525a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10525a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10525ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10525b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10525b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10525baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10525c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10525c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10525cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10525d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10525d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10525dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10525e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10525e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10525eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10525f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10525f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10525fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x105260000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x105260550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x105260aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x105260f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1052613e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x105261880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x105261d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1052621c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x105262660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x105262b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105262fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x105263440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1052638e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105263d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x105264220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1052646c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x105264b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x105265000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x105265550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x105265c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x105266390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x105266ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1052671d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x105267490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x105267c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x105267f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x105268550 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.143.568 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.143.572 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x105225310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105225780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105225bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x105226060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1052264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105226940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105226db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x105227220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x105227690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105227b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105227f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105228550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105228e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1052295c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x105229da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10522a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10522ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10522b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10522b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10522c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10522c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10522d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10522d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10522dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10522e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10522ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10522ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10522f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10522f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10522fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x105230030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1052304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105230910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105230bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105231040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1052314b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105231920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x105231d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105232200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105232670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105232ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x105232f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1052333c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105233830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x105233ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105234110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x105234580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1052349f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105234e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1052352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x105235740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x105235bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105236020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x105236490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105236900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105236d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1052371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105237650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x105237ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105237f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1052383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x105238810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105238c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1052390f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x105239560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1052399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x105239e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10523a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10523a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10523ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10523b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10523b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10523b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10523bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10523c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10523c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10523caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10523cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10523d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10523d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10523dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10523e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10523e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10523e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10523ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10523f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10523f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10523fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10523ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x105240450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1052408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x105240d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1052411a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x105241610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x105241a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x105241ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105242360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1052427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x105242c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1052430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x105243520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x105243990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x105243e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x105244270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1052446e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x105244b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x105244fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x105245430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1052458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x105245d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x105246180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1052465f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x105246a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x105246ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105247340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1052477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x105247c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x105248090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x105248500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105248970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x105248de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x105249250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1052496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x105249b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x105249fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10524a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10524a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10524acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10524b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10524b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10524ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10524beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10524c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10524c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10524cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10524d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10524d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10524d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10524ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10524e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10524e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10524eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10524ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10524f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10524f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10524fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x105250140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1052505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105250a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105250e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105251300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105251770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105251be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105252050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1052524c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105252930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105252da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105253210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105253680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105253af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105253f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1052543d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x105254840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105254cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105255120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105255590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x105255a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x105255e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1052562e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x105256750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x105256bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105257030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1052574a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105257910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105257d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1052581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105258660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x105258ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x105258f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1052593b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x105259820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x105259c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10525a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10525a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10525a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10525ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10525b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10525b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10525bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10525c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10525c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10525c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10525cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10525d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10525d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10525dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10525df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10525e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10525e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10525ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10525f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10525f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10525f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10525fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1052602a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x105260710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x105260b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105260ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x105261460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1052618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x105262050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1052624c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x105262930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x105262da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x105263210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x105263680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x105263af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x105263f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1052643d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x105264840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x105264cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x105265120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x105265590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x105265a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x105265e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1052662e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105266750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x105266bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x105267030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1052674a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x105267910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x105267d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1052681f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x105268660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10520b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10520ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10520a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x105217650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x105217910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x105217d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1052181f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x105218660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x105218ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x105218f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1052193b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x105219820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x105219c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10521a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10521a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10521a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10521ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10521b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10521b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10521bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10521c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10521c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10521c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10521cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10521d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10521d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10521dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10521df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10521e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10521e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10521ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10521f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10521f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10521f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10521fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1052202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x105220710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x105220b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x105220ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x105221460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1052218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x105221d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1052221b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105222620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x105222a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x105222f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x105223370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1052237e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x105223c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x105224340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1052160e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1052167d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x105216ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10520d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10520d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10520de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10520e2b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1052162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105216730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105216ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x105217010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105217800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105217c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1052180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x105218550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1052189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105218e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1052192a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105219880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10521a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10521a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10521b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10521b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10521beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10521c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10521cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10521d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10521dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10521e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10521eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10521f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10521f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10521fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1052201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x105220610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x105220a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x105220ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x105221360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1052217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105221c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105221f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105222370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1052227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105222c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1052230c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105223530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1052239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105223e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x105224280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10520a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10520acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10520b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105224eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x105225170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1052255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105225a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105225ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x105226330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1052267a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105226c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x105227080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1052274f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105227960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105227dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105228240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1052286b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105228b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105228f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x105229400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105229870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x105229ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10522a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10522a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10522aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10522aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10522b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10522b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10522bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10522c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10522c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10522c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10522cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10522d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10522d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10522db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10522df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10522e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10522e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10522ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10522f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10522f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10522fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10522fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1052302f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x105230760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x105230bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x105231040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1052314b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x105231920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x105231d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x105232200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x105232670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x105232ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105232f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1052333c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x105233830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x105233ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x105234110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x105234580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1052349f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x105234e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1052352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x105235740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x105235bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x105236020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x105236490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x105236900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x105236d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1052371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x105237650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x105237ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105237f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1052383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x105238810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x105238c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1052390f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105239560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1052399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x105239e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10523a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10523a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10523ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10523b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10523b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10523b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10523bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10523c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10523c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10523caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10523cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10523d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10523d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10523dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10523e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10523e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10523e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10523ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10523f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10523f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10523fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10523ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x105240450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1052408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x105240d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1052411a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105241610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105241a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105241ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105242360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1052427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105242c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1052430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105243520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105243990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105243e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105244270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1052446e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105244b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105244fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x105245430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1052458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105245d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105246180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1052465f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x105246a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x105246ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x105247340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1052477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105247c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x105248090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105248500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105248970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105248de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105249250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1052496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x105249b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x105249fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10524a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10524a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10524acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10524b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10524b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10524ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10524beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10524c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10524c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10524cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10524d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10524d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10524d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10524ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10524e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10524e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10524eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10524ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10524f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10524f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10524fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x105250140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1052505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x105250a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x105250e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x105251300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x105251770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105251be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x105252050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1052524c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x105252c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1052530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x105253520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x105253990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x105253e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x105254270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1052546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x105254b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x105254fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x105255430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1052558a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x105255d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x105256180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1052565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x105256a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x105256ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105257340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1052577b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x105257c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x105258090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x105258500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x105258970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x105258de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x105259250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1052596c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x105259b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x105259fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10525a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10525a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10525acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10525b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10525b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10525ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10525beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10525c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10525c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10525cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10525d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10525d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10525d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10525ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10525e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10525e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10525eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10525ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10525f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10525f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10525fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105260140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1052605b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x105260a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x105260e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x105261300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x105261770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x105261be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x105262050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1052624c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x105262930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x105262da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x105263210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x105263680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x105263af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x105263f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1052643d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105264840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x105264cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x105265120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105265590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x105265a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x105265e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1052662e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x105266750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x105266bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x105267420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x105267b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x105268200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1052097f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10520d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10520db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10520dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10520e460 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.810s
user	0m0.295s
sys	0m0.306s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4375 (9d5c7115)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13d70eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13d70f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13d70f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13d70ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13d7104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13d710a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13d711020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13d7115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13d711b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13d712080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13d712580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13d712a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13d7135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13d713d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13d714560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13d714c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13d7153a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13d715ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13d7161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13d7169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13d7170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13d7177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13d717f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13d7187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13d718ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13d719190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13d7197a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13d71a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13d71a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13d71ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13d71b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13d71b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13d71bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13d71c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13d71c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13d71c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13d71cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13d71d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13d71d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13d71db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13d71dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13d71e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13d71e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13d71eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13d71f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13d71f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13d71fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13d7205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13d720bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13d7211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13d7217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13d721de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13d7223f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13d722a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13d7231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13d723690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13d723b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13d723df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13d724400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13d724bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13d724eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13d725350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13d7257f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13d725c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13d726130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13d7265d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13d726a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13d726f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13d7273b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13d727850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13d727cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13d728190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13d728630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13d728b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13d7290d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13d729620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13d729b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13d72a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13d72a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13d72ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13d72b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13d72b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13d72bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13d72c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13d72c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13d72cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13d72d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13d72d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13d72db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13d72e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13d72e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13d72eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13d72f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13d72f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13d72fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13d730060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13d7305b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13d720290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13d730a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13d7311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13d731720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13d731c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13d7321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13d732710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13d732c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13d7331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13d733700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13d733c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13d7341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13d7346f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13d734c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13d735190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13d7356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13d735b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13d736020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13d7364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13d736960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13d736e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13d7372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13d737740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13d737be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13d738080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13d738520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13d7389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13d738e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13d739300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13d7397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13d739c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13d73a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13d73a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13d73aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13d73aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13d73b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13d73b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13d73bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13d73c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13d73c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13d73ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13d73cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13d73d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13d73d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13d73dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13d73e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13d73e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13d73eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13d73ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13d73f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13d73f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13d73fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13d740200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13d7406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13d740b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13d740fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13d741480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13d741920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13d741dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13d742260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13d742700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13d742ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13d743040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13d7434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13d743980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13d743e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13d7442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13d744760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13d744c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13d7450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13d745540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13d7459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13d745e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13d746320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13d7467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13d746c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13d747100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13d7475a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13d747a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13d747ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13d748380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13d748820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13d748cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13d749160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13d749600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13d749aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13d749f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13d74a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13d74a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13d74ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13d74b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13d74b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13d74bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13d74bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13d74c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13d74c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13d74ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13d74d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13d74d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13d74de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13d74e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13d74e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13d74ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13d74f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13d74fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13d74ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13d750260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13d750870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13d750e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13d751670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13d751b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13d751fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13d752450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13d752c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13d753150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13d7536a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13d753bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13d754140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13d754690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13d754be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13d755130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13d755680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13d755bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13d756120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13d756670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13d756bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13d757110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13d757660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13d757bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13d758100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13d758650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13d758ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13d7590f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13d759640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13d759b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13d75a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13d75a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13d75ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13d75b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13d75b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13d75bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13d75c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13d75c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13d75cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13d75d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13d75d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13d75db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13d75e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13d75e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13d75eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13d75f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13d75f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13d75fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13d760080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13d7605d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13d760b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13d761070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13d7615c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13d761b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13d762060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13d7625b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13d762b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13d763050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13d7635a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13d763af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13d764040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13d764590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13d764ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13d765030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13d765580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13d765a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13d765ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13d766360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13d766800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13d766ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13d767140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13d7675e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13d767a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13d767f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13d7683c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13d768860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13d768d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13d7691a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13d769640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13d769ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13d76a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13d76a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13d76ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13d76b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13d76bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13d76bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13d76c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13d76ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13d76d030 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.938 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.941 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12df04d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12df051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12df05630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12df05aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12df05f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12df06380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12df067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12df06c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12df070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12df07540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12df079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12df080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12df08bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12df09370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12df09b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12df0a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12df0a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12df0b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12df0b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12df0bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12df0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12df0cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12df0d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12df0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12df0e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12df0e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12df0e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12df0ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12df0f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12df0f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12df0fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12df0ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12df103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12df10670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12df10ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12df10f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12df113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12df11830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12df11ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12df12110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12df12580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12df129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12df12e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12df132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12df13740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12df13bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12df14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12df14490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12df14900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12df14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12df151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12df15650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12df15ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12df15f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12df163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12df16810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12df16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12df17280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12df176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12df17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12df17fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12df18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12df188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12df18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12df19190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12df19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12df19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12df19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12df1a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12df1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12df1ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12df1b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12df1b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12df1b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12df1bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12df1c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12df1c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12df1cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12df1cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12df1d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12df1d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12df1dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12df1e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12df1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12df1ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12df1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12df1f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12df1f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12df1fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12df20080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12df204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12df20960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12df20dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12df21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12df216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12df21b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12df21f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12df22400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12df22870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12df22ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12df23150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12df235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12df23a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12df23ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12df24310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12df24780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12df24bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12df25060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12df254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12df25940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12df25db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12df26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12df26690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12df26b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12df26f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12df273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12df27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12df27cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12df28130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12df285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12df28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12df28e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12df292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12df29760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12df29bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12df2a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12df2a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12df2a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12df2ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12df2b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12df2b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12df2bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12df2bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12df2c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12df2c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12df2cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12df2d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12df2d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12df2d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12df2de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12df2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12df2e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12df2ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12df2f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12df2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12df2f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12df2fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12df301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12df30650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12df30ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12df30f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12df313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12df31810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12df31c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12df320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12df32560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12df329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12df32e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12df332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12df33720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12df33b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12df34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12df34470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12df348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12df34d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12df351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12df35630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12df35aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12df35f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12df36380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12df367f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12df36c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12df370d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12df37540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12df379b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12df37e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12df38290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12df38700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12df38b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12df38fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12df39450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12df398c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12df39d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12df3a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12df3a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12df3aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12df3aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12df3b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12df3b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12df3bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12df3c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12df3c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12df3c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12df3ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12df3d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12df3d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12df3db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12df3dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12df3e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12df3e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12df3ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12df3f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12df3f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12df3fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12df3fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12df40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12df407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12df40d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12df411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12df41620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12df42170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12df42430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12df426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12df42b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12df42fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12df43440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12df438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12df43d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12df44190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12df44600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12df44a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12df44ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12df45350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12df457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12df45c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12df460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12df46510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12df46980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12df46df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12df47260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12df476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12df47b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12df47fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12df48420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12df48890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12df48d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12df49170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12df495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12df49a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12df49ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12df4a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12df4a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12df4ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12df4b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12df4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12df4b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12df4bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12df4c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12df4c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12df4cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12df4cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12df4d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12df4d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12df4dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12df4e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12df4e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12df4ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12df4eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12df4f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12df4f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12df4fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12df50060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12df504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12df50940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12df50db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12df51220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12df51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12df51b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12df51f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12df523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12df52850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12df52cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12df53130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12df535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12df53a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12df53e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12df542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12df54760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12df54bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12df55040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12df554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12df55920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12df55d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12df56800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12df56f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12df57640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12df57d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12df58020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12df58490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12df58a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12df590a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13f005aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13f005f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13f006380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13f0067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13f006c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13f0070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13f007540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13f0079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13f007e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13f008290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13f008700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13f008df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13f009910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13f00a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13f00a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13f00aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13f00b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13f00be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13f00c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13f00cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13f00d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13f00dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13f00e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13f00e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13f00f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13f00f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13f00f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13f00fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13f00fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13f0102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13f010760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13f010c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13f011100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13f0113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13f011830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13f011ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13f012110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13f012580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13f0129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13f012e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13f0132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13f013740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13f013bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13f014020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13f014490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13f014900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13f014d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13f0151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13f015650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13f015ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13f015f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13f0163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13f016810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13f016c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13f0170f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13f017560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13f017ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13f017fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13f018440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13f0188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13f018d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13f019190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13f019600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13f019a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13f019ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13f01a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13f01a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13f01ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13f01b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13f01b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13f01b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13f01bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13f01c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13f01c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13f01cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13f01cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13f01d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13f01d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13f01dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13f01e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13f01e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13f01ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13f01eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13f01f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13f01f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13f01fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13f020080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13f0204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13f020960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13f020dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13f021240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13f0216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13f021b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13f021f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13f022400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13f022870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13f022ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13f023150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13f0235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13f023a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13f023ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13f024310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13f024780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13f024bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13f025060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13f0254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13f025940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13f025db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13f026220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13f026690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13f026b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13f026f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13f0273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13f027850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13f027cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13f028130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13f0285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13f028a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13f028e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13f0292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13f029760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13f029bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13f02a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13f02a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13f02a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13f02ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13f02b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13f02b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13f02bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13f02bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13f02c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13f02c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13f02cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13f02d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13f02d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13f02d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13f02de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13f02e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13f02e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13f02ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13f02f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13f02f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13f02f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13f02fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13f0301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13f030650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13f030ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13f030f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13f0313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13f031810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13f031c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13f0320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13f032560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13f0329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13f032e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13f0332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13f033720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13f033b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13f034000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13f034470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13f0348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13f034d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13f0351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13f035630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13f035aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13f035f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13f036380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13f0367f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13f036c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13f0370d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13f037540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13f0379b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13f037e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13f038290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13f038700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13f038b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13f038fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13f039450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13f0398c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13f039d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13f03a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13f03a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13f03aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13f03aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13f03b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13f03b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13f03bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13f03c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13f03c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13f03c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13f03ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13f03d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13f03d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13f03db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13f03dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13f03e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13f03e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13f03ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13f03f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13f03f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13f03fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13f03fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13f040340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13f0407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13f040c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13f041090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13f041500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13f041a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13f041f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13f042370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13f042ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13f043180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13f043440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13f0438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13f043d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13f044190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13f044600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13f044a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13f044ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13f045350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13f0457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13f045c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13f0460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13f046510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13f046980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13f046df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13f047260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13f0476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13f047b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13f047fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13f048420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13f048890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13f048d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13f049170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13f0495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13f049a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13f049ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13f04a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13f04a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13f04ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13f04b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13f04b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13f04b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13f04bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13f04c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13f04ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13f04ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13f04d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13f04d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13f04dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13f04e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13f04e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13f04e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13f04ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13f04f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13f04f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13f04fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13f04ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13f0503b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13f050820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13f050c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13f051100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13f051570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13f0519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13f051e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13f0522c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13f052730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13f052ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13f053010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13f053480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13f0538f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13f053d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13f0541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13f054640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13f054ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13f054f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13f055390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13f055800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13f055c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13f0560e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13f056550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13f0569c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13f056e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13f0578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13f057fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13f0586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13f058e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13f0590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13f059530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13f059b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13f05a140 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.945s
user	0m0.245s
sys	0m0.150s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
