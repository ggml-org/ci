### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.67 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.11 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.09 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.61 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.19 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.20 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.32 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.34 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.36 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.16 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.25 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.35 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    3.00 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.95 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  192.48 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    1.00 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.14 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.40 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 254.59 sec*proc (29 tests)

Total Test time (real) = 254.61 sec

real	4m14.721s
user	8m35.186s
sys	0m7.190s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.37 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.22 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.93 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.24 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.80 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.23 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.36 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.25 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.49 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.52 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.78 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.39 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   15.14 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.23 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.23 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  56.46 sec*proc (29 tests)

Total Test time (real) =  56.48 sec

real	0m56.488s
user	1m18.136s
sys	0m6.386s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.191 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.204 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.765 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.028.773 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.775 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.028.776 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.777 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.028.778 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.028.778 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.028.780 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.028.780 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.028.781 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.028.784 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.028.784 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.028.788 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.028.789 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.028.789 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.028.790 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.028.790 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.028.791 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.028.792 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.033.950 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.035.325 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.327 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.035.328 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.035.328 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.035.329 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.035.329 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.035.330 I llama_model_loader: - type  f32:  124 tensors
0.00.035.330 I llama_model_loader: - type  f16:   73 tensors
0.00.035.331 I print_info: file format = GGUF V3 (latest)
0.00.035.332 I print_info: file type   = F16
0.00.035.333 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.039.804 I load: special tokens cache size = 5
0.00.042.313 I load: token to piece cache size = 0.2032 MB
0.00.042.317 I print_info: arch             = bert
0.00.042.318 I print_info: vocab_only       = 0
0.00.042.318 I print_info: n_ctx_train      = 512
0.00.042.318 I print_info: n_embd           = 384
0.00.042.319 I print_info: n_layer          = 12
0.00.042.322 I print_info: n_head           = 12
0.00.042.323 I print_info: n_head_kv        = 12
0.00.042.323 I print_info: n_rot            = 32
0.00.042.324 I print_info: n_swa            = 0
0.00.042.324 I print_info: n_embd_head_k    = 32
0.00.042.324 I print_info: n_embd_head_v    = 32
0.00.042.325 I print_info: n_gqa            = 1
0.00.042.326 I print_info: n_embd_k_gqa     = 384
0.00.042.327 I print_info: n_embd_v_gqa     = 384
0.00.042.331 I print_info: f_norm_eps       = 1.0e-12
0.00.042.332 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.332 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.333 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.333 I print_info: f_logit_scale    = 0.0e+00
0.00.042.334 I print_info: n_ff             = 1536
0.00.042.334 I print_info: n_expert         = 0
0.00.042.334 I print_info: n_expert_used    = 0
0.00.042.335 I print_info: causal attn      = 0
0.00.042.335 I print_info: pooling type     = 2
0.00.042.335 I print_info: rope type        = 2
0.00.042.336 I print_info: rope scaling     = linear
0.00.042.336 I print_info: freq_base_train  = 10000.0
0.00.042.337 I print_info: freq_scale_train = 1
0.00.042.337 I print_info: n_ctx_orig_yarn  = 512
0.00.042.338 I print_info: rope_finetuned   = unknown
0.00.042.338 I print_info: ssm_d_conv       = 0
0.00.042.338 I print_info: ssm_d_inner      = 0
0.00.042.338 I print_info: ssm_d_state      = 0
0.00.042.339 I print_info: ssm_dt_rank      = 0
0.00.042.339 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.339 I print_info: model type       = 33M
0.00.042.340 I print_info: model params     = 33.21 M
0.00.042.340 I print_info: general.name     = Bge Small
0.00.042.341 I print_info: vocab type       = WPM
0.00.042.341 I print_info: n_vocab          = 30522
0.00.042.341 I print_info: n_merges         = 0
0.00.042.342 I print_info: BOS token        = 101 '[CLS]'
0.00.042.342 I print_info: UNK token        = 100 '[UNK]'
0.00.042.343 I print_info: SEP token        = 102 '[SEP]'
0.00.042.343 I print_info: PAD token        = 0 '[PAD]'
0.00.042.343 I print_info: MASK token       = 103 '[MASK]'
0.00.042.344 I print_info: LF token         = 0 '[PAD]'
0.00.042.344 I print_info: max token length = 21
0.00.042.345 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.045.661 I load_tensors: offloading 12 repeating layers to GPU
0.00.045.662 I load_tensors: offloading output layer to GPU
0.00.045.663 I load_tensors: offloaded 13/13 layers to GPU
0.00.045.687 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.045.689 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.045.952 I llama_init_from_model: n_seq_max     = 1
0.00.045.953 I llama_init_from_model: n_ctx         = 512
0.00.045.953 I llama_init_from_model: n_ctx_per_seq = 512
0.00.045.953 I llama_init_from_model: n_batch       = 2048
0.00.045.954 I llama_init_from_model: n_ubatch      = 2048
0.00.045.954 I llama_init_from_model: flash_attn    = 0
0.00.045.955 I llama_init_from_model: freq_base     = 10000.0
0.00.045.955 I llama_init_from_model: freq_scale    = 1
0.00.045.956 I ggml_metal_init: allocating
0.00.045.961 I ggml_metal_init: found device: Apple M4
0.00.045.968 I ggml_metal_init: picking default device: Apple M4
0.00.046.719 I ggml_metal_init: using embedded metal library
0.00.050.911 I ggml_metal_init: GPU name:   Apple M4
0.00.050.913 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.050.914 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.050.914 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.050.915 I ggml_metal_init: simdgroup reduction   = true
0.00.050.915 I ggml_metal_init: simdgroup matrix mul. = true
0.00.050.915 I ggml_metal_init: has residency sets    = true
0.00.050.915 I ggml_metal_init: has bfloat            = true
0.00.050.916 I ggml_metal_init: use bfloat            = true
0.00.050.916 I ggml_metal_init: hasUnifiedMemory      = true
0.00.050.917 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.494 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.064.203 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.064.205 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.064.227 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.065.486 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.065.487 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.065.488 I llama_init_from_model: graph nodes  = 429
0.00.065.488 I llama_init_from_model: graph splits = 2
0.00.065.489 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.065.489 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.071.220 I 
0.00.071.248 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.071.947 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.076.991 I llama_perf_context_print:        load time =      48.01 ms
0.00.076.992 I llama_perf_context_print: prompt eval time =       4.89 ms /     9 tokens (    0.54 ms per token,  1840.87 tokens per second)
0.00.076.993 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.076.993 I llama_perf_context_print:       total time =       5.77 ms /    10 tokens
0.00.077.147 I ggml_metal_free: deallocating

real	0m0.285s
user	0m0.053s
sys	0m0.035s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.049 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.641 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.467 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.470 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.472 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.473 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.473 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.473 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.473 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.474 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.475 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.475 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.475 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.476 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.478 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.478 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.479 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.479 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.479 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.479 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.942 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.590 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.591 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.591 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.591 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.592 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.592 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.592 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.593 I llama_model_loader: - type  f32:  124 tensors
0.00.015.593 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.594 I print_info: file format = GGUF V3 (latest)
0.00.015.594 I print_info: file type   = Q8_0
0.00.015.595 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.018.186 I load: special tokens cache size = 5
0.00.019.546 I load: token to piece cache size = 0.2032 MB
0.00.019.549 I print_info: arch             = bert
0.00.019.549 I print_info: vocab_only       = 0
0.00.019.549 I print_info: n_ctx_train      = 512
0.00.019.549 I print_info: n_embd           = 384
0.00.019.549 I print_info: n_layer          = 12
0.00.019.553 I print_info: n_head           = 12
0.00.019.553 I print_info: n_head_kv        = 12
0.00.019.555 I print_info: n_rot            = 32
0.00.019.555 I print_info: n_swa            = 0
0.00.019.556 I print_info: n_embd_head_k    = 32
0.00.019.556 I print_info: n_embd_head_v    = 32
0.00.019.557 I print_info: n_gqa            = 1
0.00.019.557 I print_info: n_embd_k_gqa     = 384
0.00.019.558 I print_info: n_embd_v_gqa     = 384
0.00.019.559 I print_info: f_norm_eps       = 1.0e-12
0.00.019.559 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.559 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.559 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.560 I print_info: f_logit_scale    = 0.0e+00
0.00.019.560 I print_info: n_ff             = 1536
0.00.019.561 I print_info: n_expert         = 0
0.00.019.561 I print_info: n_expert_used    = 0
0.00.019.561 I print_info: causal attn      = 0
0.00.019.563 I print_info: pooling type     = 2
0.00.019.563 I print_info: rope type        = 2
0.00.019.564 I print_info: rope scaling     = linear
0.00.019.564 I print_info: freq_base_train  = 10000.0
0.00.019.564 I print_info: freq_scale_train = 1
0.00.019.564 I print_info: n_ctx_orig_yarn  = 512
0.00.019.565 I print_info: rope_finetuned   = unknown
0.00.019.565 I print_info: ssm_d_conv       = 0
0.00.019.565 I print_info: ssm_d_inner      = 0
0.00.019.565 I print_info: ssm_d_state      = 0
0.00.019.565 I print_info: ssm_dt_rank      = 0
0.00.019.565 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.565 I print_info: model type       = 33M
0.00.019.566 I print_info: model params     = 33.21 M
0.00.019.566 I print_info: general.name     = Bge Small
0.00.019.571 I print_info: vocab type       = WPM
0.00.019.571 I print_info: n_vocab          = 30522
0.00.019.571 I print_info: n_merges         = 0
0.00.019.572 I print_info: BOS token        = 101 '[CLS]'
0.00.019.572 I print_info: UNK token        = 100 '[UNK]'
0.00.019.572 I print_info: SEP token        = 102 '[SEP]'
0.00.019.572 I print_info: PAD token        = 0 '[PAD]'
0.00.019.572 I print_info: MASK token       = 103 '[MASK]'
0.00.019.572 I print_info: LF token         = 0 '[PAD]'
0.00.019.573 I print_info: max token length = 21
0.00.019.573 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.021.378 I load_tensors: offloading 12 repeating layers to GPU
0.00.021.379 I load_tensors: offloading output layer to GPU
0.00.021.379 I load_tensors: offloaded 13/13 layers to GPU
0.00.021.385 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.386 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.021.537 I llama_init_from_model: n_seq_max     = 1
0.00.021.537 I llama_init_from_model: n_ctx         = 512
0.00.021.538 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.538 I llama_init_from_model: n_batch       = 2048
0.00.021.538 I llama_init_from_model: n_ubatch      = 2048
0.00.021.538 I llama_init_from_model: flash_attn    = 0
0.00.021.539 I llama_init_from_model: freq_base     = 10000.0
0.00.021.539 I llama_init_from_model: freq_scale    = 1
0.00.021.539 I ggml_metal_init: allocating
0.00.021.543 I ggml_metal_init: found device: Apple M4
0.00.021.546 I ggml_metal_init: picking default device: Apple M4
0.00.022.075 I ggml_metal_init: using embedded metal library
0.00.024.686 I ggml_metal_init: GPU name:   Apple M4
0.00.024.688 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.688 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.689 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.689 I ggml_metal_init: simdgroup reduction   = true
0.00.024.689 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.690 I ggml_metal_init: has residency sets    = true
0.00.024.690 I ggml_metal_init: has bfloat            = true
0.00.024.690 I ggml_metal_init: use bfloat            = true
0.00.024.690 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.691 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.947 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.566 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.568 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.581 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.598 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.599 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.599 I llama_init_from_model: graph nodes  = 429
0.00.036.599 I llama_init_from_model: graph splits = 2
0.00.036.601 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.601 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.615 I 
0.00.040.638 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.166 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.502 I llama_perf_context_print:        load time =      30.97 ms
0.00.044.503 I llama_perf_context_print: prompt eval time =       3.22 ms /     9 tokens (    0.36 ms per token,  2795.03 tokens per second)
0.00.044.503 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.504 I llama_perf_context_print:       total time =       3.89 ms /    10 tokens
0.00.044.703 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.283 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.026.812 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.828 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.834 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.836 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.040.837 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.840 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.040.841 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.040.842 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.040.843 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.040.844 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.040.844 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.040.845 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.040.846 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.040.852 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.040.852 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.040.853 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.040.854 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.863 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.048.671 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.050.797 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.530 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.055.532 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.533 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.055.534 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.055.534 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.055.534 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.055.535 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.055.535 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.055.536 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.055.536 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.055.536 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.055.537 I llama_model_loader: - type  f32:   40 tensors
0.00.055.542 I llama_model_loader: - type  f16:   30 tensors
0.00.055.543 I print_info: file format = GGUF V3 (latest)
0.00.055.544 I print_info: file type   = F16
0.00.055.545 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.060.010 W load: empty token at index 5
0.00.065.227 W load: model vocab missing newline token, using special_pad_id instead
0.00.066.700 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.066.734 I load: special tokens cache size = 5
0.00.323.686 I load: token to piece cache size = 1.5060 MB
0.00.323.692 I print_info: arch             = jina-bert-v2
0.00.323.692 I print_info: vocab_only       = 0
0.00.323.693 I print_info: n_ctx_train      = 8192
0.00.323.693 I print_info: n_embd           = 384
0.00.323.693 I print_info: n_layer          = 4
0.00.323.701 I print_info: n_head           = 12
0.00.323.703 I print_info: n_head_kv        = 12
0.00.323.703 I print_info: n_rot            = 32
0.00.323.703 I print_info: n_swa            = 0
0.00.323.703 I print_info: n_embd_head_k    = 32
0.00.323.703 I print_info: n_embd_head_v    = 32
0.00.323.704 I print_info: n_gqa            = 1
0.00.323.704 I print_info: n_embd_k_gqa     = 384
0.00.323.705 I print_info: n_embd_v_gqa     = 384
0.00.323.706 I print_info: f_norm_eps       = 1.0e-12
0.00.323.707 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.323.707 I print_info: f_clamp_kqv      = 0.0e+00
0.00.323.709 I print_info: f_max_alibi_bias = 8.0e+00
0.00.323.709 I print_info: f_logit_scale    = 0.0e+00
0.00.323.711 I print_info: n_ff             = 1536
0.00.323.711 I print_info: n_expert         = 0
0.00.323.711 I print_info: n_expert_used    = 0
0.00.323.711 I print_info: causal attn      = 0
0.00.323.711 I print_info: pooling type     = -1
0.00.323.711 I print_info: rope type        = -1
0.00.323.712 I print_info: rope scaling     = linear
0.00.323.712 I print_info: freq_base_train  = 10000.0
0.00.323.712 I print_info: freq_scale_train = 1
0.00.323.712 I print_info: n_ctx_orig_yarn  = 8192
0.00.323.713 I print_info: rope_finetuned   = unknown
0.00.323.714 I print_info: ssm_d_conv       = 0
0.00.323.714 I print_info: ssm_d_inner      = 0
0.00.323.714 I print_info: ssm_d_state      = 0
0.00.323.714 I print_info: ssm_dt_rank      = 0
0.00.323.715 I print_info: ssm_dt_b_c_rms   = 0
0.00.323.715 I print_info: model type       = 33M
0.00.323.715 I print_info: model params     = 32.90 M
0.00.323.715 I print_info: general.name     = Jina Bert Implementation
0.00.323.716 I print_info: vocab type       = BPE
0.00.323.717 I print_info: n_vocab          = 61056
0.00.323.717 I print_info: n_merges         = 39382
0.00.323.717 I print_info: BOS token        = 0 '<s>'
0.00.323.717 I print_info: EOS token        = 2 '</s>'
0.00.323.717 I print_info: UNK token        = 3 '<unk>'
0.00.323.718 I print_info: SEP token        = 2 '</s>'
0.00.323.718 I print_info: PAD token        = 1 '<pad>'
0.00.323.718 I print_info: MASK token       = 4 '<mask>'
0.00.323.718 I print_info: EOG token        = 2 '</s>'
0.00.323.719 I print_info: max token length = 45
0.00.323.719 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.326.197 I load_tensors: offloading 4 repeating layers to GPU
0.00.326.198 I load_tensors: offloading output layer to GPU
0.00.326.198 I load_tensors: offloaded 5/5 layers to GPU
0.00.326.223 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.326.224 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.326.655 I llama_init_from_model: n_seq_max     = 1
0.00.326.655 I llama_init_from_model: n_ctx         = 8192
0.00.326.656 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.326.656 I llama_init_from_model: n_batch       = 2048
0.00.326.656 I llama_init_from_model: n_ubatch      = 2048
0.00.326.656 I llama_init_from_model: flash_attn    = 0
0.00.326.656 I llama_init_from_model: freq_base     = 10000.0
0.00.326.657 I llama_init_from_model: freq_scale    = 1
0.00.326.657 I ggml_metal_init: allocating
0.00.326.661 I ggml_metal_init: found device: Apple M4
0.00.326.664 I ggml_metal_init: picking default device: Apple M4
0.00.327.218 I ggml_metal_init: using embedded metal library
0.00.329.729 I ggml_metal_init: GPU name:   Apple M4
0.00.329.731 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.329.731 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.329.732 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.329.732 I ggml_metal_init: simdgroup reduction   = true
0.00.329.732 I ggml_metal_init: simdgroup matrix mul. = true
0.00.329.732 I ggml_metal_init: has residency sets    = true
0.00.329.732 I ggml_metal_init: has bfloat            = true
0.00.329.732 I ggml_metal_init: use bfloat            = true
0.00.329.733 I ggml_metal_init: hasUnifiedMemory      = true
0.00.329.734 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.339.098 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.342.221 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.342.222 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.342.242 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.348.715 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.348.717 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.348.717 I llama_init_from_model: graph nodes  = 154
0.00.348.717 I llama_init_from_model: graph splits = 2
0.00.348.718 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.348.719 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.355.301 I 
0.00.355.340 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.355.541 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.355.542 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.355.551 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.355.551 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.355.558 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.355.558 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.356.136 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.359.951 I llama_perf_context_print:        load time =     328.48 ms
0.00.359.952 I llama_perf_context_print: prompt eval time =       3.81 ms /    62 tokens (    0.06 ms per token, 16285.79 tokens per second)
0.00.359.953 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.359.953 I llama_perf_context_print:       total time =       4.65 ms /    63 tokens
0.00.360.166 I ggml_metal_free: deallocating

real	0m1.150s
user	0m0.327s
sys	0m0.050s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.240 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.418 I main: llama backend init
0.00.000.425 I main: load the model and apply lora adapter, if any
0.00.057.071 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.070.354 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.070.379 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.070.383 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.070.384 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.070.385 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.070.385 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.070.386 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.070.389 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.070.389 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.070.390 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.070.391 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.070.392 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.070.393 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.070.393 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.070.397 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.070.398 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.070.398 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.077.470 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.079.848 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.087.193 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.087.209 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.087.209 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.087.220 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.087.221 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.087.223 I llama_model_loader: - type  f32:  194 tensors
0.00.087.224 I llama_model_loader: - type  f16:   98 tensors
0.00.087.226 I print_info: file format = GGUF V3 (latest)
0.00.087.227 I print_info: file type   = all F32 (guessed)
0.00.087.231 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.104.868 I load: special tokens cache size = 25
0.00.114.864 I load: token to piece cache size = 0.2984 MB
0.00.114.870 I print_info: arch             = gptneox
0.00.114.870 I print_info: vocab_only       = 0
0.00.114.870 I print_info: n_ctx_train      = 2048
0.00.114.872 I print_info: n_embd           = 2048
0.00.114.872 I print_info: n_layer          = 24
0.00.114.877 I print_info: n_head           = 16
0.00.114.879 I print_info: n_head_kv        = 16
0.00.114.879 I print_info: n_rot            = 32
0.00.114.879 I print_info: n_swa            = 0
0.00.114.879 I print_info: n_embd_head_k    = 128
0.00.114.881 I print_info: n_embd_head_v    = 128
0.00.114.881 I print_info: n_gqa            = 1
0.00.114.882 I print_info: n_embd_k_gqa     = 2048
0.00.114.885 I print_info: n_embd_v_gqa     = 2048
0.00.114.886 I print_info: f_norm_eps       = 1.0e-05
0.00.114.886 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.114.886 I print_info: f_clamp_kqv      = 0.0e+00
0.00.114.887 I print_info: f_max_alibi_bias = 0.0e+00
0.00.114.887 I print_info: f_logit_scale    = 0.0e+00
0.00.114.888 I print_info: n_ff             = 8192
0.00.114.888 I print_info: n_expert         = 0
0.00.114.888 I print_info: n_expert_used    = 0
0.00.114.888 I print_info: causal attn      = 1
0.00.114.889 I print_info: pooling type     = 0
0.00.114.889 I print_info: rope type        = 2
0.00.114.889 I print_info: rope scaling     = linear
0.00.114.890 I print_info: freq_base_train  = 10000.0
0.00.114.890 I print_info: freq_scale_train = 1
0.00.114.890 I print_info: n_ctx_orig_yarn  = 2048
0.00.114.891 I print_info: rope_finetuned   = unknown
0.00.114.891 I print_info: ssm_d_conv       = 0
0.00.114.891 I print_info: ssm_d_inner      = 0
0.00.114.891 I print_info: ssm_d_state      = 0
0.00.114.891 I print_info: ssm_dt_rank      = 0
0.00.114.891 I print_info: ssm_dt_b_c_rms   = 0
0.00.114.892 I print_info: model type       = 1.4B
0.00.114.892 I print_info: model params     = 1.41 B
0.00.114.892 I print_info: general.name     = 1.4B
0.00.114.893 I print_info: vocab type       = BPE
0.00.114.893 I print_info: n_vocab          = 50304
0.00.114.894 I print_info: n_merges         = 50009
0.00.114.894 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.114.894 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.114.894 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.114.895 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.114.895 I print_info: LF token         = 187 ''
0.00.114.895 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.114.895 I print_info: max token length = 1024
0.00.114.896 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.153.635 I load_tensors: offloading 24 repeating layers to GPU
0.00.153.639 I load_tensors: offloading output layer to GPU
0.00.153.640 I load_tensors: offloaded 25/25 layers to GPU
0.00.153.657 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.153.658 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.153.964 I llama_init_from_model: n_seq_max     = 1
0.00.153.966 I llama_init_from_model: n_ctx         = 2048
0.00.153.966 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.153.966 I llama_init_from_model: n_batch       = 2048
0.00.153.966 I llama_init_from_model: n_ubatch      = 512
0.00.153.966 I llama_init_from_model: flash_attn    = 0
0.00.153.967 I llama_init_from_model: freq_base     = 10000.0
0.00.153.967 I llama_init_from_model: freq_scale    = 1
0.00.153.968 I ggml_metal_init: allocating
0.00.153.984 I ggml_metal_init: found device: Apple M4
0.00.153.988 I ggml_metal_init: picking default device: Apple M4
0.00.154.570 I ggml_metal_init: using embedded metal library
0.00.165.020 I ggml_metal_init: GPU name:   Apple M4
0.00.165.022 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.165.022 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.165.023 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.165.023 I ggml_metal_init: simdgroup reduction   = true
0.00.165.023 I ggml_metal_init: simdgroup matrix mul. = true
0.00.165.023 I ggml_metal_init: has residency sets    = true
0.00.165.023 I ggml_metal_init: has bfloat            = true
0.00.165.023 I ggml_metal_init: use bfloat            = true
0.00.165.024 I ggml_metal_init: hasUnifiedMemory      = true
0.00.165.025 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.189.623 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.219.652 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.219.659 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.219.704 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.223.903 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.223.905 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.223.906 I llama_init_from_model: graph nodes  = 967
0.00.223.906 I llama_init_from_model: graph splits = 2
0.00.223.910 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.224.038 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.224.039 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.290.224 I main: llama threadpool init, n_threads = 4
0.00.290.262 I 
0.00.290.295 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.290.296 I 
0.00.290.338 I sampler seed: 1234
0.00.290.342 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.290.366 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.290.368 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.290.368 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.130.002 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59364.55 tokens per second)
0.02.130.002 I llama_perf_context_print:        load time =     232.31 ms
0.02.130.003 I llama_perf_context_print: prompt eval time =      53.47 ms /     7 tokens (    7.64 ms per token,   130.92 tokens per second)
0.02.130.004 I llama_perf_context_print:        eval time =    1783.30 ms /    63 runs   (   28.31 ms per token,    35.33 tokens per second)
0.02.130.004 I llama_perf_context_print:       total time =    1840.61 ms /    70 tokens
0.02.130.223 I ggml_metal_free: deallocating

real	0m2.527s
user	0m0.135s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.870 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.906 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.994 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.001 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.004 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.005 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.005 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.006 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.006 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.008 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.009 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.009 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.010 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.011 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.011 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.012 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.015 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.016 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.016 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.135 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.881 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.175 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.176 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.177 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.177 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.178 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.178 I llama_model_loader: - type  f32:  194 tensors
0.00.055.179 I llama_model_loader: - type  f16:   98 tensors
0.00.055.180 I print_info: file format = GGUF V3 (latest)
0.00.055.180 I print_info: file type   = all F32 (guessed)
0.00.055.181 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.066.953 I load: special tokens cache size = 25
0.00.074.360 I load: token to piece cache size = 0.2984 MB
0.00.074.364 I print_info: arch             = gptneox
0.00.074.364 I print_info: vocab_only       = 0
0.00.074.364 I print_info: n_ctx_train      = 2048
0.00.074.364 I print_info: n_embd           = 2048
0.00.074.364 I print_info: n_layer          = 24
0.00.074.367 I print_info: n_head           = 16
0.00.074.368 I print_info: n_head_kv        = 16
0.00.074.369 I print_info: n_rot            = 32
0.00.074.369 I print_info: n_swa            = 0
0.00.074.369 I print_info: n_embd_head_k    = 128
0.00.074.369 I print_info: n_embd_head_v    = 128
0.00.074.370 I print_info: n_gqa            = 1
0.00.074.371 I print_info: n_embd_k_gqa     = 2048
0.00.074.371 I print_info: n_embd_v_gqa     = 2048
0.00.074.373 I print_info: f_norm_eps       = 1.0e-05
0.00.074.374 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.074.374 I print_info: f_clamp_kqv      = 0.0e+00
0.00.074.374 I print_info: f_max_alibi_bias = 0.0e+00
0.00.074.374 I print_info: f_logit_scale    = 0.0e+00
0.00.074.375 I print_info: n_ff             = 8192
0.00.074.375 I print_info: n_expert         = 0
0.00.074.375 I print_info: n_expert_used    = 0
0.00.074.375 I print_info: causal attn      = 1
0.00.074.376 I print_info: pooling type     = 0
0.00.074.376 I print_info: rope type        = 2
0.00.074.376 I print_info: rope scaling     = linear
0.00.074.377 I print_info: freq_base_train  = 10000.0
0.00.074.377 I print_info: freq_scale_train = 1
0.00.074.378 I print_info: n_ctx_orig_yarn  = 2048
0.00.074.378 I print_info: rope_finetuned   = unknown
0.00.074.379 I print_info: ssm_d_conv       = 0
0.00.074.379 I print_info: ssm_d_inner      = 0
0.00.074.379 I print_info: ssm_d_state      = 0
0.00.074.379 I print_info: ssm_dt_rank      = 0
0.00.074.379 I print_info: ssm_dt_b_c_rms   = 0
0.00.074.379 I print_info: model type       = 1.4B
0.00.074.380 I print_info: model params     = 1.41 B
0.00.074.380 I print_info: general.name     = 1.4B
0.00.074.380 I print_info: vocab type       = BPE
0.00.074.381 I print_info: n_vocab          = 50304
0.00.074.381 I print_info: n_merges         = 50009
0.00.074.381 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.074.382 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.074.382 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.074.383 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.074.387 I print_info: LF token         = 187 ''
0.00.074.387 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.074.387 I print_info: max token length = 1024
0.00.074.388 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.083.455 I load_tensors: offloading 24 repeating layers to GPU
0.01.083.461 I load_tensors: offloading output layer to GPU
0.01.083.462 I load_tensors: offloaded 25/25 layers to GPU
0.01.083.492 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.083.494 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.084.536 I llama_init_from_model: n_seq_max     = 1
0.01.084.537 I llama_init_from_model: n_ctx         = 128
0.01.084.538 I llama_init_from_model: n_ctx_per_seq = 128
0.01.084.538 I llama_init_from_model: n_batch       = 128
0.01.084.538 I llama_init_from_model: n_ubatch      = 128
0.01.084.538 I llama_init_from_model: flash_attn    = 0
0.01.084.539 I llama_init_from_model: freq_base     = 10000.0
0.01.084.540 I llama_init_from_model: freq_scale    = 1
0.01.084.540 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.084.541 I ggml_metal_init: allocating
0.01.084.610 I ggml_metal_init: found device: Apple M4
0.01.084.617 I ggml_metal_init: picking default device: Apple M4
0.01.085.735 I ggml_metal_init: using embedded metal library
0.01.089.742 I ggml_metal_init: GPU name:   Apple M4
0.01.089.745 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.089.745 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.089.746 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.089.747 I ggml_metal_init: simdgroup reduction   = true
0.01.089.747 I ggml_metal_init: simdgroup matrix mul. = true
0.01.089.747 I ggml_metal_init: has residency sets    = true
0.01.089.747 I ggml_metal_init: has bfloat            = true
0.01.089.747 I ggml_metal_init: use bfloat            = true
0.01.089.748 I ggml_metal_init: hasUnifiedMemory      = true
0.01.089.748 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.101.657 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.103.438 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.103.440 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.103.467 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.105.175 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.105.176 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.105.177 I llama_init_from_model: graph nodes  = 967
0.01.105.177 I llama_init_from_model: graph splits = 2
0.01.105.178 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.105.178 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.141.692 I 
0.01.141.736 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.141.741 I perplexity: tokenizing the input ..
0.01.147.470 I perplexity: tokenization took 5.727 ms
0.01.147.475 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.266.315 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.267.860 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.267.894 I llama_perf_context_print:        load time =    1117.78 ms
0.01.267.895 I llama_perf_context_print: prompt eval time =     118.49 ms /   128 tokens (    0.93 ms per token,  1080.26 tokens per second)
0.01.267.896 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.267.896 I llama_perf_context_print:       total time =     126.20 ms /   129 tokens
0.01.268.220 I ggml_metal_free: deallocating

real	0m1.487s
user	0m0.100s
sys	0m0.215s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.010.052 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.587 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.593 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.595 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.596 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.596 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.596 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.597 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.598 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.598 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.599 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.599 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.601 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.602 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.602 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.604 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.604 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.604 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.413 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.522 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.389 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.390 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.390 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.391 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.391 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.392 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.392 I llama_model_loader: - type  f32:  194 tensors
0.00.033.393 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.393 I print_info: file format = GGUF V3 (latest)
0.00.033.396 I print_info: file type   = Q8_0
0.00.033.397 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.042.029 I load: special tokens cache size = 25
0.00.048.123 I load: token to piece cache size = 0.2984 MB
0.00.048.129 I print_info: arch             = gptneox
0.00.048.129 I print_info: vocab_only       = 0
0.00.048.129 I print_info: n_ctx_train      = 2048
0.00.048.130 I print_info: n_embd           = 2048
0.00.048.130 I print_info: n_layer          = 24
0.00.048.136 I print_info: n_head           = 16
0.00.048.137 I print_info: n_head_kv        = 16
0.00.048.137 I print_info: n_rot            = 32
0.00.048.138 I print_info: n_swa            = 0
0.00.048.138 I print_info: n_embd_head_k    = 128
0.00.048.138 I print_info: n_embd_head_v    = 128
0.00.048.139 I print_info: n_gqa            = 1
0.00.048.139 I print_info: n_embd_k_gqa     = 2048
0.00.048.140 I print_info: n_embd_v_gqa     = 2048
0.00.048.141 I print_info: f_norm_eps       = 1.0e-05
0.00.048.141 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.141 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.141 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.141 I print_info: f_logit_scale    = 0.0e+00
0.00.048.144 I print_info: n_ff             = 8192
0.00.048.144 I print_info: n_expert         = 0
0.00.048.144 I print_info: n_expert_used    = 0
0.00.048.144 I print_info: causal attn      = 1
0.00.048.144 I print_info: pooling type     = 0
0.00.048.145 I print_info: rope type        = 2
0.00.048.145 I print_info: rope scaling     = linear
0.00.048.146 I print_info: freq_base_train  = 10000.0
0.00.048.146 I print_info: freq_scale_train = 1
0.00.048.148 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.149 I print_info: rope_finetuned   = unknown
0.00.048.149 I print_info: ssm_d_conv       = 0
0.00.048.149 I print_info: ssm_d_inner      = 0
0.00.048.149 I print_info: ssm_d_state      = 0
0.00.048.149 I print_info: ssm_dt_rank      = 0
0.00.048.149 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.150 I print_info: model type       = 1.4B
0.00.048.150 I print_info: model params     = 1.41 B
0.00.048.150 I print_info: general.name     = 1.4B
0.00.048.151 I print_info: vocab type       = BPE
0.00.048.151 I print_info: n_vocab          = 50304
0.00.048.151 I print_info: n_merges         = 50009
0.00.048.152 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.152 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.152 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.152 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.152 I print_info: LF token         = 187 ''
0.00.048.152 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.153 I print_info: max token length = 1024
0.00.048.153 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.212.895 I load_tensors: offloading 24 repeating layers to GPU
0.01.212.901 I load_tensors: offloading output layer to GPU
0.01.212.903 I load_tensors: offloaded 25/25 layers to GPU
0.01.212.926 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.212.927 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.213.583 I llama_init_from_model: n_seq_max     = 1
0.01.213.585 I llama_init_from_model: n_ctx         = 2048
0.01.213.585 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.213.586 I llama_init_from_model: n_batch       = 2048
0.01.213.586 I llama_init_from_model: n_ubatch      = 512
0.01.213.586 I llama_init_from_model: flash_attn    = 0
0.01.213.587 I llama_init_from_model: freq_base     = 10000.0
0.01.213.587 I llama_init_from_model: freq_scale    = 1
0.01.213.589 I ggml_metal_init: allocating
0.01.213.603 I ggml_metal_init: found device: Apple M4
0.01.213.611 I ggml_metal_init: picking default device: Apple M4
0.01.214.931 I ggml_metal_init: using embedded metal library
0.01.220.689 I ggml_metal_init: GPU name:   Apple M4
0.01.220.692 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.220.693 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.220.694 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.220.694 I ggml_metal_init: simdgroup reduction   = true
0.01.220.694 I ggml_metal_init: simdgroup matrix mul. = true
0.01.220.695 I ggml_metal_init: has residency sets    = true
0.01.220.695 I ggml_metal_init: has bfloat            = true
0.01.220.695 I ggml_metal_init: use bfloat            = true
0.01.220.696 I ggml_metal_init: hasUnifiedMemory      = true
0.01.220.699 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.238.373 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.282.872 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.282.878 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.282.960 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.287.386 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.287.388 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.287.388 I llama_init_from_model: graph nodes  = 967
0.01.287.389 I llama_init_from_model: graph splits = 2
0.01.287.392 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.287.517 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.287.517 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.341.820 I main: llama threadpool init, n_threads = 4
0.01.341.868 I 
0.01.341.893 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.341.894 I 
0.01.342.049 I sampler seed: 1234
0.01.342.054 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.342.089 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.342.092 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.342.093 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.429.769 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57119.87 tokens per second)
0.02.429.769 I llama_perf_context_print:        load time =    1331.06 ms
0.02.429.770 I llama_perf_context_print: prompt eval time =      50.17 ms /     7 tokens (    7.17 ms per token,   139.53 tokens per second)
0.02.429.771 I llama_perf_context_print:        eval time =    1034.76 ms /    63 runs   (   16.42 ms per token,    60.88 tokens per second)
0.02.429.771 I llama_perf_context_print:       total time =    1088.65 ms /    70 tokens
0.02.430.016 I ggml_metal_free: deallocating

real	0m2.450s
user	0m0.108s
sys	0m0.255s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.241 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.491 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.733 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.737 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.744 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.744 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.745 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.745 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.747 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.747 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.748 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.748 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.748 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.749 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.749 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.749 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.751 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.751 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.751 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.732 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.783 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.545 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.547 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.547 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.548 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.548 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.548 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.549 I llama_model_loader: - type  f32:  194 tensors
0.00.026.549 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.550 I print_info: file format = GGUF V3 (latest)
0.00.026.550 I print_info: file type   = Q8_0
0.00.026.551 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.432 I load: special tokens cache size = 25
0.00.040.373 I load: token to piece cache size = 0.2984 MB
0.00.040.376 I print_info: arch             = gptneox
0.00.040.376 I print_info: vocab_only       = 0
0.00.040.376 I print_info: n_ctx_train      = 2048
0.00.040.376 I print_info: n_embd           = 2048
0.00.040.376 I print_info: n_layer          = 24
0.00.040.379 I print_info: n_head           = 16
0.00.040.380 I print_info: n_head_kv        = 16
0.00.040.380 I print_info: n_rot            = 32
0.00.040.380 I print_info: n_swa            = 0
0.00.040.382 I print_info: n_embd_head_k    = 128
0.00.040.382 I print_info: n_embd_head_v    = 128
0.00.040.383 I print_info: n_gqa            = 1
0.00.040.384 I print_info: n_embd_k_gqa     = 2048
0.00.040.385 I print_info: n_embd_v_gqa     = 2048
0.00.040.385 I print_info: f_norm_eps       = 1.0e-05
0.00.040.386 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.386 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.386 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.386 I print_info: f_logit_scale    = 0.0e+00
0.00.040.387 I print_info: n_ff             = 8192
0.00.040.387 I print_info: n_expert         = 0
0.00.040.387 I print_info: n_expert_used    = 0
0.00.040.387 I print_info: causal attn      = 1
0.00.040.387 I print_info: pooling type     = 0
0.00.040.387 I print_info: rope type        = 2
0.00.040.388 I print_info: rope scaling     = linear
0.00.040.388 I print_info: freq_base_train  = 10000.0
0.00.040.388 I print_info: freq_scale_train = 1
0.00.040.388 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.389 I print_info: rope_finetuned   = unknown
0.00.040.389 I print_info: ssm_d_conv       = 0
0.00.040.389 I print_info: ssm_d_inner      = 0
0.00.040.389 I print_info: ssm_d_state      = 0
0.00.040.389 I print_info: ssm_dt_rank      = 0
0.00.040.389 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.395 I print_info: model type       = 1.4B
0.00.040.395 I print_info: model params     = 1.41 B
0.00.040.395 I print_info: general.name     = 1.4B
0.00.040.396 I print_info: vocab type       = BPE
0.00.040.396 I print_info: n_vocab          = 50304
0.00.040.397 I print_info: n_merges         = 50009
0.00.040.399 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.399 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.399 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.399 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.399 I print_info: LF token         = 187 ''
0.00.040.400 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.400 I print_info: max token length = 1024
0.00.040.400 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.902.701 I load_tensors: offloading 24 repeating layers to GPU
0.00.902.715 I load_tensors: offloading output layer to GPU
0.00.902.716 I load_tensors: offloaded 25/25 layers to GPU
0.00.902.749 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.902.750 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.903.701 I llama_init_from_model: n_seq_max     = 1
0.00.903.708 I llama_init_from_model: n_ctx         = 128
0.00.903.709 I llama_init_from_model: n_ctx_per_seq = 128
0.00.903.709 I llama_init_from_model: n_batch       = 128
0.00.903.709 I llama_init_from_model: n_ubatch      = 128
0.00.903.710 I llama_init_from_model: flash_attn    = 0
0.00.903.711 I llama_init_from_model: freq_base     = 10000.0
0.00.903.712 I llama_init_from_model: freq_scale    = 1
0.00.903.713 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.903.714 I ggml_metal_init: allocating
0.00.903.803 I ggml_metal_init: found device: Apple M4
0.00.903.819 I ggml_metal_init: picking default device: Apple M4
0.00.905.558 I ggml_metal_init: using embedded metal library
0.00.910.050 I ggml_metal_init: GPU name:   Apple M4
0.00.910.054 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.910.055 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.910.055 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.910.056 I ggml_metal_init: simdgroup reduction   = true
0.00.910.056 I ggml_metal_init: simdgroup matrix mul. = true
0.00.910.056 I ggml_metal_init: has residency sets    = true
0.00.910.056 I ggml_metal_init: has bfloat            = true
0.00.910.056 I ggml_metal_init: use bfloat            = true
0.00.910.057 I ggml_metal_init: hasUnifiedMemory      = true
0.00.910.059 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.919.652 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.921.312 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.921.316 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.921.346 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.922.944 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.922.946 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.922.946 I llama_init_from_model: graph nodes  = 967
0.00.922.946 I llama_init_from_model: graph splits = 2
0.00.922.948 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.922.948 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.946.162 I 
0.00.946.196 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.946.199 I perplexity: tokenizing the input ..
0.00.950.190 I perplexity: tokenization took 3.99 ms
0.00.950.193 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.073.673 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.075.277 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.075.303 I llama_perf_context_print:        load time =     935.67 ms
0.01.075.305 I llama_perf_context_print: prompt eval time =     123.24 ms /   128 tokens (    0.96 ms per token,  1038.66 tokens per second)
0.01.075.306 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.075.306 I llama_perf_context_print:       total time =     129.14 ms /   129 tokens
0.01.075.735 I ggml_metal_free: deallocating

real	0m1.094s
user	0m0.067s
sys	0m0.133s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.061 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.100 I main: llama backend init
0.00.000.102 I main: load the model and apply lora adapter, if any
0.00.011.057 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.832 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.838 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.840 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.841 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.841 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.841 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.842 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.843 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.843 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.844 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.844 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.845 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.846 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.846 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.848 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.848 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.848 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.693 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.792 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.572 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.574 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.574 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.574 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.575 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.575 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.576 I llama_model_loader: - type  f32:  194 tensors
0.00.027.576 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.576 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.577 I print_info: file format = GGUF V3 (latest)
0.00.027.578 I print_info: file type   = Q4_0
0.00.027.579 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.990 I load: special tokens cache size = 25
0.00.042.078 I load: token to piece cache size = 0.2984 MB
0.00.042.080 I print_info: arch             = gptneox
0.00.042.080 I print_info: vocab_only       = 0
0.00.042.081 I print_info: n_ctx_train      = 2048
0.00.042.081 I print_info: n_embd           = 2048
0.00.042.081 I print_info: n_layer          = 24
0.00.042.086 I print_info: n_head           = 16
0.00.042.087 I print_info: n_head_kv        = 16
0.00.042.089 I print_info: n_rot            = 32
0.00.042.090 I print_info: n_swa            = 0
0.00.042.090 I print_info: n_embd_head_k    = 128
0.00.042.090 I print_info: n_embd_head_v    = 128
0.00.042.091 I print_info: n_gqa            = 1
0.00.042.091 I print_info: n_embd_k_gqa     = 2048
0.00.042.092 I print_info: n_embd_v_gqa     = 2048
0.00.042.093 I print_info: f_norm_eps       = 1.0e-05
0.00.042.094 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.094 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.094 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.094 I print_info: f_logit_scale    = 0.0e+00
0.00.042.095 I print_info: n_ff             = 8192
0.00.042.095 I print_info: n_expert         = 0
0.00.042.095 I print_info: n_expert_used    = 0
0.00.042.095 I print_info: causal attn      = 1
0.00.042.096 I print_info: pooling type     = 0
0.00.042.096 I print_info: rope type        = 2
0.00.042.097 I print_info: rope scaling     = linear
0.00.042.098 I print_info: freq_base_train  = 10000.0
0.00.042.098 I print_info: freq_scale_train = 1
0.00.042.098 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.099 I print_info: rope_finetuned   = unknown
0.00.042.099 I print_info: ssm_d_conv       = 0
0.00.042.099 I print_info: ssm_d_inner      = 0
0.00.042.099 I print_info: ssm_d_state      = 0
0.00.042.099 I print_info: ssm_dt_rank      = 0
0.00.042.099 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.100 I print_info: model type       = 1.4B
0.00.042.100 I print_info: model params     = 1.41 B
0.00.042.100 I print_info: general.name     = 1.4B
0.00.042.105 I print_info: vocab type       = BPE
0.00.042.105 I print_info: n_vocab          = 50304
0.00.042.107 I print_info: n_merges         = 50009
0.00.042.107 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.107 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.107 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.108 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.108 I print_info: LF token         = 187 ''
0.00.042.108 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.108 I print_info: max token length = 1024
0.00.042.109 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.607.467 I load_tensors: offloading 24 repeating layers to GPU
0.00.607.480 I load_tensors: offloading output layer to GPU
0.00.607.481 I load_tensors: offloaded 25/25 layers to GPU
0.00.607.517 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.607.519 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.608.933 I llama_init_from_model: n_seq_max     = 1
0.00.608.944 I llama_init_from_model: n_ctx         = 2048
0.00.608.945 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.608.945 I llama_init_from_model: n_batch       = 2048
0.00.608.946 I llama_init_from_model: n_ubatch      = 512
0.00.608.946 I llama_init_from_model: flash_attn    = 0
0.00.608.947 I llama_init_from_model: freq_base     = 10000.0
0.00.608.948 I llama_init_from_model: freq_scale    = 1
0.00.608.950 I ggml_metal_init: allocating
0.00.608.998 I ggml_metal_init: found device: Apple M4
0.00.609.008 I ggml_metal_init: picking default device: Apple M4
0.00.610.721 I ggml_metal_init: using embedded metal library
0.00.616.310 I ggml_metal_init: GPU name:   Apple M4
0.00.616.316 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.616.317 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.616.318 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.616.319 I ggml_metal_init: simdgroup reduction   = true
0.00.616.319 I ggml_metal_init: simdgroup matrix mul. = true
0.00.616.319 I ggml_metal_init: has residency sets    = true
0.00.616.320 I ggml_metal_init: has bfloat            = true
0.00.616.320 I ggml_metal_init: use bfloat            = true
0.00.616.321 I ggml_metal_init: hasUnifiedMemory      = true
0.00.616.323 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.561 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.697.724 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.697.734 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.697.829 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.703.289 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.703.292 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.703.292 I llama_init_from_model: graph nodes  = 967
0.00.703.292 I llama_init_from_model: graph splits = 2
0.00.703.298 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.703.423 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.703.424 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.473 I main: llama threadpool init, n_threads = 4
0.00.762.512 I 
0.00.762.537 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.762.537 I 
0.00.762.717 I sampler seed: 1234
0.00.762.721 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.762.755 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.762.759 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.762.759 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.437.127 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50426.14 tokens per second)
0.01.437.128 I llama_perf_context_print:        load time =     750.71 ms
0.01.437.129 I llama_perf_context_print: prompt eval time =      45.76 ms /     7 tokens (    6.54 ms per token,   152.97 tokens per second)
0.01.437.129 I llama_perf_context_print:        eval time =     625.62 ms /    63 runs   (    9.93 ms per token,   100.70 tokens per second)
0.01.437.130 I llama_perf_context_print:       total time =     675.36 ms /    70 tokens
0.01.437.362 I ggml_metal_free: deallocating

real	0m1.457s
user	0m0.110s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.243 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.155 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.830 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.836 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.838 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.838 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.839 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.839 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.839 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.842 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.842 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.842 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.843 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.844 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.845 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.845 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.847 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.847 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.847 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.845 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.935 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.061 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.063 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.063 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.063 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.063 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.064 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.064 I llama_model_loader: - type  f32:  194 tensors
0.00.027.065 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.065 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.065 I print_info: file format = GGUF V3 (latest)
0.00.027.066 I print_info: file type   = Q4_0
0.00.027.067 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.690 I load: special tokens cache size = 25
0.00.041.769 I load: token to piece cache size = 0.2984 MB
0.00.041.775 I print_info: arch             = gptneox
0.00.041.775 I print_info: vocab_only       = 0
0.00.041.776 I print_info: n_ctx_train      = 2048
0.00.041.776 I print_info: n_embd           = 2048
0.00.041.776 I print_info: n_layer          = 24
0.00.041.781 I print_info: n_head           = 16
0.00.041.784 I print_info: n_head_kv        = 16
0.00.041.784 I print_info: n_rot            = 32
0.00.041.784 I print_info: n_swa            = 0
0.00.041.785 I print_info: n_embd_head_k    = 128
0.00.041.785 I print_info: n_embd_head_v    = 128
0.00.041.785 I print_info: n_gqa            = 1
0.00.041.787 I print_info: n_embd_k_gqa     = 2048
0.00.041.787 I print_info: n_embd_v_gqa     = 2048
0.00.041.788 I print_info: f_norm_eps       = 1.0e-05
0.00.041.788 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.788 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.788 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.789 I print_info: f_logit_scale    = 0.0e+00
0.00.041.789 I print_info: n_ff             = 8192
0.00.041.789 I print_info: n_expert         = 0
0.00.041.790 I print_info: n_expert_used    = 0
0.00.041.790 I print_info: causal attn      = 1
0.00.041.790 I print_info: pooling type     = 0
0.00.041.791 I print_info: rope type        = 2
0.00.041.792 I print_info: rope scaling     = linear
0.00.041.792 I print_info: freq_base_train  = 10000.0
0.00.041.792 I print_info: freq_scale_train = 1
0.00.041.792 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.793 I print_info: rope_finetuned   = unknown
0.00.041.793 I print_info: ssm_d_conv       = 0
0.00.041.793 I print_info: ssm_d_inner      = 0
0.00.041.793 I print_info: ssm_d_state      = 0
0.00.041.793 I print_info: ssm_dt_rank      = 0
0.00.041.793 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.793 I print_info: model type       = 1.4B
0.00.041.794 I print_info: model params     = 1.41 B
0.00.041.794 I print_info: general.name     = 1.4B
0.00.041.794 I print_info: vocab type       = BPE
0.00.041.795 I print_info: n_vocab          = 50304
0.00.041.799 I print_info: n_merges         = 50009
0.00.041.799 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.799 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.800 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.800 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.800 I print_info: LF token         = 187 ''
0.00.041.800 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.800 I print_info: max token length = 1024
0.00.041.801 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.656.321 I load_tensors: offloading 24 repeating layers to GPU
0.00.656.328 I load_tensors: offloading output layer to GPU
0.00.656.329 I load_tensors: offloaded 25/25 layers to GPU
0.00.656.346 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.656.347 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.657.073 I llama_init_from_model: n_seq_max     = 1
0.00.657.079 I llama_init_from_model: n_ctx         = 128
0.00.657.079 I llama_init_from_model: n_ctx_per_seq = 128
0.00.657.080 I llama_init_from_model: n_batch       = 128
0.00.657.080 I llama_init_from_model: n_ubatch      = 128
0.00.657.080 I llama_init_from_model: flash_attn    = 0
0.00.657.081 I llama_init_from_model: freq_base     = 10000.0
0.00.657.082 I llama_init_from_model: freq_scale    = 1
0.00.657.082 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.657.085 I ggml_metal_init: allocating
0.00.657.122 I ggml_metal_init: found device: Apple M4
0.00.657.132 I ggml_metal_init: picking default device: Apple M4
0.00.658.176 I ggml_metal_init: using embedded metal library
0.00.667.294 I ggml_metal_init: GPU name:   Apple M4
0.00.667.301 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.667.301 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.667.302 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.667.302 I ggml_metal_init: simdgroup reduction   = true
0.00.667.303 I ggml_metal_init: simdgroup matrix mul. = true
0.00.667.303 I ggml_metal_init: has residency sets    = true
0.00.667.303 I ggml_metal_init: has bfloat            = true
0.00.667.303 I ggml_metal_init: use bfloat            = true
0.00.667.304 I ggml_metal_init: hasUnifiedMemory      = true
0.00.667.307 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.683.858 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.685.509 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.685.511 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.685.534 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.687.125 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.687.126 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.687.126 I llama_init_from_model: graph nodes  = 967
0.00.687.126 I llama_init_from_model: graph splits = 2
0.00.687.128 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.687.128 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.124 I 
0.00.712.164 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.167 I perplexity: tokenizing the input ..
0.00.716.064 I perplexity: tokenization took 3.896 ms
0.00.716.068 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.849.292 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.850.697 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.850.721 I llama_perf_context_print:        load time =     701.96 ms
0.00.850.722 I llama_perf_context_print: prompt eval time =     132.97 ms /   128 tokens (    1.04 ms per token,   962.64 tokens per second)
0.00.850.723 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.850.723 I llama_perf_context_print:       total time =     138.60 ms /   129 tokens
0.00.851.158 I ggml_metal_free: deallocating

real	0m0.868s
user	0m0.073s
sys	0m0.095s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.819 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.977 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.982 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.987 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.988 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.988 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.989 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.989 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.990 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.990 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.991 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.991 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.991 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.992 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.992 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.994 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.994 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.994 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.830 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.844 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.623 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.624 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.624 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.625 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.625 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.625 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.626 I llama_model_loader: - type  f32:  194 tensors
0.00.025.626 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.626 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.627 I print_info: file format = GGUF V3 (latest)
0.00.025.627 I print_info: file type   = Q4_1
0.00.025.628 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.785 I load: special tokens cache size = 25
0.00.040.055 I load: token to piece cache size = 0.2984 MB
0.00.040.057 I print_info: arch             = gptneox
0.00.040.058 I print_info: vocab_only       = 0
0.00.040.058 I print_info: n_ctx_train      = 2048
0.00.040.058 I print_info: n_embd           = 2048
0.00.040.058 I print_info: n_layer          = 24
0.00.040.061 I print_info: n_head           = 16
0.00.040.062 I print_info: n_head_kv        = 16
0.00.040.062 I print_info: n_rot            = 32
0.00.040.062 I print_info: n_swa            = 0
0.00.040.062 I print_info: n_embd_head_k    = 128
0.00.040.062 I print_info: n_embd_head_v    = 128
0.00.040.063 I print_info: n_gqa            = 1
0.00.040.064 I print_info: n_embd_k_gqa     = 2048
0.00.040.064 I print_info: n_embd_v_gqa     = 2048
0.00.040.065 I print_info: f_norm_eps       = 1.0e-05
0.00.040.066 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.066 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.066 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.066 I print_info: f_logit_scale    = 0.0e+00
0.00.040.067 I print_info: n_ff             = 8192
0.00.040.067 I print_info: n_expert         = 0
0.00.040.067 I print_info: n_expert_used    = 0
0.00.040.067 I print_info: causal attn      = 1
0.00.040.067 I print_info: pooling type     = 0
0.00.040.067 I print_info: rope type        = 2
0.00.040.068 I print_info: rope scaling     = linear
0.00.040.068 I print_info: freq_base_train  = 10000.0
0.00.040.070 I print_info: freq_scale_train = 1
0.00.040.071 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.071 I print_info: rope_finetuned   = unknown
0.00.040.071 I print_info: ssm_d_conv       = 0
0.00.040.071 I print_info: ssm_d_inner      = 0
0.00.040.071 I print_info: ssm_d_state      = 0
0.00.040.071 I print_info: ssm_dt_rank      = 0
0.00.040.071 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.072 I print_info: model type       = 1.4B
0.00.040.072 I print_info: model params     = 1.41 B
0.00.040.072 I print_info: general.name     = 1.4B
0.00.040.073 I print_info: vocab type       = BPE
0.00.040.073 I print_info: n_vocab          = 50304
0.00.040.073 I print_info: n_merges         = 50009
0.00.040.074 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.074 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.074 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.074 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.074 I print_info: LF token         = 187 ''
0.00.040.075 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.075 I print_info: max token length = 1024
0.00.040.075 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.627.508 I load_tensors: offloading 24 repeating layers to GPU
0.00.627.524 I load_tensors: offloading output layer to GPU
0.00.627.525 I load_tensors: offloaded 25/25 layers to GPU
0.00.627.559 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.627.563 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.628.894 I llama_init_from_model: n_seq_max     = 1
0.00.628.900 I llama_init_from_model: n_ctx         = 2048
0.00.628.900 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.628.901 I llama_init_from_model: n_batch       = 2048
0.00.628.901 I llama_init_from_model: n_ubatch      = 512
0.00.628.902 I llama_init_from_model: flash_attn    = 0
0.00.628.903 I llama_init_from_model: freq_base     = 10000.0
0.00.628.904 I llama_init_from_model: freq_scale    = 1
0.00.628.906 I ggml_metal_init: allocating
0.00.628.980 I ggml_metal_init: found device: Apple M4
0.00.628.993 I ggml_metal_init: picking default device: Apple M4
0.00.630.831 I ggml_metal_init: using embedded metal library
0.00.636.320 I ggml_metal_init: GPU name:   Apple M4
0.00.636.324 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.636.325 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.636.326 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.636.327 I ggml_metal_init: simdgroup reduction   = true
0.00.636.327 I ggml_metal_init: simdgroup matrix mul. = true
0.00.636.327 I ggml_metal_init: has residency sets    = true
0.00.636.328 I ggml_metal_init: has bfloat            = true
0.00.636.328 I ggml_metal_init: use bfloat            = true
0.00.636.329 I ggml_metal_init: hasUnifiedMemory      = true
0.00.636.330 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.655.401 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.710.006 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.710.013 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.710.048 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.714.718 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.714.721 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.714.721 I llama_init_from_model: graph nodes  = 967
0.00.714.721 I llama_init_from_model: graph splits = 2
0.00.714.727 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.714.851 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.714.851 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.764.146 I main: llama threadpool init, n_threads = 4
0.00.764.191 I 
0.00.764.237 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.764.256 I 
0.00.764.410 I sampler seed: 1234
0.00.764.417 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.764.443 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.764.446 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.764.446 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.490.679 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57443.37 tokens per second)
0.01.490.679 I llama_perf_context_print:        load time =     754.66 ms
0.01.490.682 I llama_perf_context_print: prompt eval time =      39.76 ms /     7 tokens (    5.68 ms per token,   176.06 tokens per second)
0.01.490.682 I llama_perf_context_print:        eval time =     683.77 ms /    63 runs   (   10.85 ms per token,    92.14 tokens per second)
0.01.490.683 I llama_perf_context_print:       total time =     727.20 ms /    70 tokens
0.01.490.914 I ggml_metal_free: deallocating

real	0m1.508s
user	0m0.109s
sys	0m0.198s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.693 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.881 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.022.887 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.888 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.889 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.889 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.889 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.890 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.891 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.891 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.891 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.892 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.892 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.892 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.893 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.895 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.895 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.895 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.809 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.856 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.702 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.703 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.704 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.704 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.705 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.705 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.031.705 I llama_model_loader: - type  f32:  194 tensors
0.00.031.706 I llama_model_loader: - type q4_1:   97 tensors
0.00.031.706 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.707 I print_info: file format = GGUF V3 (latest)
0.00.031.707 I print_info: file type   = Q4_1
0.00.031.713 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.039.936 I load: special tokens cache size = 25
0.00.045.984 I load: token to piece cache size = 0.2984 MB
0.00.045.987 I print_info: arch             = gptneox
0.00.045.987 I print_info: vocab_only       = 0
0.00.045.987 I print_info: n_ctx_train      = 2048
0.00.045.988 I print_info: n_embd           = 2048
0.00.045.988 I print_info: n_layer          = 24
0.00.045.991 I print_info: n_head           = 16
0.00.045.992 I print_info: n_head_kv        = 16
0.00.045.992 I print_info: n_rot            = 32
0.00.045.994 I print_info: n_swa            = 0
0.00.045.994 I print_info: n_embd_head_k    = 128
0.00.045.994 I print_info: n_embd_head_v    = 128
0.00.045.995 I print_info: n_gqa            = 1
0.00.045.996 I print_info: n_embd_k_gqa     = 2048
0.00.046.001 I print_info: n_embd_v_gqa     = 2048
0.00.046.001 I print_info: f_norm_eps       = 1.0e-05
0.00.046.002 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.046.002 I print_info: f_clamp_kqv      = 0.0e+00
0.00.046.002 I print_info: f_max_alibi_bias = 0.0e+00
0.00.046.002 I print_info: f_logit_scale    = 0.0e+00
0.00.046.003 I print_info: n_ff             = 8192
0.00.046.003 I print_info: n_expert         = 0
0.00.046.004 I print_info: n_expert_used    = 0
0.00.046.004 I print_info: causal attn      = 1
0.00.046.007 I print_info: pooling type     = 0
0.00.046.007 I print_info: rope type        = 2
0.00.046.007 I print_info: rope scaling     = linear
0.00.046.007 I print_info: freq_base_train  = 10000.0
0.00.046.008 I print_info: freq_scale_train = 1
0.00.046.009 I print_info: n_ctx_orig_yarn  = 2048
0.00.046.010 I print_info: rope_finetuned   = unknown
0.00.046.010 I print_info: ssm_d_conv       = 0
0.00.046.010 I print_info: ssm_d_inner      = 0
0.00.046.010 I print_info: ssm_d_state      = 0
0.00.046.010 I print_info: ssm_dt_rank      = 0
0.00.046.010 I print_info: ssm_dt_b_c_rms   = 0
0.00.046.011 I print_info: model type       = 1.4B
0.00.046.011 I print_info: model params     = 1.41 B
0.00.046.011 I print_info: general.name     = 1.4B
0.00.046.012 I print_info: vocab type       = BPE
0.00.046.012 I print_info: n_vocab          = 50304
0.00.046.012 I print_info: n_merges         = 50009
0.00.046.012 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.012 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.012 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.012 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.013 I print_info: LF token         = 187 ''
0.00.046.013 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.013 I print_info: max token length = 1024
0.00.046.014 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.697.369 I load_tensors: offloading 24 repeating layers to GPU
0.00.697.385 I load_tensors: offloading output layer to GPU
0.00.697.385 I load_tensors: offloaded 25/25 layers to GPU
0.00.697.420 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.697.422 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.698.955 I llama_init_from_model: n_seq_max     = 1
0.00.698.959 I llama_init_from_model: n_ctx         = 128
0.00.698.960 I llama_init_from_model: n_ctx_per_seq = 128
0.00.698.960 I llama_init_from_model: n_batch       = 128
0.00.698.961 I llama_init_from_model: n_ubatch      = 128
0.00.698.961 I llama_init_from_model: flash_attn    = 0
0.00.698.964 I llama_init_from_model: freq_base     = 10000.0
0.00.698.964 I llama_init_from_model: freq_scale    = 1
0.00.698.965 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.698.979 I ggml_metal_init: allocating
0.00.699.070 I ggml_metal_init: found device: Apple M4
0.00.699.085 I ggml_metal_init: picking default device: Apple M4
0.00.700.891 I ggml_metal_init: using embedded metal library
0.00.707.492 I ggml_metal_init: GPU name:   Apple M4
0.00.707.496 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.707.497 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.707.498 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.707.499 I ggml_metal_init: simdgroup reduction   = true
0.00.707.499 I ggml_metal_init: simdgroup matrix mul. = true
0.00.707.499 I ggml_metal_init: has residency sets    = true
0.00.707.500 I ggml_metal_init: has bfloat            = true
0.00.707.500 I ggml_metal_init: use bfloat            = true
0.00.707.501 I ggml_metal_init: hasUnifiedMemory      = true
0.00.707.503 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.725.635 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.729.282 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.729.286 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.729.327 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.732.607 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.732.609 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.732.610 I llama_init_from_model: graph nodes  = 967
0.00.732.610 I llama_init_from_model: graph splits = 2
0.00.732.613 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.732.613 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.761.683 I 
0.00.761.768 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.761.776 I perplexity: tokenizing the input ..
0.00.768.664 I perplexity: tokenization took 6.886 ms
0.00.768.668 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.904.735 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.906.267 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.906.291 I llama_perf_context_print:        load time =     752.98 ms
0.00.906.292 I llama_perf_context_print: prompt eval time =     135.84 ms /   128 tokens (    1.06 ms per token,   942.29 tokens per second)
0.00.906.293 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.906.293 I llama_perf_context_print:       total time =     144.61 ms /   129 tokens
0.00.906.664 I ggml_metal_free: deallocating

real	0m0.920s
user	0m0.080s
sys	0m0.129s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.709 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.359 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.364 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.371 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.371 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.372 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.372 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.372 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.373 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.374 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.374 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.375 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.375 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.375 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.376 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.377 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.378 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.378 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.251 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.252 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.048 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.049 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.050 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.050 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.050 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.051 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.051 I llama_model_loader: - type  f32:  194 tensors
0.00.026.051 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.052 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.052 I print_info: file format = GGUF V3 (latest)
0.00.026.053 I print_info: file type   = Q5_0
0.00.026.053 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.239 I load: special tokens cache size = 25
0.00.040.493 I load: token to piece cache size = 0.2984 MB
0.00.040.496 I print_info: arch             = gptneox
0.00.040.496 I print_info: vocab_only       = 0
0.00.040.496 I print_info: n_ctx_train      = 2048
0.00.040.496 I print_info: n_embd           = 2048
0.00.040.496 I print_info: n_layer          = 24
0.00.040.499 I print_info: n_head           = 16
0.00.040.500 I print_info: n_head_kv        = 16
0.00.040.502 I print_info: n_rot            = 32
0.00.040.502 I print_info: n_swa            = 0
0.00.040.502 I print_info: n_embd_head_k    = 128
0.00.040.502 I print_info: n_embd_head_v    = 128
0.00.040.503 I print_info: n_gqa            = 1
0.00.040.504 I print_info: n_embd_k_gqa     = 2048
0.00.040.513 I print_info: n_embd_v_gqa     = 2048
0.00.040.514 I print_info: f_norm_eps       = 1.0e-05
0.00.040.515 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.515 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.515 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.516 I print_info: f_logit_scale    = 0.0e+00
0.00.040.517 I print_info: n_ff             = 8192
0.00.040.518 I print_info: n_expert         = 0
0.00.040.518 I print_info: n_expert_used    = 0
0.00.040.518 I print_info: causal attn      = 1
0.00.040.518 I print_info: pooling type     = 0
0.00.040.518 I print_info: rope type        = 2
0.00.040.519 I print_info: rope scaling     = linear
0.00.040.519 I print_info: freq_base_train  = 10000.0
0.00.040.519 I print_info: freq_scale_train = 1
0.00.040.521 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.521 I print_info: rope_finetuned   = unknown
0.00.040.521 I print_info: ssm_d_conv       = 0
0.00.040.521 I print_info: ssm_d_inner      = 0
0.00.040.521 I print_info: ssm_d_state      = 0
0.00.040.521 I print_info: ssm_dt_rank      = 0
0.00.040.521 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.522 I print_info: model type       = 1.4B
0.00.040.522 I print_info: model params     = 1.41 B
0.00.040.522 I print_info: general.name     = 1.4B
0.00.040.523 I print_info: vocab type       = BPE
0.00.040.523 I print_info: n_vocab          = 50304
0.00.040.524 I print_info: n_merges         = 50009
0.00.040.525 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.525 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.525 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.525 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.525 I print_info: LF token         = 187 ''
0.00.040.526 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.526 I print_info: max token length = 1024
0.00.040.526 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.701.407 I load_tensors: offloading 24 repeating layers to GPU
0.00.701.423 I load_tensors: offloading output layer to GPU
0.00.701.424 I load_tensors: offloaded 25/25 layers to GPU
0.00.701.458 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.701.460 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.702.821 I llama_init_from_model: n_seq_max     = 1
0.00.702.826 I llama_init_from_model: n_ctx         = 2048
0.00.702.826 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.702.827 I llama_init_from_model: n_batch       = 2048
0.00.702.827 I llama_init_from_model: n_ubatch      = 512
0.00.702.828 I llama_init_from_model: flash_attn    = 0
0.00.702.830 I llama_init_from_model: freq_base     = 10000.0
0.00.702.830 I llama_init_from_model: freq_scale    = 1
0.00.702.845 I ggml_metal_init: allocating
0.00.702.911 I ggml_metal_init: found device: Apple M4
0.00.702.925 I ggml_metal_init: picking default device: Apple M4
0.00.704.769 I ggml_metal_init: using embedded metal library
0.00.711.518 I ggml_metal_init: GPU name:   Apple M4
0.00.711.522 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.711.523 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.711.524 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.711.525 I ggml_metal_init: simdgroup reduction   = true
0.00.711.525 I ggml_metal_init: simdgroup matrix mul. = true
0.00.711.525 I ggml_metal_init: has residency sets    = true
0.00.711.526 I ggml_metal_init: has bfloat            = true
0.00.711.526 I ggml_metal_init: use bfloat            = true
0.00.711.527 I ggml_metal_init: hasUnifiedMemory      = true
0.00.711.529 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.730.024 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.787.717 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.787.723 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.787.760 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.792.348 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.792.350 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.792.350 I llama_init_from_model: graph nodes  = 967
0.00.792.350 I llama_init_from_model: graph splits = 2
0.00.792.355 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.792.483 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.792.484 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.851.883 I main: llama threadpool init, n_threads = 4
0.00.851.929 I 
0.00.851.953 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.851.953 I 
0.00.852.101 I sampler seed: 1234
0.00.852.106 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.852.117 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.852.117 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.852.119 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.649.129 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51042.42 tokens per second)
0.01.649.130 I llama_perf_context_print:        load time =     841.48 ms
0.01.649.131 I llama_perf_context_print: prompt eval time =      49.62 ms /     7 tokens (    7.09 ms per token,   141.06 tokens per second)
0.01.649.135 I llama_perf_context_print:        eval time =     744.68 ms /    63 runs   (   11.82 ms per token,    84.60 tokens per second)
0.01.649.135 I llama_perf_context_print:       total time =     797.93 ms /    70 tokens
0.01.649.412 I ggml_metal_free: deallocating

real	0m1.668s
user	0m0.109s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.026.734 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.742 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.035.748 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.749 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.750 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.750 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.751 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.751 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.752 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.752 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.752 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.753 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.753 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.754 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.754 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.756 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.756 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.757 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.673 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.986 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.678 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.045.679 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.680 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.680 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.680 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.685 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.045.686 I llama_model_loader: - type  f32:  194 tensors
0.00.045.687 I llama_model_loader: - type q5_0:   97 tensors
0.00.045.687 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.688 I print_info: file format = GGUF V3 (latest)
0.00.045.689 I print_info: file type   = Q5_0
0.00.045.690 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.054.995 I load: special tokens cache size = 25
0.00.062.447 I load: token to piece cache size = 0.2984 MB
0.00.062.450 I print_info: arch             = gptneox
0.00.062.450 I print_info: vocab_only       = 0
0.00.062.450 I print_info: n_ctx_train      = 2048
0.00.062.450 I print_info: n_embd           = 2048
0.00.062.450 I print_info: n_layer          = 24
0.00.062.453 I print_info: n_head           = 16
0.00.062.454 I print_info: n_head_kv        = 16
0.00.062.454 I print_info: n_rot            = 32
0.00.062.454 I print_info: n_swa            = 0
0.00.062.454 I print_info: n_embd_head_k    = 128
0.00.062.454 I print_info: n_embd_head_v    = 128
0.00.062.455 I print_info: n_gqa            = 1
0.00.062.456 I print_info: n_embd_k_gqa     = 2048
0.00.062.457 I print_info: n_embd_v_gqa     = 2048
0.00.062.457 I print_info: f_norm_eps       = 1.0e-05
0.00.062.458 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.458 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.458 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.458 I print_info: f_logit_scale    = 0.0e+00
0.00.062.459 I print_info: n_ff             = 8192
0.00.062.459 I print_info: n_expert         = 0
0.00.062.459 I print_info: n_expert_used    = 0
0.00.062.460 I print_info: causal attn      = 1
0.00.062.460 I print_info: pooling type     = 0
0.00.062.460 I print_info: rope type        = 2
0.00.062.460 I print_info: rope scaling     = linear
0.00.062.460 I print_info: freq_base_train  = 10000.0
0.00.062.461 I print_info: freq_scale_train = 1
0.00.062.461 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.461 I print_info: rope_finetuned   = unknown
0.00.062.461 I print_info: ssm_d_conv       = 0
0.00.062.462 I print_info: ssm_d_inner      = 0
0.00.062.462 I print_info: ssm_d_state      = 0
0.00.062.462 I print_info: ssm_dt_rank      = 0
0.00.062.462 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.462 I print_info: model type       = 1.4B
0.00.062.463 I print_info: model params     = 1.41 B
0.00.062.463 I print_info: general.name     = 1.4B
0.00.062.464 I print_info: vocab type       = BPE
0.00.062.464 I print_info: n_vocab          = 50304
0.00.062.464 I print_info: n_merges         = 50009
0.00.062.464 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.465 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.465 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.465 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.465 I print_info: LF token         = 187 ''
0.00.062.466 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.466 I print_info: max token length = 1024
0.00.062.466 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.775.833 I load_tensors: offloading 24 repeating layers to GPU
0.00.775.846 I load_tensors: offloading output layer to GPU
0.00.775.847 I load_tensors: offloaded 25/25 layers to GPU
0.00.775.873 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.775.875 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.777.255 I llama_init_from_model: n_seq_max     = 1
0.00.777.261 I llama_init_from_model: n_ctx         = 128
0.00.777.262 I llama_init_from_model: n_ctx_per_seq = 128
0.00.777.263 I llama_init_from_model: n_batch       = 128
0.00.777.263 I llama_init_from_model: n_ubatch      = 128
0.00.777.263 I llama_init_from_model: flash_attn    = 0
0.00.777.264 I llama_init_from_model: freq_base     = 10000.0
0.00.777.265 I llama_init_from_model: freq_scale    = 1
0.00.777.265 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.777.271 I ggml_metal_init: allocating
0.00.777.334 I ggml_metal_init: found device: Apple M4
0.00.777.348 I ggml_metal_init: picking default device: Apple M4
0.00.779.097 I ggml_metal_init: using embedded metal library
0.00.785.619 I ggml_metal_init: GPU name:   Apple M4
0.00.785.622 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.785.623 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.785.625 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.785.625 I ggml_metal_init: simdgroup reduction   = true
0.00.785.625 I ggml_metal_init: simdgroup matrix mul. = true
0.00.785.626 I ggml_metal_init: has residency sets    = true
0.00.785.626 I ggml_metal_init: has bfloat            = true
0.00.785.626 I ggml_metal_init: use bfloat            = true
0.00.785.627 I ggml_metal_init: hasUnifiedMemory      = true
0.00.785.628 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.803.513 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.807.097 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.807.101 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.807.144 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.810.412 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.810.414 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.810.415 I llama_init_from_model: graph nodes  = 967
0.00.810.415 I llama_init_from_model: graph splits = 2
0.00.810.418 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.810.419 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.842.224 I 
0.00.842.297 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.842.306 I perplexity: tokenizing the input ..
0.00.849.540 I perplexity: tokenization took 7.232 ms
0.00.849.550 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.996.159 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.997.676 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.997.700 I llama_perf_context_print:        load time =     815.48 ms
0.00.997.701 I llama_perf_context_print: prompt eval time =     145.59 ms /   128 tokens (    1.14 ms per token,   879.19 tokens per second)
0.00.997.702 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.997.702 I llama_perf_context_print:       total time =     155.48 ms /   129 tokens
0.00.998.116 I ggml_metal_free: deallocating

real	0m1.016s
user	0m0.086s
sys	0m0.147s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.062 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.015.592 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.338 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.023.344 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.350 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.351 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.351 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.351 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.352 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.353 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.353 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.353 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.354 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.354 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.354 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.355 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.356 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.357 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.357 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.240 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.247 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.100 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.101 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.102 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.102 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.102 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.103 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.032.103 I llama_model_loader: - type  f32:  194 tensors
0.00.032.104 I llama_model_loader: - type q5_1:   97 tensors
0.00.032.104 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.105 I print_info: file format = GGUF V3 (latest)
0.00.032.105 I print_info: file type   = Q5_1
0.00.032.106 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.040.465 I load: special tokens cache size = 25
0.00.046.816 I load: token to piece cache size = 0.2984 MB
0.00.046.821 I print_info: arch             = gptneox
0.00.046.821 I print_info: vocab_only       = 0
0.00.046.821 I print_info: n_ctx_train      = 2048
0.00.046.821 I print_info: n_embd           = 2048
0.00.046.822 I print_info: n_layer          = 24
0.00.046.837 I print_info: n_head           = 16
0.00.046.841 I print_info: n_head_kv        = 16
0.00.046.841 I print_info: n_rot            = 32
0.00.046.841 I print_info: n_swa            = 0
0.00.046.841 I print_info: n_embd_head_k    = 128
0.00.046.842 I print_info: n_embd_head_v    = 128
0.00.046.842 I print_info: n_gqa            = 1
0.00.046.843 I print_info: n_embd_k_gqa     = 2048
0.00.046.843 I print_info: n_embd_v_gqa     = 2048
0.00.046.844 I print_info: f_norm_eps       = 1.0e-05
0.00.046.845 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.046.845 I print_info: f_clamp_kqv      = 0.0e+00
0.00.046.845 I print_info: f_max_alibi_bias = 0.0e+00
0.00.046.845 I print_info: f_logit_scale    = 0.0e+00
0.00.046.845 I print_info: n_ff             = 8192
0.00.046.846 I print_info: n_expert         = 0
0.00.046.846 I print_info: n_expert_used    = 0
0.00.046.846 I print_info: causal attn      = 1
0.00.046.846 I print_info: pooling type     = 0
0.00.046.846 I print_info: rope type        = 2
0.00.046.846 I print_info: rope scaling     = linear
0.00.046.847 I print_info: freq_base_train  = 10000.0
0.00.046.847 I print_info: freq_scale_train = 1
0.00.046.847 I print_info: n_ctx_orig_yarn  = 2048
0.00.046.847 I print_info: rope_finetuned   = unknown
0.00.046.848 I print_info: ssm_d_conv       = 0
0.00.046.848 I print_info: ssm_d_inner      = 0
0.00.046.848 I print_info: ssm_d_state      = 0
0.00.046.848 I print_info: ssm_dt_rank      = 0
0.00.046.848 I print_info: ssm_dt_b_c_rms   = 0
0.00.046.848 I print_info: model type       = 1.4B
0.00.046.849 I print_info: model params     = 1.41 B
0.00.046.849 I print_info: general.name     = 1.4B
0.00.046.849 I print_info: vocab type       = BPE
0.00.046.849 I print_info: n_vocab          = 50304
0.00.046.850 I print_info: n_merges         = 50009
0.00.046.850 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.850 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.850 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.850 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.851 I print_info: LF token         = 187 ''
0.00.046.853 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.853 I print_info: max token length = 1024
0.00.046.853 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.645.816 I load_tensors: offloading 24 repeating layers to GPU
0.00.645.826 I load_tensors: offloading output layer to GPU
0.00.645.826 I load_tensors: offloaded 25/25 layers to GPU
0.00.645.860 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.645.863 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.647.052 I llama_init_from_model: n_seq_max     = 1
0.00.647.055 I llama_init_from_model: n_ctx         = 2048
0.00.647.055 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.647.055 I llama_init_from_model: n_batch       = 2048
0.00.647.056 I llama_init_from_model: n_ubatch      = 512
0.00.647.056 I llama_init_from_model: flash_attn    = 0
0.00.647.057 I llama_init_from_model: freq_base     = 10000.0
0.00.647.058 I llama_init_from_model: freq_scale    = 1
0.00.647.060 I ggml_metal_init: allocating
0.00.647.125 I ggml_metal_init: found device: Apple M4
0.00.647.139 I ggml_metal_init: picking default device: Apple M4
0.00.648.711 I ggml_metal_init: using embedded metal library
0.00.654.735 I ggml_metal_init: GPU name:   Apple M4
0.00.654.739 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.654.740 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.654.741 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.654.741 I ggml_metal_init: simdgroup reduction   = true
0.00.654.741 I ggml_metal_init: simdgroup matrix mul. = true
0.00.654.742 I ggml_metal_init: has residency sets    = true
0.00.654.742 I ggml_metal_init: has bfloat            = true
0.00.654.742 I ggml_metal_init: use bfloat            = true
0.00.654.743 I ggml_metal_init: hasUnifiedMemory      = true
0.00.654.754 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.671.173 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.729.422 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.729.433 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.729.470 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.733.969 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.733.971 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.733.971 I llama_init_from_model: graph nodes  = 967
0.00.733.972 I llama_init_from_model: graph splits = 2
0.00.733.977 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.734.108 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.734.109 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.789.573 I main: llama threadpool init, n_threads = 4
0.00.789.620 I 
0.00.789.644 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.789.645 I 
0.00.789.817 I sampler seed: 1234
0.00.789.821 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.789.832 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.789.832 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.789.832 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.630.115 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55555.56 tokens per second)
0.01.630.116 I llama_perf_context_print:        load time =     773.28 ms
0.01.630.117 I llama_perf_context_print: prompt eval time =      42.27 ms /     7 tokens (    6.04 ms per token,   165.61 tokens per second)
0.01.630.118 I llama_perf_context_print:        eval time =     795.16 ms /    63 runs   (   12.62 ms per token,    79.23 tokens per second)
0.01.630.118 I llama_perf_context_print:       total time =     841.24 ms /    70 tokens
0.01.630.356 I ggml_metal_free: deallocating

real	0m1.649s
user	0m0.110s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.966 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.275 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.027.281 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.287 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.288 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.288 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.289 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.289 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.290 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.290 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.291 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.291 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.291 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.292 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.292 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.294 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.294 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.295 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.158 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.221 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.025 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.027 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.027 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.028 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.028 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.028 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.036.028 I llama_model_loader: - type  f32:  194 tensors
0.00.036.029 I llama_model_loader: - type q5_1:   97 tensors
0.00.036.029 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.029 I print_info: file format = GGUF V3 (latest)
0.00.036.030 I print_info: file type   = Q5_1
0.00.036.031 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.647 I load: special tokens cache size = 25
0.00.051.152 I load: token to piece cache size = 0.2984 MB
0.00.051.155 I print_info: arch             = gptneox
0.00.051.156 I print_info: vocab_only       = 0
0.00.051.156 I print_info: n_ctx_train      = 2048
0.00.051.156 I print_info: n_embd           = 2048
0.00.051.156 I print_info: n_layer          = 24
0.00.051.160 I print_info: n_head           = 16
0.00.051.160 I print_info: n_head_kv        = 16
0.00.051.161 I print_info: n_rot            = 32
0.00.051.161 I print_info: n_swa            = 0
0.00.051.163 I print_info: n_embd_head_k    = 128
0.00.051.163 I print_info: n_embd_head_v    = 128
0.00.051.164 I print_info: n_gqa            = 1
0.00.051.164 I print_info: n_embd_k_gqa     = 2048
0.00.051.165 I print_info: n_embd_v_gqa     = 2048
0.00.051.166 I print_info: f_norm_eps       = 1.0e-05
0.00.051.166 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.166 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.166 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.167 I print_info: f_logit_scale    = 0.0e+00
0.00.051.167 I print_info: n_ff             = 8192
0.00.051.167 I print_info: n_expert         = 0
0.00.051.167 I print_info: n_expert_used    = 0
0.00.051.168 I print_info: causal attn      = 1
0.00.051.168 I print_info: pooling type     = 0
0.00.051.168 I print_info: rope type        = 2
0.00.051.168 I print_info: rope scaling     = linear
0.00.051.169 I print_info: freq_base_train  = 10000.0
0.00.051.169 I print_info: freq_scale_train = 1
0.00.051.170 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.170 I print_info: rope_finetuned   = unknown
0.00.051.170 I print_info: ssm_d_conv       = 0
0.00.051.170 I print_info: ssm_d_inner      = 0
0.00.051.171 I print_info: ssm_d_state      = 0
0.00.051.172 I print_info: ssm_dt_rank      = 0
0.00.051.172 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.172 I print_info: model type       = 1.4B
0.00.051.172 I print_info: model params     = 1.41 B
0.00.051.172 I print_info: general.name     = 1.4B
0.00.051.173 I print_info: vocab type       = BPE
0.00.051.173 I print_info: n_vocab          = 50304
0.00.051.173 I print_info: n_merges         = 50009
0.00.051.174 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.174 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.174 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.174 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.174 I print_info: LF token         = 187 ''
0.00.051.175 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.176 I print_info: max token length = 1024
0.00.051.176 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.728.256 I load_tensors: offloading 24 repeating layers to GPU
0.00.728.261 I load_tensors: offloading output layer to GPU
0.00.728.263 I load_tensors: offloaded 25/25 layers to GPU
0.00.728.289 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.728.291 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.729.707 I llama_init_from_model: n_seq_max     = 1
0.00.729.710 I llama_init_from_model: n_ctx         = 128
0.00.729.710 I llama_init_from_model: n_ctx_per_seq = 128
0.00.729.711 I llama_init_from_model: n_batch       = 128
0.00.729.711 I llama_init_from_model: n_ubatch      = 128
0.00.729.712 I llama_init_from_model: flash_attn    = 0
0.00.729.713 I llama_init_from_model: freq_base     = 10000.0
0.00.729.714 I llama_init_from_model: freq_scale    = 1
0.00.729.715 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.729.716 I ggml_metal_init: allocating
0.00.729.731 I ggml_metal_init: found device: Apple M4
0.00.729.740 I ggml_metal_init: picking default device: Apple M4
0.00.731.078 I ggml_metal_init: using embedded metal library
0.00.737.087 I ggml_metal_init: GPU name:   Apple M4
0.00.737.090 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.737.091 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.737.092 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.737.092 I ggml_metal_init: simdgroup reduction   = true
0.00.737.092 I ggml_metal_init: simdgroup matrix mul. = true
0.00.737.093 I ggml_metal_init: has residency sets    = true
0.00.737.093 I ggml_metal_init: has bfloat            = true
0.00.737.093 I ggml_metal_init: use bfloat            = true
0.00.737.094 I ggml_metal_init: hasUnifiedMemory      = true
0.00.737.095 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.753.301 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.756.822 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.756.832 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.756.879 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.760.111 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.760.113 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.760.113 I llama_init_from_model: graph nodes  = 967
0.00.760.114 I llama_init_from_model: graph splits = 2
0.00.760.116 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.760.116 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.792.562 I 
0.00.792.662 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.792.669 I perplexity: tokenizing the input ..
0.00.799.717 I perplexity: tokenization took 7.045 ms
0.00.799.724 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.949.625 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.951.161 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.951.190 I llama_perf_context_print:        load time =     783.59 ms
0.00.951.190 I llama_perf_context_print: prompt eval time =     149.01 ms /   128 tokens (    1.16 ms per token,   859.00 tokens per second)
0.00.951.191 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.951.192 I llama_perf_context_print:       total time =     158.63 ms /   129 tokens
0.00.951.586 I ggml_metal_free: deallocating

real	0m0.966s
user	0m0.080s
sys	0m0.153s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.010.340 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.985 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.990 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.992 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.992 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.993 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.993 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.993 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.996 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.996 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.996 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.997 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.997 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.997 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.998 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.999 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.999 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.000 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.780 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.779 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.543 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.544 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.544 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.545 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.545 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.545 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.546 I llama_model_loader: - type  f32:  194 tensors
0.00.025.546 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.547 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.547 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.547 I print_info: file format = GGUF V3 (latest)
0.00.025.548 I print_info: file type   = Q2_K - Medium
0.00.025.549 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.761 I load: special tokens cache size = 25
0.00.039.745 I load: token to piece cache size = 0.2984 MB
0.00.039.748 I print_info: arch             = gptneox
0.00.039.748 I print_info: vocab_only       = 0
0.00.039.748 I print_info: n_ctx_train      = 2048
0.00.039.748 I print_info: n_embd           = 2048
0.00.039.749 I print_info: n_layer          = 24
0.00.039.751 I print_info: n_head           = 16
0.00.039.752 I print_info: n_head_kv        = 16
0.00.039.752 I print_info: n_rot            = 32
0.00.039.753 I print_info: n_swa            = 0
0.00.039.753 I print_info: n_embd_head_k    = 128
0.00.039.753 I print_info: n_embd_head_v    = 128
0.00.039.754 I print_info: n_gqa            = 1
0.00.039.754 I print_info: n_embd_k_gqa     = 2048
0.00.039.755 I print_info: n_embd_v_gqa     = 2048
0.00.039.756 I print_info: f_norm_eps       = 1.0e-05
0.00.039.757 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.759 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.759 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.759 I print_info: f_logit_scale    = 0.0e+00
0.00.039.760 I print_info: n_ff             = 8192
0.00.039.760 I print_info: n_expert         = 0
0.00.039.760 I print_info: n_expert_used    = 0
0.00.039.760 I print_info: causal attn      = 1
0.00.039.761 I print_info: pooling type     = 0
0.00.039.762 I print_info: rope type        = 2
0.00.039.762 I print_info: rope scaling     = linear
0.00.039.762 I print_info: freq_base_train  = 10000.0
0.00.039.763 I print_info: freq_scale_train = 1
0.00.039.763 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.763 I print_info: rope_finetuned   = unknown
0.00.039.763 I print_info: ssm_d_conv       = 0
0.00.039.763 I print_info: ssm_d_inner      = 0
0.00.039.764 I print_info: ssm_d_state      = 0
0.00.039.764 I print_info: ssm_dt_rank      = 0
0.00.039.765 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.765 I print_info: model type       = 1.4B
0.00.039.766 I print_info: model params     = 1.41 B
0.00.039.766 I print_info: general.name     = 1.4B
0.00.039.766 I print_info: vocab type       = BPE
0.00.039.767 I print_info: n_vocab          = 50304
0.00.039.767 I print_info: n_merges         = 50009
0.00.039.767 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.767 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.767 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.767 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.768 I print_info: LF token         = 187 ''
0.00.039.768 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.768 I print_info: max token length = 1024
0.00.039.769 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.341.395 I load_tensors: offloading 24 repeating layers to GPU
0.00.341.409 I load_tensors: offloading output layer to GPU
0.00.341.409 I load_tensors: offloaded 25/25 layers to GPU
0.00.341.444 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.341.446 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.342.897 I llama_init_from_model: n_seq_max     = 1
0.00.342.902 I llama_init_from_model: n_ctx         = 2048
0.00.342.903 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.342.903 I llama_init_from_model: n_batch       = 2048
0.00.342.903 I llama_init_from_model: n_ubatch      = 512
0.00.342.904 I llama_init_from_model: flash_attn    = 0
0.00.342.910 I llama_init_from_model: freq_base     = 10000.0
0.00.342.913 I llama_init_from_model: freq_scale    = 1
0.00.342.915 I ggml_metal_init: allocating
0.00.342.997 I ggml_metal_init: found device: Apple M4
0.00.343.011 I ggml_metal_init: picking default device: Apple M4
0.00.344.835 I ggml_metal_init: using embedded metal library
0.00.350.576 I ggml_metal_init: GPU name:   Apple M4
0.00.350.588 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.350.589 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.350.590 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.350.590 I ggml_metal_init: simdgroup reduction   = true
0.00.350.591 I ggml_metal_init: simdgroup matrix mul. = true
0.00.350.591 I ggml_metal_init: has residency sets    = true
0.00.350.591 I ggml_metal_init: has bfloat            = true
0.00.350.591 I ggml_metal_init: use bfloat            = true
0.00.350.593 I ggml_metal_init: hasUnifiedMemory      = true
0.00.350.597 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.372.560 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.432.364 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.432.372 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.432.408 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.436.839 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.436.841 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.436.841 I llama_init_from_model: graph nodes  = 967
0.00.436.842 I llama_init_from_model: graph splits = 2
0.00.436.853 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.436.969 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.436.970 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.496.216 I main: llama threadpool init, n_threads = 4
0.00.496.257 I 
0.00.496.280 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.496.280 I 
0.00.496.455 I sampler seed: 1234
0.00.496.460 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.496.471 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.496.471 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.496.471 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.176.831 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54699.54 tokens per second)
0.01.176.832 I llama_perf_context_print:        load time =     485.19 ms
0.01.176.832 I llama_perf_context_print: prompt eval time =      41.17 ms /     7 tokens (    5.88 ms per token,   170.02 tokens per second)
0.01.176.834 I llama_perf_context_print:        eval time =     636.38 ms /    63 runs   (   10.10 ms per token,    99.00 tokens per second)
0.01.176.834 I llama_perf_context_print:       total time =     681.30 ms /    70 tokens
0.01.177.050 I ggml_metal_free: deallocating

real	0m1.196s
user	0m0.112s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.156 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.678 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.020.683 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.684 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.685 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.685 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.686 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.686 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.687 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.687 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.688 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.688 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.688 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.689 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.689 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.691 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.691 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.691 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.028 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.208 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.094 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.096 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.096 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.096 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.097 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.097 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.030.097 I llama_model_loader: - type  f32:  194 tensors
0.00.030.098 I llama_model_loader: - type q2_K:   49 tensors
0.00.030.098 I llama_model_loader: - type q3_K:   48 tensors
0.00.030.098 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.099 I print_info: file format = GGUF V3 (latest)
0.00.030.099 I print_info: file type   = Q2_K - Medium
0.00.030.100 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.039.569 I load: special tokens cache size = 25
0.00.046.863 I load: token to piece cache size = 0.2984 MB
0.00.046.867 I print_info: arch             = gptneox
0.00.046.867 I print_info: vocab_only       = 0
0.00.046.867 I print_info: n_ctx_train      = 2048
0.00.046.867 I print_info: n_embd           = 2048
0.00.046.868 I print_info: n_layer          = 24
0.00.046.870 I print_info: n_head           = 16
0.00.046.871 I print_info: n_head_kv        = 16
0.00.046.872 I print_info: n_rot            = 32
0.00.046.872 I print_info: n_swa            = 0
0.00.046.872 I print_info: n_embd_head_k    = 128
0.00.046.872 I print_info: n_embd_head_v    = 128
0.00.046.873 I print_info: n_gqa            = 1
0.00.046.874 I print_info: n_embd_k_gqa     = 2048
0.00.046.874 I print_info: n_embd_v_gqa     = 2048
0.00.046.875 I print_info: f_norm_eps       = 1.0e-05
0.00.046.875 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.046.877 I print_info: f_clamp_kqv      = 0.0e+00
0.00.046.878 I print_info: f_max_alibi_bias = 0.0e+00
0.00.046.878 I print_info: f_logit_scale    = 0.0e+00
0.00.046.878 I print_info: n_ff             = 8192
0.00.046.879 I print_info: n_expert         = 0
0.00.046.879 I print_info: n_expert_used    = 0
0.00.046.879 I print_info: causal attn      = 1
0.00.046.879 I print_info: pooling type     = 0
0.00.046.879 I print_info: rope type        = 2
0.00.046.880 I print_info: rope scaling     = linear
0.00.046.881 I print_info: freq_base_train  = 10000.0
0.00.046.881 I print_info: freq_scale_train = 1
0.00.046.881 I print_info: n_ctx_orig_yarn  = 2048
0.00.046.881 I print_info: rope_finetuned   = unknown
0.00.046.881 I print_info: ssm_d_conv       = 0
0.00.046.882 I print_info: ssm_d_inner      = 0
0.00.046.882 I print_info: ssm_d_state      = 0
0.00.046.882 I print_info: ssm_dt_rank      = 0
0.00.046.882 I print_info: ssm_dt_b_c_rms   = 0
0.00.046.882 I print_info: model type       = 1.4B
0.00.046.883 I print_info: model params     = 1.41 B
0.00.046.883 I print_info: general.name     = 1.4B
0.00.046.884 I print_info: vocab type       = BPE
0.00.046.884 I print_info: n_vocab          = 50304
0.00.046.884 I print_info: n_merges         = 50009
0.00.046.884 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.884 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.885 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.885 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.885 I print_info: LF token         = 187 ''
0.00.046.885 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.886 I print_info: max token length = 1024
0.00.046.886 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.448.486 I load_tensors: offloading 24 repeating layers to GPU
0.00.448.497 I load_tensors: offloading output layer to GPU
0.00.448.497 I load_tensors: offloaded 25/25 layers to GPU
0.00.448.529 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.448.531 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.450.067 I llama_init_from_model: n_seq_max     = 1
0.00.450.072 I llama_init_from_model: n_ctx         = 128
0.00.450.073 I llama_init_from_model: n_ctx_per_seq = 128
0.00.450.074 I llama_init_from_model: n_batch       = 128
0.00.450.074 I llama_init_from_model: n_ubatch      = 128
0.00.450.074 I llama_init_from_model: flash_attn    = 0
0.00.450.076 I llama_init_from_model: freq_base     = 10000.0
0.00.450.077 I llama_init_from_model: freq_scale    = 1
0.00.450.077 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.450.084 I ggml_metal_init: allocating
0.00.450.125 I ggml_metal_init: found device: Apple M4
0.00.450.140 I ggml_metal_init: picking default device: Apple M4
0.00.451.802 I ggml_metal_init: using embedded metal library
0.00.457.927 I ggml_metal_init: GPU name:   Apple M4
0.00.457.942 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.457.943 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.457.944 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.457.944 I ggml_metal_init: simdgroup reduction   = true
0.00.457.944 I ggml_metal_init: simdgroup matrix mul. = true
0.00.457.945 I ggml_metal_init: has residency sets    = true
0.00.457.945 I ggml_metal_init: has bfloat            = true
0.00.457.945 I ggml_metal_init: use bfloat            = true
0.00.457.947 I ggml_metal_init: hasUnifiedMemory      = true
0.00.457.951 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.479.743 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.483.402 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.483.410 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.483.462 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.486.950 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.486.952 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.486.953 I llama_init_from_model: graph nodes  = 967
0.00.486.953 I llama_init_from_model: graph splits = 2
0.00.486.956 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.486.957 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.516.960 I 
0.00.517.037 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.517.046 I perplexity: tokenizing the input ..
0.00.523.809 I perplexity: tokenization took 6.761 ms
0.00.523.815 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.668.595 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.670.528 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.670.550 I llama_perf_context_print:        load time =     504.80 ms
0.00.670.551 I llama_perf_context_print: prompt eval time =     143.92 ms /   128 tokens (    1.12 ms per token,   889.39 tokens per second)
0.00.670.551 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.670.551 I llama_perf_context_print:       total time =     153.59 ms /   129 tokens
0.00.670.877 I ggml_metal_free: deallocating

real	0m0.698s
user	0m0.086s
sys	0m0.096s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.782 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.290 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.295 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.296 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.297 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.297 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.297 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.298 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.299 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.299 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.299 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.300 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.300 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.301 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.301 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.304 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.304 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.304 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.003 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.986 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.656 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.657 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.658 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.658 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.658 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.658 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.659 I llama_model_loader: - type  f32:  194 tensors
0.00.024.659 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.660 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.660 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.660 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.661 I print_info: file format = GGUF V3 (latest)
0.00.024.661 I print_info: file type   = Q3_K - Medium
0.00.024.662 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.442 I load: special tokens cache size = 25
0.00.038.331 I load: token to piece cache size = 0.2984 MB
0.00.038.334 I print_info: arch             = gptneox
0.00.038.334 I print_info: vocab_only       = 0
0.00.038.335 I print_info: n_ctx_train      = 2048
0.00.038.335 I print_info: n_embd           = 2048
0.00.038.335 I print_info: n_layer          = 24
0.00.038.338 I print_info: n_head           = 16
0.00.038.338 I print_info: n_head_kv        = 16
0.00.038.338 I print_info: n_rot            = 32
0.00.038.339 I print_info: n_swa            = 0
0.00.038.339 I print_info: n_embd_head_k    = 128
0.00.038.339 I print_info: n_embd_head_v    = 128
0.00.038.340 I print_info: n_gqa            = 1
0.00.038.340 I print_info: n_embd_k_gqa     = 2048
0.00.038.341 I print_info: n_embd_v_gqa     = 2048
0.00.038.342 I print_info: f_norm_eps       = 1.0e-05
0.00.038.342 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.342 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.343 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.343 I print_info: f_logit_scale    = 0.0e+00
0.00.038.346 I print_info: n_ff             = 8192
0.00.038.346 I print_info: n_expert         = 0
0.00.038.346 I print_info: n_expert_used    = 0
0.00.038.347 I print_info: causal attn      = 1
0.00.038.349 I print_info: pooling type     = 0
0.00.038.349 I print_info: rope type        = 2
0.00.038.349 I print_info: rope scaling     = linear
0.00.038.349 I print_info: freq_base_train  = 10000.0
0.00.038.350 I print_info: freq_scale_train = 1
0.00.038.350 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.350 I print_info: rope_finetuned   = unknown
0.00.038.350 I print_info: ssm_d_conv       = 0
0.00.038.351 I print_info: ssm_d_inner      = 0
0.00.038.351 I print_info: ssm_d_state      = 0
0.00.038.351 I print_info: ssm_dt_rank      = 0
0.00.038.351 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.351 I print_info: model type       = 1.4B
0.00.038.352 I print_info: model params     = 1.41 B
0.00.038.352 I print_info: general.name     = 1.4B
0.00.038.352 I print_info: vocab type       = BPE
0.00.038.353 I print_info: n_vocab          = 50304
0.00.038.353 I print_info: n_merges         = 50009
0.00.038.354 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.354 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.354 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.354 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.355 I print_info: LF token         = 187 ''
0.00.038.355 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.355 I print_info: max token length = 1024
0.00.038.356 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.433.009 I load_tensors: offloading 24 repeating layers to GPU
0.00.433.026 I load_tensors: offloading output layer to GPU
0.00.433.026 I load_tensors: offloaded 25/25 layers to GPU
0.00.433.059 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.433.061 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.434.570 I llama_init_from_model: n_seq_max     = 1
0.00.434.575 I llama_init_from_model: n_ctx         = 2048
0.00.434.575 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.434.576 I llama_init_from_model: n_batch       = 2048
0.00.434.576 I llama_init_from_model: n_ubatch      = 512
0.00.434.580 I llama_init_from_model: flash_attn    = 0
0.00.434.594 I llama_init_from_model: freq_base     = 10000.0
0.00.434.595 I llama_init_from_model: freq_scale    = 1
0.00.434.597 I ggml_metal_init: allocating
0.00.434.673 I ggml_metal_init: found device: Apple M4
0.00.434.687 I ggml_metal_init: picking default device: Apple M4
0.00.436.583 I ggml_metal_init: using embedded metal library
0.00.442.204 I ggml_metal_init: GPU name:   Apple M4
0.00.442.218 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.442.218 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.442.219 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.442.220 I ggml_metal_init: simdgroup reduction   = true
0.00.442.220 I ggml_metal_init: simdgroup matrix mul. = true
0.00.442.220 I ggml_metal_init: has residency sets    = true
0.00.442.221 I ggml_metal_init: has bfloat            = true
0.00.442.221 I ggml_metal_init: use bfloat            = true
0.00.442.222 I ggml_metal_init: hasUnifiedMemory      = true
0.00.442.227 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.462.291 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.520.400 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.520.406 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.520.442 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.524.692 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.524.694 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.524.694 I llama_init_from_model: graph nodes  = 967
0.00.524.695 I llama_init_from_model: graph splits = 2
0.00.524.700 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.524.830 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.524.831 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.584.142 I main: llama threadpool init, n_threads = 4
0.00.584.189 I 
0.00.584.209 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.584.210 I 
0.00.584.378 I sampler seed: 1234
0.00.584.383 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.584.393 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.584.394 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.584.394 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.336.785 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53584.91 tokens per second)
0.01.336.786 I llama_perf_context_print:        load time =     574.62 ms
0.01.336.787 I llama_perf_context_print: prompt eval time =      49.96 ms /     7 tokens (    7.14 ms per token,   140.11 tokens per second)
0.01.336.787 I llama_perf_context_print:        eval time =     699.46 ms /    63 runs   (   11.10 ms per token,    90.07 tokens per second)
0.01.336.788 I llama_perf_context_print:       total time =     753.38 ms /    70 tokens
0.01.337.011 I ggml_metal_free: deallocating

real	0m1.354s
user	0m0.110s
sys	0m0.178s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.889 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.153 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.022.158 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.164 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.164 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.165 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.165 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.165 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.168 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.169 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.169 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.169 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.170 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.170 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.170 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.173 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.173 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.173 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.919 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.943 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.812 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.813 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.813 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.814 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.814 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.814 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.030.815 I llama_model_loader: - type  f32:  194 tensors
0.00.030.815 I llama_model_loader: - type q3_K:   25 tensors
0.00.030.815 I llama_model_loader: - type q4_K:   71 tensors
0.00.030.816 I llama_model_loader: - type q5_K:    1 tensors
0.00.030.816 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.817 I print_info: file format = GGUF V3 (latest)
0.00.030.817 I print_info: file type   = Q3_K - Medium
0.00.030.818 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.039.095 I load: special tokens cache size = 25
0.00.045.184 I load: token to piece cache size = 0.2984 MB
0.00.045.186 I print_info: arch             = gptneox
0.00.045.187 I print_info: vocab_only       = 0
0.00.045.187 I print_info: n_ctx_train      = 2048
0.00.045.187 I print_info: n_embd           = 2048
0.00.045.187 I print_info: n_layer          = 24
0.00.045.190 I print_info: n_head           = 16
0.00.045.190 I print_info: n_head_kv        = 16
0.00.045.191 I print_info: n_rot            = 32
0.00.045.191 I print_info: n_swa            = 0
0.00.045.191 I print_info: n_embd_head_k    = 128
0.00.045.191 I print_info: n_embd_head_v    = 128
0.00.045.192 I print_info: n_gqa            = 1
0.00.045.193 I print_info: n_embd_k_gqa     = 2048
0.00.045.193 I print_info: n_embd_v_gqa     = 2048
0.00.045.194 I print_info: f_norm_eps       = 1.0e-05
0.00.045.195 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.195 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.195 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.195 I print_info: f_logit_scale    = 0.0e+00
0.00.045.196 I print_info: n_ff             = 8192
0.00.045.196 I print_info: n_expert         = 0
0.00.045.196 I print_info: n_expert_used    = 0
0.00.045.196 I print_info: causal attn      = 1
0.00.045.196 I print_info: pooling type     = 0
0.00.045.197 I print_info: rope type        = 2
0.00.045.197 I print_info: rope scaling     = linear
0.00.045.197 I print_info: freq_base_train  = 10000.0
0.00.045.198 I print_info: freq_scale_train = 1
0.00.045.198 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.198 I print_info: rope_finetuned   = unknown
0.00.045.198 I print_info: ssm_d_conv       = 0
0.00.045.200 I print_info: ssm_d_inner      = 0
0.00.045.200 I print_info: ssm_d_state      = 0
0.00.045.201 I print_info: ssm_dt_rank      = 0
0.00.045.201 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.201 I print_info: model type       = 1.4B
0.00.045.201 I print_info: model params     = 1.41 B
0.00.045.202 I print_info: general.name     = 1.4B
0.00.045.202 I print_info: vocab type       = BPE
0.00.045.202 I print_info: n_vocab          = 50304
0.00.045.202 I print_info: n_merges         = 50009
0.00.045.203 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.203 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.203 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.207 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.207 I print_info: LF token         = 187 ''
0.00.045.208 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.208 I print_info: max token length = 1024
0.00.045.208 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.470.918 I load_tensors: offloading 24 repeating layers to GPU
0.00.470.925 I load_tensors: offloading output layer to GPU
0.00.470.926 I load_tensors: offloaded 25/25 layers to GPU
0.00.470.959 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.470.961 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.472.449 I llama_init_from_model: n_seq_max     = 1
0.00.472.452 I llama_init_from_model: n_ctx         = 128
0.00.472.453 I llama_init_from_model: n_ctx_per_seq = 128
0.00.472.453 I llama_init_from_model: n_batch       = 128
0.00.472.453 I llama_init_from_model: n_ubatch      = 128
0.00.472.454 I llama_init_from_model: flash_attn    = 0
0.00.472.456 I llama_init_from_model: freq_base     = 10000.0
0.00.472.456 I llama_init_from_model: freq_scale    = 1
0.00.472.457 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.472.459 I ggml_metal_init: allocating
0.00.472.510 I ggml_metal_init: found device: Apple M4
0.00.472.522 I ggml_metal_init: picking default device: Apple M4
0.00.474.221 I ggml_metal_init: using embedded metal library
0.00.479.722 I ggml_metal_init: GPU name:   Apple M4
0.00.479.731 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.479.731 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.479.732 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.479.733 I ggml_metal_init: simdgroup reduction   = true
0.00.479.733 I ggml_metal_init: simdgroup matrix mul. = true
0.00.479.734 I ggml_metal_init: has residency sets    = true
0.00.479.734 I ggml_metal_init: has bfloat            = true
0.00.479.734 I ggml_metal_init: use bfloat            = true
0.00.479.736 I ggml_metal_init: hasUnifiedMemory      = true
0.00.479.741 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.500.336 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.504.095 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.504.106 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.504.172 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.507.527 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.507.528 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.507.529 I llama_init_from_model: graph nodes  = 967
0.00.507.529 I llama_init_from_model: graph splits = 2
0.00.507.533 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.507.533 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.534.329 I 
0.00.534.408 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.534.416 I perplexity: tokenizing the input ..
0.00.541.130 I perplexity: tokenization took 6.712 ms
0.00.541.136 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.673.672 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.675.171 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.675.193 I llama_perf_context_print:        load time =     525.43 ms
0.00.675.194 I llama_perf_context_print: prompt eval time =     131.99 ms /   128 tokens (    1.03 ms per token,   969.80 tokens per second)
0.00.675.194 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.675.195 I llama_perf_context_print:       total time =     140.87 ms /   129 tokens
0.00.675.576 I ggml_metal_free: deallocating

real	0m0.690s
user	0m0.082s
sys	0m0.120s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.060 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.010.101 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.938 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.942 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.947 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.948 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.948 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.949 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.949 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.950 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.951 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.951 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.951 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.952 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.952 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.953 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.954 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.954 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.955 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.865 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.903 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.678 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.679 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.679 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.680 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.680 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.680 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.681 I llama_model_loader: - type  f32:  194 tensors
0.00.026.681 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.681 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.681 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.682 I print_info: file format = GGUF V3 (latest)
0.00.026.683 I print_info: file type   = Q4_K - Medium
0.00.026.684 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.911 I load: special tokens cache size = 25
0.00.040.865 I load: token to piece cache size = 0.2984 MB
0.00.040.868 I print_info: arch             = gptneox
0.00.040.868 I print_info: vocab_only       = 0
0.00.040.868 I print_info: n_ctx_train      = 2048
0.00.040.868 I print_info: n_embd           = 2048
0.00.040.868 I print_info: n_layer          = 24
0.00.040.871 I print_info: n_head           = 16
0.00.040.872 I print_info: n_head_kv        = 16
0.00.040.872 I print_info: n_rot            = 32
0.00.040.872 I print_info: n_swa            = 0
0.00.040.874 I print_info: n_embd_head_k    = 128
0.00.040.874 I print_info: n_embd_head_v    = 128
0.00.040.875 I print_info: n_gqa            = 1
0.00.040.875 I print_info: n_embd_k_gqa     = 2048
0.00.040.876 I print_info: n_embd_v_gqa     = 2048
0.00.040.877 I print_info: f_norm_eps       = 1.0e-05
0.00.040.877 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.877 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.877 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.878 I print_info: f_logit_scale    = 0.0e+00
0.00.040.878 I print_info: n_ff             = 8192
0.00.040.878 I print_info: n_expert         = 0
0.00.040.878 I print_info: n_expert_used    = 0
0.00.040.879 I print_info: causal attn      = 1
0.00.040.881 I print_info: pooling type     = 0
0.00.040.881 I print_info: rope type        = 2
0.00.040.881 I print_info: rope scaling     = linear
0.00.040.881 I print_info: freq_base_train  = 10000.0
0.00.040.882 I print_info: freq_scale_train = 1
0.00.040.882 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.882 I print_info: rope_finetuned   = unknown
0.00.040.882 I print_info: ssm_d_conv       = 0
0.00.040.883 I print_info: ssm_d_inner      = 0
0.00.040.883 I print_info: ssm_d_state      = 0
0.00.040.883 I print_info: ssm_dt_rank      = 0
0.00.040.883 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.883 I print_info: model type       = 1.4B
0.00.040.884 I print_info: model params     = 1.41 B
0.00.040.884 I print_info: general.name     = 1.4B
0.00.040.885 I print_info: vocab type       = BPE
0.00.040.885 I print_info: n_vocab          = 50304
0.00.040.885 I print_info: n_merges         = 50009
0.00.040.887 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.887 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.887 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.887 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.887 I print_info: LF token         = 187 ''
0.00.040.888 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.889 I print_info: max token length = 1024
0.00.040.889 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.519.628 I load_tensors: offloading 24 repeating layers to GPU
0.00.519.644 I load_tensors: offloading output layer to GPU
0.00.519.645 I load_tensors: offloaded 25/25 layers to GPU
0.00.519.676 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.519.678 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.520.963 I llama_init_from_model: n_seq_max     = 1
0.00.520.971 I llama_init_from_model: n_ctx         = 2048
0.00.520.972 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.520.973 I llama_init_from_model: n_batch       = 2048
0.00.520.973 I llama_init_from_model: n_ubatch      = 512
0.00.520.973 I llama_init_from_model: flash_attn    = 0
0.00.520.975 I llama_init_from_model: freq_base     = 10000.0
0.00.520.976 I llama_init_from_model: freq_scale    = 1
0.00.520.979 I ggml_metal_init: allocating
0.00.521.060 I ggml_metal_init: found device: Apple M4
0.00.521.073 I ggml_metal_init: picking default device: Apple M4
0.00.522.903 I ggml_metal_init: using embedded metal library
0.00.529.083 I ggml_metal_init: GPU name:   Apple M4
0.00.529.088 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.529.088 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.529.089 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.529.090 I ggml_metal_init: simdgroup reduction   = true
0.00.529.090 I ggml_metal_init: simdgroup matrix mul. = true
0.00.529.091 I ggml_metal_init: has residency sets    = true
0.00.529.091 I ggml_metal_init: has bfloat            = true
0.00.529.091 I ggml_metal_init: use bfloat            = true
0.00.529.092 I ggml_metal_init: hasUnifiedMemory      = true
0.00.529.094 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.548.015 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.608.466 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.608.473 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.608.517 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.612.624 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.612.626 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.612.626 I llama_init_from_model: graph nodes  = 967
0.00.612.627 I llama_init_from_model: graph splits = 2
0.00.612.633 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.612.763 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.612.764 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.866 I main: llama threadpool init, n_threads = 4
0.00.668.908 I 
0.00.668.931 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.931 I 
0.00.669.081 I sampler seed: 1234
0.00.669.085 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.669.096 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.669.096 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.669.096 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.422.848 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50035.24 tokens per second)
0.01.422.849 I llama_perf_context_print:        load time =     658.08 ms
0.01.422.850 I llama_perf_context_print: prompt eval time =      46.74 ms /     7 tokens (    6.68 ms per token,   149.77 tokens per second)
0.01.422.851 I llama_perf_context_print:        eval time =     704.09 ms /    63 runs   (   11.18 ms per token,    89.48 tokens per second)
0.01.422.851 I llama_perf_context_print:       total time =     754.67 ms /    70 tokens
0.01.423.080 I ggml_metal_free: deallocating

real	0m1.444s
user	0m0.110s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.528 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.493 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.031.501 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.503 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.503 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.504 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.504 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.504 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.505 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.506 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.506 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.506 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.507 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.507 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.510 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.512 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.512 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.513 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.280 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.334 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.235 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.236 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.237 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.237 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.237 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.238 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.040.238 I llama_model_loader: - type  f32:  194 tensors
0.00.040.239 I llama_model_loader: - type q4_K:   61 tensors
0.00.040.239 I llama_model_loader: - type q5_K:   24 tensors
0.00.040.239 I llama_model_loader: - type q6_K:   13 tensors
0.00.040.240 I print_info: file format = GGUF V3 (latest)
0.00.040.240 I print_info: file type   = Q4_K - Medium
0.00.040.241 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.049.315 I load: special tokens cache size = 25
0.00.056.175 I load: token to piece cache size = 0.2984 MB
0.00.056.178 I print_info: arch             = gptneox
0.00.056.178 I print_info: vocab_only       = 0
0.00.056.178 I print_info: n_ctx_train      = 2048
0.00.056.178 I print_info: n_embd           = 2048
0.00.056.179 I print_info: n_layer          = 24
0.00.056.182 I print_info: n_head           = 16
0.00.056.183 I print_info: n_head_kv        = 16
0.00.056.183 I print_info: n_rot            = 32
0.00.056.183 I print_info: n_swa            = 0
0.00.056.186 I print_info: n_embd_head_k    = 128
0.00.056.186 I print_info: n_embd_head_v    = 128
0.00.056.186 I print_info: n_gqa            = 1
0.00.056.187 I print_info: n_embd_k_gqa     = 2048
0.00.056.188 I print_info: n_embd_v_gqa     = 2048
0.00.056.188 I print_info: f_norm_eps       = 1.0e-05
0.00.056.189 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.189 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.189 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.189 I print_info: f_logit_scale    = 0.0e+00
0.00.056.190 I print_info: n_ff             = 8192
0.00.056.190 I print_info: n_expert         = 0
0.00.056.190 I print_info: n_expert_used    = 0
0.00.056.190 I print_info: causal attn      = 1
0.00.056.190 I print_info: pooling type     = 0
0.00.056.191 I print_info: rope type        = 2
0.00.056.191 I print_info: rope scaling     = linear
0.00.056.192 I print_info: freq_base_train  = 10000.0
0.00.056.192 I print_info: freq_scale_train = 1
0.00.056.193 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.193 I print_info: rope_finetuned   = unknown
0.00.056.193 I print_info: ssm_d_conv       = 0
0.00.056.193 I print_info: ssm_d_inner      = 0
0.00.056.193 I print_info: ssm_d_state      = 0
0.00.056.195 I print_info: ssm_dt_rank      = 0
0.00.056.195 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.195 I print_info: model type       = 1.4B
0.00.056.196 I print_info: model params     = 1.41 B
0.00.056.196 I print_info: general.name     = 1.4B
0.00.056.196 I print_info: vocab type       = BPE
0.00.056.196 I print_info: n_vocab          = 50304
0.00.056.197 I print_info: n_merges         = 50009
0.00.056.197 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.197 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.197 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.197 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.198 I print_info: LF token         = 187 ''
0.00.056.198 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.198 I print_info: max token length = 1024
0.00.056.200 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.564.102 I load_tensors: offloading 24 repeating layers to GPU
0.00.564.116 I load_tensors: offloading output layer to GPU
0.00.564.117 I load_tensors: offloaded 25/25 layers to GPU
0.00.564.146 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.564.147 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.565.522 I llama_init_from_model: n_seq_max     = 1
0.00.565.531 I llama_init_from_model: n_ctx         = 128
0.00.565.532 I llama_init_from_model: n_ctx_per_seq = 128
0.00.565.532 I llama_init_from_model: n_batch       = 128
0.00.565.532 I llama_init_from_model: n_ubatch      = 128
0.00.565.533 I llama_init_from_model: flash_attn    = 0
0.00.565.534 I llama_init_from_model: freq_base     = 10000.0
0.00.565.535 I llama_init_from_model: freq_scale    = 1
0.00.565.535 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.565.537 I ggml_metal_init: allocating
0.00.565.607 I ggml_metal_init: found device: Apple M4
0.00.565.621 I ggml_metal_init: picking default device: Apple M4
0.00.567.278 I ggml_metal_init: using embedded metal library
0.00.572.917 I ggml_metal_init: GPU name:   Apple M4
0.00.572.922 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.572.923 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.572.924 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.572.925 I ggml_metal_init: simdgroup reduction   = true
0.00.572.925 I ggml_metal_init: simdgroup matrix mul. = true
0.00.572.925 I ggml_metal_init: has residency sets    = true
0.00.572.925 I ggml_metal_init: has bfloat            = true
0.00.572.926 I ggml_metal_init: use bfloat            = true
0.00.572.927 I ggml_metal_init: hasUnifiedMemory      = true
0.00.572.928 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.591.424 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.594.960 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.594.966 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.595.022 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.598.061 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.598.063 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.598.063 I llama_init_from_model: graph nodes  = 967
0.00.598.063 I llama_init_from_model: graph splits = 2
0.00.598.066 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.598.067 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.624.447 I 
0.00.624.521 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.624.529 I perplexity: tokenizing the input ..
0.00.632.191 I perplexity: tokenization took 7.658 ms
0.00.632.206 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.767.690 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.769.211 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.769.232 I llama_perf_context_print:        load time =     610.91 ms
0.00.769.233 I llama_perf_context_print: prompt eval time =     134.62 ms /   128 tokens (    1.05 ms per token,   950.82 tokens per second)
0.00.769.234 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.769.234 I llama_perf_context_print:       total time =     144.79 ms /   129 tokens
0.00.769.601 I ggml_metal_free: deallocating

real	0m0.785s
user	0m0.083s
sys	0m0.123s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.008.817 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.853 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.858 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.860 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.865 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.865 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.867 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.867 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.868 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.869 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.869 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.869 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.870 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.870 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.871 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.873 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.874 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.874 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.720 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.706 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.560 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.562 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.562 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.562 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.563 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.563 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.563 I llama_model_loader: - type  f32:  194 tensors
0.00.025.564 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.564 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.564 I print_info: file format = GGUF V3 (latest)
0.00.025.565 I print_info: file type   = Q5_K - Medium
0.00.025.566 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.432 I load: special tokens cache size = 25
0.00.039.446 I load: token to piece cache size = 0.2984 MB
0.00.039.449 I print_info: arch             = gptneox
0.00.039.449 I print_info: vocab_only       = 0
0.00.039.449 I print_info: n_ctx_train      = 2048
0.00.039.449 I print_info: n_embd           = 2048
0.00.039.450 I print_info: n_layer          = 24
0.00.039.452 I print_info: n_head           = 16
0.00.039.453 I print_info: n_head_kv        = 16
0.00.039.453 I print_info: n_rot            = 32
0.00.039.453 I print_info: n_swa            = 0
0.00.039.453 I print_info: n_embd_head_k    = 128
0.00.039.454 I print_info: n_embd_head_v    = 128
0.00.039.454 I print_info: n_gqa            = 1
0.00.039.455 I print_info: n_embd_k_gqa     = 2048
0.00.039.458 I print_info: n_embd_v_gqa     = 2048
0.00.039.458 I print_info: f_norm_eps       = 1.0e-05
0.00.039.459 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.459 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.459 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.459 I print_info: f_logit_scale    = 0.0e+00
0.00.039.460 I print_info: n_ff             = 8192
0.00.039.460 I print_info: n_expert         = 0
0.00.039.462 I print_info: n_expert_used    = 0
0.00.039.462 I print_info: causal attn      = 1
0.00.039.462 I print_info: pooling type     = 0
0.00.039.464 I print_info: rope type        = 2
0.00.039.465 I print_info: rope scaling     = linear
0.00.039.466 I print_info: freq_base_train  = 10000.0
0.00.039.466 I print_info: freq_scale_train = 1
0.00.039.466 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.466 I print_info: rope_finetuned   = unknown
0.00.039.466 I print_info: ssm_d_conv       = 0
0.00.039.467 I print_info: ssm_d_inner      = 0
0.00.039.467 I print_info: ssm_d_state      = 0
0.00.039.467 I print_info: ssm_dt_rank      = 0
0.00.039.467 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.467 I print_info: model type       = 1.4B
0.00.039.468 I print_info: model params     = 1.41 B
0.00.039.468 I print_info: general.name     = 1.4B
0.00.039.468 I print_info: vocab type       = BPE
0.00.039.468 I print_info: n_vocab          = 50304
0.00.039.469 I print_info: n_merges         = 50009
0.00.039.469 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.469 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.469 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.469 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.470 I print_info: LF token         = 187 ''
0.00.039.470 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.470 I print_info: max token length = 1024
0.00.039.470 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.605.252 I load_tensors: offloading 24 repeating layers to GPU
0.00.605.256 I load_tensors: offloading output layer to GPU
0.00.605.258 I load_tensors: offloaded 25/25 layers to GPU
0.00.605.281 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.605.282 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.606.659 I llama_init_from_model: n_seq_max     = 1
0.00.606.661 I llama_init_from_model: n_ctx         = 2048
0.00.606.662 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.606.662 I llama_init_from_model: n_batch       = 2048
0.00.606.662 I llama_init_from_model: n_ubatch      = 512
0.00.606.663 I llama_init_from_model: flash_attn    = 0
0.00.606.664 I llama_init_from_model: freq_base     = 10000.0
0.00.606.667 I llama_init_from_model: freq_scale    = 1
0.00.606.669 I ggml_metal_init: allocating
0.00.606.679 I ggml_metal_init: found device: Apple M4
0.00.606.687 I ggml_metal_init: picking default device: Apple M4
0.00.608.137 I ggml_metal_init: using embedded metal library
0.00.614.128 I ggml_metal_init: GPU name:   Apple M4
0.00.614.131 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.614.132 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.614.133 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.614.133 I ggml_metal_init: simdgroup reduction   = true
0.00.614.134 I ggml_metal_init: simdgroup matrix mul. = true
0.00.614.134 I ggml_metal_init: has residency sets    = true
0.00.614.134 I ggml_metal_init: has bfloat            = true
0.00.614.134 I ggml_metal_init: use bfloat            = true
0.00.614.135 I ggml_metal_init: hasUnifiedMemory      = true
0.00.614.138 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.631.039 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.686.842 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.686.852 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.686.889 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.691.038 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.691.041 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.691.041 I llama_init_from_model: graph nodes  = 967
0.00.691.041 I llama_init_from_model: graph splits = 2
0.00.691.048 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.691.177 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.691.177 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.969 I main: llama threadpool init, n_threads = 4
0.00.751.016 I 
0.00.751.038 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.040 I 
0.00.751.216 I sampler seed: 1234
0.00.751.221 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.232 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.232 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.232 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.592.830 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55167.06 tokens per second)
0.01.592.831 I llama_perf_context_print:        load time =     741.45 ms
0.01.592.832 I llama_perf_context_print: prompt eval time =      51.58 ms /     7 tokens (    7.37 ms per token,   135.71 tokens per second)
0.01.592.832 I llama_perf_context_print:        eval time =     787.13 ms /    63 runs   (   12.49 ms per token,    80.04 tokens per second)
0.01.592.833 I llama_perf_context_print:       total time =     842.56 ms /    70 tokens
0.01.593.056 I ggml_metal_free: deallocating

real	0m1.611s
user	0m0.107s
sys	0m0.220s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.920 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.187 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.019.193 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.195 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.197 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.197 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.198 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.198 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.199 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.200 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.200 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.200 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.201 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.201 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.202 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.203 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.204 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.204 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.053 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.068 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.941 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.943 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.943 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.944 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.944 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.945 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.945 I llama_model_loader: - type  f32:  194 tensors
0.00.027.946 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.946 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.946 I print_info: file format = GGUF V3 (latest)
0.00.027.947 I print_info: file type   = Q5_K - Medium
0.00.027.949 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.036.363 I load: special tokens cache size = 25
0.00.042.516 I load: token to piece cache size = 0.2984 MB
0.00.042.520 I print_info: arch             = gptneox
0.00.042.521 I print_info: vocab_only       = 0
0.00.042.521 I print_info: n_ctx_train      = 2048
0.00.042.521 I print_info: n_embd           = 2048
0.00.042.521 I print_info: n_layer          = 24
0.00.042.526 I print_info: n_head           = 16
0.00.042.526 I print_info: n_head_kv        = 16
0.00.042.527 I print_info: n_rot            = 32
0.00.042.527 I print_info: n_swa            = 0
0.00.042.527 I print_info: n_embd_head_k    = 128
0.00.042.527 I print_info: n_embd_head_v    = 128
0.00.042.528 I print_info: n_gqa            = 1
0.00.042.529 I print_info: n_embd_k_gqa     = 2048
0.00.042.529 I print_info: n_embd_v_gqa     = 2048
0.00.042.530 I print_info: f_norm_eps       = 1.0e-05
0.00.042.530 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.530 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.533 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.533 I print_info: f_logit_scale    = 0.0e+00
0.00.042.534 I print_info: n_ff             = 8192
0.00.042.534 I print_info: n_expert         = 0
0.00.042.534 I print_info: n_expert_used    = 0
0.00.042.534 I print_info: causal attn      = 1
0.00.042.535 I print_info: pooling type     = 0
0.00.042.535 I print_info: rope type        = 2
0.00.042.535 I print_info: rope scaling     = linear
0.00.042.535 I print_info: freq_base_train  = 10000.0
0.00.042.535 I print_info: freq_scale_train = 1
0.00.042.536 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.536 I print_info: rope_finetuned   = unknown
0.00.042.536 I print_info: ssm_d_conv       = 0
0.00.042.536 I print_info: ssm_d_inner      = 0
0.00.042.536 I print_info: ssm_d_state      = 0
0.00.042.536 I print_info: ssm_dt_rank      = 0
0.00.042.537 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.538 I print_info: model type       = 1.4B
0.00.042.538 I print_info: model params     = 1.41 B
0.00.042.538 I print_info: general.name     = 1.4B
0.00.042.538 I print_info: vocab type       = BPE
0.00.042.539 I print_info: n_vocab          = 50304
0.00.042.539 I print_info: n_merges         = 50009
0.00.042.539 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.539 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.539 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.539 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.540 I print_info: LF token         = 187 ''
0.00.042.540 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.540 I print_info: max token length = 1024
0.00.042.541 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.780.282 I load_tensors: offloading 24 repeating layers to GPU
0.00.780.292 I load_tensors: offloading output layer to GPU
0.00.780.293 I load_tensors: offloaded 25/25 layers to GPU
0.00.780.316 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.780.318 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.781.460 I llama_init_from_model: n_seq_max     = 1
0.00.781.465 I llama_init_from_model: n_ctx         = 128
0.00.781.466 I llama_init_from_model: n_ctx_per_seq = 128
0.00.781.466 I llama_init_from_model: n_batch       = 128
0.00.781.466 I llama_init_from_model: n_ubatch      = 128
0.00.781.467 I llama_init_from_model: flash_attn    = 0
0.00.781.468 I llama_init_from_model: freq_base     = 10000.0
0.00.781.469 I llama_init_from_model: freq_scale    = 1
0.00.781.469 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.781.471 I ggml_metal_init: allocating
0.00.781.521 I ggml_metal_init: found device: Apple M4
0.00.781.533 I ggml_metal_init: picking default device: Apple M4
0.00.783.062 I ggml_metal_init: using embedded metal library
0.00.789.959 I ggml_metal_init: GPU name:   Apple M4
0.00.789.964 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.789.965 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.789.966 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.789.967 I ggml_metal_init: simdgroup reduction   = true
0.00.789.967 I ggml_metal_init: simdgroup matrix mul. = true
0.00.789.967 I ggml_metal_init: has residency sets    = true
0.00.789.967 I ggml_metal_init: has bfloat            = true
0.00.789.968 I ggml_metal_init: use bfloat            = true
0.00.789.968 I ggml_metal_init: hasUnifiedMemory      = true
0.00.789.970 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.807.562 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.811.500 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.811.504 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.811.556 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.814.905 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.814.907 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.814.907 I llama_init_from_model: graph nodes  = 967
0.00.814.907 I llama_init_from_model: graph splits = 2
0.00.814.911 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.814.911 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.853.794 I 
0.00.853.879 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.853.886 I perplexity: tokenizing the input ..
0.00.860.132 I perplexity: tokenization took 6.244 ms
0.00.860.140 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.011.795 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.01.013.583 I Final estimate: PPL = 10.2433 +/- 3.24778

0.01.013.606 I llama_perf_context_print:        load time =     844.87 ms
0.01.013.607 I llama_perf_context_print: prompt eval time =     151.40 ms /   128 tokens (    1.18 ms per token,   845.44 tokens per second)
0.01.013.607 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.013.609 I llama_perf_context_print:       total time =     159.82 ms /   129 tokens
0.01.013.976 I ggml_metal_free: deallocating

real	0m1.028s
user	0m0.079s
sys	0m0.146s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.342 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.336 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.347 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.349 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.350 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.351 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.351 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.351 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.352 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.353 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.353 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.353 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.354 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.355 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.356 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.357 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.358 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.358 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.218 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.217 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.007 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.008 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.009 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.009 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.009 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.010 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.010 I llama_model_loader: - type  f32:  194 tensors
0.00.027.011 I llama_model_loader: - type q6_K:   98 tensors
0.00.027.011 I print_info: file format = GGUF V3 (latest)
0.00.027.011 I print_info: file type   = Q6_K
0.00.027.012 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.035.278 I load: special tokens cache size = 25
0.00.041.405 I load: token to piece cache size = 0.2984 MB
0.00.041.408 I print_info: arch             = gptneox
0.00.041.408 I print_info: vocab_only       = 0
0.00.041.408 I print_info: n_ctx_train      = 2048
0.00.041.409 I print_info: n_embd           = 2048
0.00.041.409 I print_info: n_layer          = 24
0.00.041.412 I print_info: n_head           = 16
0.00.041.413 I print_info: n_head_kv        = 16
0.00.041.413 I print_info: n_rot            = 32
0.00.041.413 I print_info: n_swa            = 0
0.00.041.413 I print_info: n_embd_head_k    = 128
0.00.041.414 I print_info: n_embd_head_v    = 128
0.00.041.414 I print_info: n_gqa            = 1
0.00.041.415 I print_info: n_embd_k_gqa     = 2048
0.00.041.416 I print_info: n_embd_v_gqa     = 2048
0.00.041.417 I print_info: f_norm_eps       = 1.0e-05
0.00.041.417 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.417 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.417 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.417 I print_info: f_logit_scale    = 0.0e+00
0.00.041.418 I print_info: n_ff             = 8192
0.00.041.418 I print_info: n_expert         = 0
0.00.041.419 I print_info: n_expert_used    = 0
0.00.041.419 I print_info: causal attn      = 1
0.00.041.419 I print_info: pooling type     = 0
0.00.041.419 I print_info: rope type        = 2
0.00.041.419 I print_info: rope scaling     = linear
0.00.041.420 I print_info: freq_base_train  = 10000.0
0.00.041.420 I print_info: freq_scale_train = 1
0.00.041.420 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.421 I print_info: rope_finetuned   = unknown
0.00.041.421 I print_info: ssm_d_conv       = 0
0.00.041.421 I print_info: ssm_d_inner      = 0
0.00.041.421 I print_info: ssm_d_state      = 0
0.00.041.421 I print_info: ssm_dt_rank      = 0
0.00.041.421 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.422 I print_info: model type       = 1.4B
0.00.041.422 I print_info: model params     = 1.41 B
0.00.041.422 I print_info: general.name     = 1.4B
0.00.041.423 I print_info: vocab type       = BPE
0.00.041.423 I print_info: n_vocab          = 50304
0.00.041.423 I print_info: n_merges         = 50009
0.00.041.423 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.424 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.424 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.424 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.424 I print_info: LF token         = 187 ''
0.00.041.425 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.425 I print_info: max token length = 1024
0.00.041.425 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.654.767 I load_tensors: offloading 24 repeating layers to GPU
0.00.654.772 I load_tensors: offloading output layer to GPU
0.00.654.773 I load_tensors: offloaded 25/25 layers to GPU
0.00.654.797 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.654.799 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.655.985 I llama_init_from_model: n_seq_max     = 1
0.00.655.987 I llama_init_from_model: n_ctx         = 2048
0.00.655.988 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.655.988 I llama_init_from_model: n_batch       = 2048
0.00.655.989 I llama_init_from_model: n_ubatch      = 512
0.00.655.989 I llama_init_from_model: flash_attn    = 0
0.00.655.990 I llama_init_from_model: freq_base     = 10000.0
0.00.655.990 I llama_init_from_model: freq_scale    = 1
0.00.655.992 I ggml_metal_init: allocating
0.00.656.003 I ggml_metal_init: found device: Apple M4
0.00.656.010 I ggml_metal_init: picking default device: Apple M4
0.00.657.476 I ggml_metal_init: using embedded metal library
0.00.663.298 I ggml_metal_init: GPU name:   Apple M4
0.00.663.301 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.663.302 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.663.303 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.663.303 I ggml_metal_init: simdgroup reduction   = true
0.00.663.303 I ggml_metal_init: simdgroup matrix mul. = true
0.00.663.304 I ggml_metal_init: has residency sets    = true
0.00.663.304 I ggml_metal_init: has bfloat            = true
0.00.663.304 I ggml_metal_init: use bfloat            = true
0.00.663.305 I ggml_metal_init: hasUnifiedMemory      = true
0.00.663.306 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.679.414 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.733.664 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.733.670 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.733.704 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.738.195 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.738.197 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.738.197 I llama_init_from_model: graph nodes  = 967
0.00.738.197 I llama_init_from_model: graph splits = 2
0.00.738.204 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.738.335 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.738.336 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.803.079 I main: llama threadpool init, n_threads = 4
0.00.803.120 I 
0.00.803.143 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.145 I 
0.00.803.299 I sampler seed: 1234
0.00.803.304 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.803.315 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.803.317 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.803.317 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.675.492 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53665.91 tokens per second)
0.01.675.493 I llama_perf_context_print:        load time =     792.04 ms
0.01.675.493 I llama_perf_context_print: prompt eval time =      54.32 ms /     7 tokens (    7.76 ms per token,   128.87 tokens per second)
0.01.675.494 I llama_perf_context_print:        eval time =     814.85 ms /    63 runs   (   12.93 ms per token,    77.32 tokens per second)
0.01.675.494 I llama_perf_context_print:       total time =     873.11 ms /    70 tokens
0.01.675.752 I ggml_metal_free: deallocating

real	0m1.694s
user	0m0.108s
sys	0m0.227s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4652 (9dd7a039) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.439 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.349 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.019.353 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.355 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.355 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.356 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.356 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.356 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.357 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.358 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.358 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.358 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.359 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.360 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.361 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.362 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.363 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.365 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.118 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.090 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.756 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.757 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.758 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.758 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.758 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.759 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.759 I llama_model_loader: - type  f32:  194 tensors
0.00.027.760 I llama_model_loader: - type q6_K:   98 tensors
0.00.027.760 I print_info: file format = GGUF V3 (latest)
0.00.027.760 I print_info: file type   = Q6_K
0.00.027.761 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.035.485 I load: special tokens cache size = 25
0.00.041.334 I load: token to piece cache size = 0.2984 MB
0.00.041.337 I print_info: arch             = gptneox
0.00.041.337 I print_info: vocab_only       = 0
0.00.041.337 I print_info: n_ctx_train      = 2048
0.00.041.337 I print_info: n_embd           = 2048
0.00.041.338 I print_info: n_layer          = 24
0.00.041.341 I print_info: n_head           = 16
0.00.041.341 I print_info: n_head_kv        = 16
0.00.041.342 I print_info: n_rot            = 32
0.00.041.342 I print_info: n_swa            = 0
0.00.041.342 I print_info: n_embd_head_k    = 128
0.00.041.342 I print_info: n_embd_head_v    = 128
0.00.041.343 I print_info: n_gqa            = 1
0.00.041.344 I print_info: n_embd_k_gqa     = 2048
0.00.041.345 I print_info: n_embd_v_gqa     = 2048
0.00.041.346 I print_info: f_norm_eps       = 1.0e-05
0.00.041.346 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.346 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.347 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.347 I print_info: f_logit_scale    = 0.0e+00
0.00.041.347 I print_info: n_ff             = 8192
0.00.041.348 I print_info: n_expert         = 0
0.00.041.348 I print_info: n_expert_used    = 0
0.00.041.348 I print_info: causal attn      = 1
0.00.041.348 I print_info: pooling type     = 0
0.00.041.348 I print_info: rope type        = 2
0.00.041.349 I print_info: rope scaling     = linear
0.00.041.349 I print_info: freq_base_train  = 10000.0
0.00.041.349 I print_info: freq_scale_train = 1
0.00.041.349 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.350 I print_info: rope_finetuned   = unknown
0.00.041.350 I print_info: ssm_d_conv       = 0
0.00.041.350 I print_info: ssm_d_inner      = 0
0.00.041.350 I print_info: ssm_d_state      = 0
0.00.041.350 I print_info: ssm_dt_rank      = 0
0.00.041.350 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.352 I print_info: model type       = 1.4B
0.00.041.352 I print_info: model params     = 1.41 B
0.00.041.352 I print_info: general.name     = 1.4B
0.00.041.353 I print_info: vocab type       = BPE
0.00.041.353 I print_info: n_vocab          = 50304
0.00.041.353 I print_info: n_merges         = 50009
0.00.041.353 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.353 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.353 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.354 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.354 I print_info: LF token         = 187 ''
0.00.041.354 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.354 I print_info: max token length = 1024
0.00.041.355 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.366.382 I load_tensors: offloading 24 repeating layers to GPU
0.00.366.399 I load_tensors: offloading output layer to GPU
0.00.366.400 I load_tensors: offloaded 25/25 layers to GPU
0.00.366.436 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.366.437 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.367.906 I llama_init_from_model: n_seq_max     = 1
0.00.367.909 I llama_init_from_model: n_ctx         = 128
0.00.367.909 I llama_init_from_model: n_ctx_per_seq = 128
0.00.367.910 I llama_init_from_model: n_batch       = 128
0.00.367.910 I llama_init_from_model: n_ubatch      = 128
0.00.367.911 I llama_init_from_model: flash_attn    = 0
0.00.367.912 I llama_init_from_model: freq_base     = 10000.0
0.00.367.912 I llama_init_from_model: freq_scale    = 1
0.00.367.913 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.367.915 I ggml_metal_init: allocating
0.00.367.945 I ggml_metal_init: found device: Apple M4
0.00.367.955 I ggml_metal_init: picking default device: Apple M4
0.00.369.370 I ggml_metal_init: using embedded metal library
0.00.375.577 I ggml_metal_init: GPU name:   Apple M4
0.00.375.581 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.375.581 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.375.582 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.375.583 I ggml_metal_init: simdgroup reduction   = true
0.00.375.583 I ggml_metal_init: simdgroup matrix mul. = true
0.00.375.583 I ggml_metal_init: has residency sets    = true
0.00.375.584 I ggml_metal_init: has bfloat            = true
0.00.375.584 I ggml_metal_init: use bfloat            = true
0.00.375.585 I ggml_metal_init: hasUnifiedMemory      = true
0.00.375.586 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.391.927 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.395.419 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.395.423 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.395.464 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.398.881 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.398.882 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.398.883 I llama_init_from_model: graph nodes  = 967
0.00.398.883 I llama_init_from_model: graph splits = 2
0.00.398.885 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.398.886 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.429.641 I 
0.00.429.727 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.429.735 I perplexity: tokenizing the input ..
0.00.437.035 I perplexity: tokenization took 7.296 ms
0.00.437.043 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.578.322 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.579.854 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.579.879 I llama_perf_context_print:        load time =     418.19 ms
0.00.579.880 I llama_perf_context_print: prompt eval time =     140.39 ms /   128 tokens (    1.10 ms per token,   911.75 tokens per second)
0.00.579.881 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.579.881 I llama_perf_context_print:       total time =     150.24 ms /   129 tokens
0.00.580.267 I ggml_metal_free: deallocating

real	0m0.599s
user	0m0.079s
sys	0m0.110s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4652 (9dd7a039)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x122205d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x122206370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1222067e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x122206c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1222070c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x122207690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x122207c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1222081f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1222087a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x122208ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1222091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1222096a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12220a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12220a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12220b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12220b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12220bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12220c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12220ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12220d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12220dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12220e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12220eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12220f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12220faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12220fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1222103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x122211030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x122211570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x122211830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x122211cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x122211f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x122212820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x122212d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x122213020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1222134c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x122213960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x122213e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1222142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x122214740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x122214be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x122215080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x122215520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1222159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x122215c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x122216290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1222168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1222171c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1222177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x122217de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1222183f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x122218a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x122219010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x122219620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x122219e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12221a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12221a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12221aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12221b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12221b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12221bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12221bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12221c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12221c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12221cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12221d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12221d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12221db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12221dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12221e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12221e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12221edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12221f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12221f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12221fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x122220240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x122220790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x122220ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x122221230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x122221780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x122221cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x122222220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x122222770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x122222cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x122223210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x122223760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x122223cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x122224200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x122224750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x122224ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1222251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x122225740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x122225c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1222261e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x122226730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x122226c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1222271d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x122216eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x122227640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x122227df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x122228340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x122228890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x122228de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x122229330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x122229880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x122229dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12222a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12222a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12222adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12222b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12222b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12222bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12222c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12222c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12222cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12222d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12222d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12222da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12222dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12222e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12222e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12222eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12222f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12222f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12222fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12222ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1222303c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x122230860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x122230d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1222311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x122231640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x122231ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x122231f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x122232420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1222328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x122232d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x122233200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1222336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x122233b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x122233fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x122234480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x122234920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x122234dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x122235260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x122235700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x122235ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x122236040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1222364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x122236980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x122236e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1222372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x122237760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x122237c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1222380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x122238540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1222389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x122238e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x122239320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1222397c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x122239c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12223a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12223a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12223aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12223aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12223b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12223b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12223bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12223c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12223c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12223caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12223cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12223d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12223d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12223dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12223e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12223e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12223eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12223efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12223f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12223f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12223fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x122240220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1222406c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x122240b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x122241000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1222414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x122241940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x122241de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x122242280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x122242720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x122242bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x122243060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x122243500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x122243a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x122243fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1222444f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x122244a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x122244d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x122245310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x122245920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x122245f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x122246720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x122246bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x122246e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x122247490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x122247aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x122248290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x122248730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x122248bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x122249070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x122249820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x122249d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12224a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12224a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12224ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12224b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12224b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12224bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12224c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12224c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12224cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12224d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12224d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12224dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12224e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12224e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12224ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12224f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12224f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12224fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x122250260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1222507b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x122250d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x122251250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1222517a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x122251cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x122252240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x122252790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x122252ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x122253230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x122253780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x122253cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x122254220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x122254770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x122254cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x122255210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x122255760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x122255cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x122256200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x122256750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x122256ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1222571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x122257740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x122257c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1222581e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x122258730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x122258c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1222591d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x122259720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x122259c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12225a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12225a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12225ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12225b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12225b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12225bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12225c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12225c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12225cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12225cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12225d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12225d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12225dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12225e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12225e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12225eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12225efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12225f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12225f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12225fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x122260260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x122260700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x122260c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x122261370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x122261a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1222621b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1222628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x122262b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x122263380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x122263640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x122263c50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.724.087 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.724.091 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x122263900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x122247140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x122244fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x122245be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x122218cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1222186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12221acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x122210070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x122216b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x122217480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x122217a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x122216550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1222192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12220f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1222198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12221b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x122227900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x122262e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x122212250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x122212510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1222461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x122210680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x122210940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x122210c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1222640b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x122264370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x122264630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1222648f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x122264bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x122264e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x122265130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1222653f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1222656b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x122265970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x122265c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x122265ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1222661b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x122266470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x122266730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1222669f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x122266cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x122266f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x122267230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1222674f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1222677b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x122267a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x122267d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x122267ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1222682b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x122268570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x122268830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x122268af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x122268db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x122269070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x122269330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1222695f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1222698b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x122269b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x122269e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12226a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12226a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12226a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12226a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12226abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12226aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12226b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12226b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12226b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12226b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12226bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12226bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12226c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12226c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12226c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12226ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12226ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12226cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12226d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12226d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12226d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12226dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12226dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12226e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12226e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12226e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12226e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12226eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12226edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12226f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12226f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12226f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12226f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12226fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12226fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x122270130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1222703f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1222706b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x122270970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x122270c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x122270ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1222711b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x122271470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x122271730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1222719f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x122271cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x122271f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x122272230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1222724f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1222727b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x122272a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x122272d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x122272ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1222732b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x122273570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x122273830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x122273af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x122273db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x122274070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x122274330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1222745f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1222748b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x122274b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x122274e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1222750f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1222753b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x122275670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x122275930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x122275bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x122275eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x122276170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x122276430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1222766f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1222769b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x122276c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x122276f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1222771f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1222774b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x122277770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x122277a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x122277cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x122277fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x122278270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x122278530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1222787f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x122278ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x122278d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x122279030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1222792f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1222795b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x122279870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x122279b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x122279df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12227a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12227a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12227a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12227a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12227abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12227ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12227b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12227b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12227b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12227b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12227bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12227bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12227c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12227c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12227c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12227c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12227ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12227cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12227d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12227d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12227d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12227da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12227dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12227dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12227e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12227e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12227e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12227eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12227edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12227f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12227f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12227f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12227f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12227fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12227fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1222800f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1222803b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x122280670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x122280930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x122280bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x122280eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x122281170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x122281430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1222816f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1222819b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x122281c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x122281f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1222821f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1222824b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x122282770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x122282a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x122282f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1222834b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1222839f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x122283cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1222840b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x122284550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1222849f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1222851a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x122285460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x122285720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x122285b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x122286000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x122286470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1222868e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x122286d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1222871c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x122287630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x122287aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x122287f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x122288380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1222887f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x122288c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1222890d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x122289540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1222899b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x122289e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12228a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12228a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12228ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12228afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12228b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12228b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12228bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12228c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12228c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12228ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12228cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12228d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12228d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12228dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12228e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12228e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12228e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12228ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12228f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12228f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12228fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12228ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x122290430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1222908a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x122290d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x122291180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1222915f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x122291a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x122291ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x122292340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1222927b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x122292c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x122293090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x122293500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x122293970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x122293de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x122294250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1222946c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x122294b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x122294fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x122295410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x122295880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x122295cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x122296160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1222965d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x122296a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x122296eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x122297320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x122297790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x122297c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x122298070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1222984e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x122298950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x122298dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x122299830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x122299f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12229a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12229ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12229b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12229b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12229bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12229c110 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x122304e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1223052c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x122305730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x122305ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x122306010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x122306480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1223068f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x122306d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1223071d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x122307640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x122307ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1223081d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x122308cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1223094a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x122309cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12230a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12230aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12230b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12230b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12230c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12230c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12230cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12230d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12230dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12230e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12230e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12230e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12230edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12230f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12230f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12230fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x122310070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1223104e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1223107a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x122310c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x122311080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1223114f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x122311960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x122311dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x122312240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1223126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x122312b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x122312f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x122313400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x122313870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x122313ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x122314150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1223145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x122314a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x122314ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x122315310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x122315780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x122315bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x122316060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1223164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x122316940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x122316eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1223173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x122317820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x122317c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x122318100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x122318570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1223189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x122318e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1223192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x122319730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x122319ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12231a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12231a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12231a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12231ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12231b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12231b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12231bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12231bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12231c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12231c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12231cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12231d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12231d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12231d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12231de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12231e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12231e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12231eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12231eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12231f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12231f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12231fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1223201b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x122320620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x122320a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x122320f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x122321370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1223217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x122321c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1223220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x122322530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1223229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x122322e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x122323280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1223236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x122323b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1223243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1223246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x122324b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x122324f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x122325400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x122325870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x122325ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x122326150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1223265c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x122326a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x122326ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x122327310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x122327780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x122327bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x122328060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1223284d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x122328940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x122328db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x122329220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x122329690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x122329b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x122329f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12232a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12232a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12232acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12232b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12232b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12232ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12232be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12232c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12232c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12232cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12232d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12232d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12232d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12232dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12232e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12232e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12232eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12232ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12232f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12232f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12232fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x122330110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x122330580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1223309f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x122330e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1223312d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x122331740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x122331bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x122332020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x122332490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x122332900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x122332d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1223331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x122333650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x122333ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x122333f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1223343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x122334810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x122334c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1223350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x122335560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1223359d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x122335e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1223362b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x122336720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x122336b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x122337000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x122337470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1223378e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x122337d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1223381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x122338630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x122338aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x122338f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x122339380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1223397f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x122339c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12233a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12233a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12233a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12233ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12233b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12233b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12233bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12233bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12233c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12233c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12233cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12233d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12233d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12233da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12233def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12233e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12233e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12233ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12233f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12233f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12233f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12233fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x122340270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1223406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x122340b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x122340fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x122341430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1223418a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x122342420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1223426e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1223429a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x122342e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x122343280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1223436f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x122343b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x122343fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x122344440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1223448b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x122344d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x122345190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x122345600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x122345a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x122345ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x122346350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1223467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x122346c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1223470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x122347510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x122347980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x122347df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x122348260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1223486d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x122348b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x122348fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x122349420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x122349890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x122349d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12234a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12234a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12234aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12234aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12234b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12234b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12234bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12234c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12234c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12234c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12234cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12234d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12234d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12234db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12234df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12234e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12234e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12234ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12234f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12234f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12234fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12234fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x122350310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x122350780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x122350bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x122351060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1223514d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x122351940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x122351db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x122352220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x122352690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x122352b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x122352f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1223533e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x122353850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x122353cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x122354130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1223545a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x122354a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x122354e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1223552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x122355760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x122355bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x122356040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x122356ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1223571d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1223578f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x122358010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1223582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x122358740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x122358d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x122359350 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.826s
user	0m0.283s
sys	0m0.301s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4652 (9dd7a039)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14160da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14160e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14160e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14160eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14160ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14160f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14160f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14160fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1416100d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141610540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1416109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141611050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141611b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141612320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141612b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141613250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141613970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141614090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1416147b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141614f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1416156a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141615dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1416164e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141616d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1416174a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141617760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141617a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141617e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141618540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1416189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141618e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1416193b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141619820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141619ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141619f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14161a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14161aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14161af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14161b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14161b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14161bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14161c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14161c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14161c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14161ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14161d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14161d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14161e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14161e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14161e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14161ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14161f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14161f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14161fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14161feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141620560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141620a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141620cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141621130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141621800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141621c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141621ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1416223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1416228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141622dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1416232c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1416237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141623cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1416241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1416246c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141624bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1416250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1416255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141625ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141626070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141626620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141626bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141627180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141627730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141627ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141628290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141628840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141628df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1416293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141629950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141629f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14162a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14162aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14162b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14162b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14162bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14162c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14162c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14162cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14162d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14162d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14162dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14161dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14162e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14162e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14162edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14162f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14162f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14162fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141630490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141630a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141630ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1416315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141631b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141632100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1416326b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141632c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141633210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1416337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141633cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1416341c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1416346c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141634bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1416350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1416355c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141635ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141635fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1416364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1416369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141636ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1416373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1416378c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141637dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1416382c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1416387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141638cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1416391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1416396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141639bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14163a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14163a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14163aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14163afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14163b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14163b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14163bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14163c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14163c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14163cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14163d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14163d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14163dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14163e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14163e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14163ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14163f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14163f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14163fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14163ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1416404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1416409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141640ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1416413c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1416418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141641dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1416422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1416427c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141642cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1416431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1416436c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141643bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1416440c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1416445c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141644ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141644fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1416454c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1416459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141645ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1416463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1416468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141646dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1416472c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1416477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141647cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1416481c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1416486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141648bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1416490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1416495c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141649ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141649fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14164a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14164a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14164aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14164b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14164b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14164bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14164c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14164c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14164cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14164d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14164d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14164de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14164e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14164eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14164f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14164f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14164fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141650000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141650610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141650c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141651410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1416518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141651d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1416521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1416529a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141652ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141653440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141653990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141653ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141654430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141654980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141654ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141655420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141655970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141655ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141656410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141656960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141656eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141657400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141657950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141657ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1416583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141658940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141658e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1416593e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141659930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141659e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14165a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14165a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14165ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14165b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14165b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14165be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14165c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14165c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14165ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14165d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14165d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14165de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14165e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14165e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14165ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14165f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14165f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14165fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141660370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1416608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141660e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141661360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1416618b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141661e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141662350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1416628a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141662df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141663340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141663890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141663de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141664330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141664880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141664dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141665320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1416657c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141665c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141666100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1416665a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141666a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141666ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141667380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141667820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141667cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141668160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141668600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141668aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141668f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1416693e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141669880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141669dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14166a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14166ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14166b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14166ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14166bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14166c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14166c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14166cdd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.101.136 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.140 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141706d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1417071e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141707650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141707ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141707f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1417083a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141708810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141708c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1417090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141709560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1417099d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14170a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14170abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14170b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14170bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14170c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14170c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14170d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14170d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14170dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14170e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14170ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14170f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14170fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141710360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141710620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1417108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141710d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1417111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141711630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141711aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141711fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141712440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141712700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141712b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141712fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141713450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1417138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141713d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1417141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141714610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141714a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141714ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141715360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1417157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141715c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1417160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141716520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141716990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141716e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141717270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1417176e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141717b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141717fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141718430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1417188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141718e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141719310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141719780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141719bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14171a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14171a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14171a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14171adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14171b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14171b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14171bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14171bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14171c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14171c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14171ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14171d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14171d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14171da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14171de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14171e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14171e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14171ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14171f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14171f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14171f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14171fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141720200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141720ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141720f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1417213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141721830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141721ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141722110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141722580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1417229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141722e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1417232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141723740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141723bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141724020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141724490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141724900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141724d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1417251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141725650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141725ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141725f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1417263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141726810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141726c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1417270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141727560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1417279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141727e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1417282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141728720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141728b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141729000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141729470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1417298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141729d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14172a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14172a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14172aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14172af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14172b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14172b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14172bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14172c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14172c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14172c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14172ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14172d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14172d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14172db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14172dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14172e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14172e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14172ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14172f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14172f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14172fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14172fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141730360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1417307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141730c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1417310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141731520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141731990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141731e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141732270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1417326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141732b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141732fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141733430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1417338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141733d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141734180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1417345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141734a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141734ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141735340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1417357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141735c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141736090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141736500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141736970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141736de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141737250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141737e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141738140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141738400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141738870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141738ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141739150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1417395c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141739a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141739ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14173a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14173a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14173abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14173b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14173b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14173b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14173bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14173c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14173c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14173cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14173cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14173d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14173d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14173dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14173e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14173e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14173ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14173ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14173f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14173f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14173fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141740040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1417404b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141740920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141740d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141741200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x141741670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x141741bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1417420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141742550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1417429c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141742e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1417432a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1417437c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141743cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141744840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141744b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1417450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141745680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141745c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141746200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1417467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141746d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141747340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141747900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141747ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141748480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141748a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141749000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1417495c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141749b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14174a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14174a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14174acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14174b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14174b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14174be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14174c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14174c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14174cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14174d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14174dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14174e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14174e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14174ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14174f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14174f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14174fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141750300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1417508c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141750e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141751440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141751a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141751fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141752580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141752b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141753100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1417536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141753c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141754240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141754800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141754dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141755380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141755940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141755f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1417564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141756a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141757040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141757600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141757bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141758180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141758740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141758d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141759200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141759700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141759c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14175a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14175a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14175ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14175b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14175b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14175ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14175bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14175c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14175c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14175ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14175d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14175d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14175e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14175e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14175f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14175f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14175fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141760220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1417604e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141760af0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1438055a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143805a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143805e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1438062f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143806760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143806bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143807040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1438074b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143807920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143807d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143808200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x143808860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x143809380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x143809b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14380a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14380aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14380b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14380b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14380bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14380c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14380ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14380d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14380dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14380e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14380eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14380edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14380f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14380f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14380f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14380fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143810270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1438107a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143810c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143810ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143811340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1438117b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143811c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143812090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143812500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143812970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143812de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143813250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1438136c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143813b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143813fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143814410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143814880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143814cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x143815160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1438155d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143815a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143815eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143816320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143816790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143816c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143817070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1438175e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143817ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143817f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1438183c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143818830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143818ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143819110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x143819580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1438199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x143819e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14381a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14381a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14381abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14381b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14381b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14381b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14381bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14381c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14381c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14381cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14381cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14381d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14381d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14381dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14381e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14381e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14381e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14381ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14381f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14381f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14381fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143820000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143820470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1438208e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143820d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1438211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143821630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143821aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143821f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143822380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1438227f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143822c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1438230d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143823540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1438239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x143823e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143824290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x143824b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143824de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x143825250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1438256c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143825b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143825fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143826410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143826880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143826cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143827160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1438275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143827a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143827eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143828320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143828790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143828c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143829070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1438294e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143829950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143829dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14382a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14382a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14382ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14382af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14382b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14382b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14382bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14382c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14382c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14382ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14382ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14382d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14382d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14382dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14382e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14382e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14382e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14382eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14382f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14382f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14382faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14382ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1438303d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143830840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143830cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143831120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143831590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143831a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143831e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1438322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143832750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143832bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143833030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1438334a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143833910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143833d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1438341f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143834660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143834ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143834f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1438353b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143835820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143835c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143836100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143836570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1438369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143836e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1438372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143837730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143837ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143838010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143838480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1438388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143838d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1438391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143839640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143839ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143839f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14383a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14383a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14383ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14383b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14383b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14383b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14383be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14383c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14383c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14383cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14383cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14383d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14383d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14383dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14383e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14383e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14383ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14383ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14383f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14383f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14383fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1438400c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143840530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1438409a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143840e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143841280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1438416f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143841b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143841fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143842b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143842e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1438430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143843540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1438439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x143843e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143844290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143844700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143844b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143844fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143845450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1438458c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143845d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1438461a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143846610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143846a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143846ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143847360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1438477d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143847c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1438480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143848520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143848990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143848e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143849270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1438496e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143849b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143849fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14384a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14384a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14384ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14384b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14384b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14384ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14384bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14384c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14384c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14384cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14384d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14384d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14384d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14384dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14384e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14384e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14384eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14384efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14384f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14384f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14384fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x143850160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1438505d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143850a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143850eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143851320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143851790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143851c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143852070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1438524e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x143852950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143852dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143853230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1438536a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143853b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143853f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1438543f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143854860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143854cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143855140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1438555b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143855a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143855e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143856300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143856770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1438571e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143857900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143858020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143858740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143858a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143858e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143859470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143859a80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.970s
user	0m0.238s
sys	0m0.184s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.57 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.73 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.30 sec*proc (2 tests)

Total Test time (real) =   2.31 sec
        2.33 real         0.54 user         0.28 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.31 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.55 sec*proc (2 tests)

Total Test time (real) =   0.56 sec
        0.57 real         0.13 user         0.08 sys
```
