Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.531s
user	0m0.876s
sys	0m1.227s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target sha1
[  5%] Built target build_info
[  5%] Built target xxhash
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Linking C executable ../bin/test-c
[ 34%] Built target llava
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 36%] Linking CXX static library libcommon.a
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple
[ 37%] Built target test-c
[ 37%] Built target llama-simple-chat
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-grammar-integration
[ 49%] Linking CXX executable ../bin/test-sampling
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-log
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-sampling
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-arg-parser
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../../bin/llama-batched-bench
[ 60%] Linking CXX executable ../bin/test-gguf
[ 60%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../bin/test-chat-template
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-model-load-cancel
[ 62%] Linking CXX executable ../bin/test-barrier
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Linking CXX executable ../bin/test-quantize-fns
[ 64%] Linking CXX executable ../bin/test-quantize-perf
[ 64%] Built target llama-batched-bench
[ 64%] Built target test-gguf
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-barrier
[ 64%] Built target test-autorelease
[ 64%] Built target test-chat-template
[ 64%] Built target test-backend-ops
[ 64%] Built target test-quantize-perf
[ 64%] Built target test-rope
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 72%] Linking CXX executable ../../bin/llama-batched
[ 72%] Linking CXX executable ../../bin/llama-embedding
[ 72%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-imatrix
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-embedding
[ 73%] Built target llama-batched
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-infill
[ 73%] Built target llama-bench
[ 73%] Built target llama-lookahead
[ 73%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-passkey
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-passkey
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-lookup
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-parallel
[ 83%] Built target llama-cli
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-quantize
[ 83%] Generating loading.html.hpp
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-perplexity
[ 84%] Built target llama-retrieval
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-run
[ 89%] Linking CXX executable ../../bin/llama-save-load-state
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-run
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 93%] Built target llama-speculative-simple
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 95%] Built target llama-gen-docs
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Built target llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.135s
user	0m6.150s
sys	0m9.733s

main: quantize time =  4483.34 ms
main:    total time =  4483.34 ms

main: quantize time =  2504.92 ms
main:    total time =  2504.92 ms

main: quantize time =  3737.44 ms
main:    total time =  3737.44 ms

main: quantize time =  2515.40 ms
main:    total time =  2515.40 ms

main: quantize time =  2627.80 ms
main:    total time =  2627.80 ms

main: quantize time =  4795.36 ms
main:    total time =  4795.36 ms

main: quantize time =  5544.49 ms
main:    total time =  5544.49 ms

main: quantize time =  6767.58 ms
main:    total time =  6767.58 ms

main: quantize time =  6061.69 ms
main:    total time =  6061.69 ms

main: quantize time =  4540.34 ms
main:    total time =  4540.34 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.151 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.335 I main: llama backend init
0.00.000.342 I main: load the model and apply lora adapter, if any
0.00.029.346 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.041.631 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.648 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.659 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.660 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.660 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.661 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.662 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.665 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.665 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.666 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.667 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.667 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.668 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.669 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.675 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.675 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.676 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.600 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.809 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.140 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.060.142 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.143 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.143 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.144 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.145 I llama_model_loader: - type  f32:  194 tensors
0.00.060.145 I llama_model_loader: - type  f16:   98 tensors
0.00.060.147 I print_info: file format = GGUF V3 (latest)
0.00.060.148 I print_info: file type   = all F32 (guessed)
0.00.060.150 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.088.553 I load: special tokens cache size = 25
0.00.095.549 I load: token to piece cache size = 0.2984 MB
0.00.095.552 I print_info: arch             = gptneox
0.00.095.552 I print_info: vocab_only       = 0
0.00.095.553 I print_info: n_ctx_train      = 2048
0.00.095.553 I print_info: n_embd           = 2048
0.00.095.553 I print_info: n_layer          = 24
0.00.095.556 I print_info: n_head           = 16
0.00.095.557 I print_info: n_head_kv        = 16
0.00.095.557 I print_info: n_rot            = 32
0.00.095.557 I print_info: n_swa            = 0
0.00.095.557 I print_info: n_embd_head_k    = 128
0.00.095.557 I print_info: n_embd_head_v    = 128
0.00.095.558 I print_info: n_gqa            = 1
0.00.095.559 I print_info: n_embd_k_gqa     = 2048
0.00.095.559 I print_info: n_embd_v_gqa     = 2048
0.00.095.560 I print_info: f_norm_eps       = 1.0e-05
0.00.095.560 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.095.560 I print_info: f_clamp_kqv      = 0.0e+00
0.00.095.561 I print_info: f_max_alibi_bias = 0.0e+00
0.00.095.561 I print_info: f_logit_scale    = 0.0e+00
0.00.095.562 I print_info: n_ff             = 8192
0.00.095.562 I print_info: n_expert         = 0
0.00.095.562 I print_info: n_expert_used    = 0
0.00.095.562 I print_info: causal attn      = 1
0.00.095.563 I print_info: pooling type     = 0
0.00.095.563 I print_info: rope type        = 2
0.00.095.564 I print_info: rope scaling     = linear
0.00.095.566 I print_info: freq_base_train  = 10000.0
0.00.095.566 I print_info: freq_scale_train = 1
0.00.095.566 I print_info: n_ctx_orig_yarn  = 2048
0.00.095.566 I print_info: rope_finetuned   = unknown
0.00.095.567 I print_info: ssm_d_conv       = 0
0.00.095.567 I print_info: ssm_d_inner      = 0
0.00.095.567 I print_info: ssm_d_state      = 0
0.00.095.567 I print_info: ssm_dt_rank      = 0
0.00.095.567 I print_info: ssm_dt_b_c_rms   = 0
0.00.095.567 I print_info: model type       = 1.4B
0.00.095.568 I print_info: model params     = 1.41 B
0.00.095.568 I print_info: general.name     = 1.4B
0.00.095.568 I print_info: vocab type       = BPE
0.00.095.568 I print_info: n_vocab          = 50304
0.00.095.569 I print_info: n_merges         = 50009
0.00.095.569 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.095.569 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.095.569 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.095.569 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.095.570 I print_info: LF token         = 128 'Ä'
0.00.095.570 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.095.570 I print_info: max token length = 1024
0.00.129.410 I load_tensors: offloading 24 repeating layers to GPU
0.00.129.414 I load_tensors: offloading output layer to GPU
0.00.129.415 I load_tensors: offloaded 25/25 layers to GPU
0.00.129.439 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.129.440 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.129.719 I llama_init_from_model: n_seq_max     = 1
0.00.129.720 I llama_init_from_model: n_ctx         = 2048
0.00.129.721 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.129.721 I llama_init_from_model: n_batch       = 2048
0.00.129.721 I llama_init_from_model: n_ubatch      = 512
0.00.129.721 I llama_init_from_model: flash_attn    = 0
0.00.129.721 I llama_init_from_model: freq_base     = 10000.0
0.00.129.722 I llama_init_from_model: freq_scale    = 1
0.00.129.722 I ggml_metal_init: allocating
0.00.129.742 I ggml_metal_init: found device: Apple M4
0.00.129.746 I ggml_metal_init: picking default device: Apple M4
0.00.130.315 I ggml_metal_init: using embedded metal library
0.00.139.250 I ggml_metal_init: GPU name:   Apple M4
0.00.139.252 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.139.252 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.139.252 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.139.253 I ggml_metal_init: simdgroup reduction   = true
0.00.139.253 I ggml_metal_init: simdgroup matrix mul. = true
0.00.139.253 I ggml_metal_init: has residency sets    = true
0.00.139.253 I ggml_metal_init: has bfloat            = true
0.00.139.253 I ggml_metal_init: use bfloat            = true
0.00.139.254 I ggml_metal_init: hasUnifiedMemory      = true
0.00.139.255 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.163.112 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.193.705 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.193.712 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.193.736 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.197.279 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.197.282 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.197.282 I llama_init_from_model: graph nodes  = 967
0.00.197.282 I llama_init_from_model: graph splits = 2
0.00.197.286 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.197.414 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.197.415 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.263.447 I main: llama threadpool init, n_threads = 4
0.00.263.489 I 
0.00.263.528 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.263.530 I 
0.00.263.595 I sampler seed: 1234
0.00.263.599 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.263.624 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.263.626 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.263.626 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.119.697 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.02.119.698 I llama_perf_context_print:        load time =     233.05 ms
0.02.119.698 I llama_perf_context_print: prompt eval time =      53.90 ms /     7 tokens (    7.70 ms per token,   129.87 tokens per second)
0.02.119.699 I llama_perf_context_print:        eval time =    1799.23 ms /    63 runs   (   28.56 ms per token,    35.02 tokens per second)
0.02.119.699 I llama_perf_context_print:       total time =    1857.29 ms /    70 tokens
0.02.119.919 I ggml_metal_free: deallocating

real	0m2.404s
user	0m0.143s
sys	0m0.128s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.928 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.787 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.792 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.794 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.794 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.795 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.795 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.795 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.796 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.797 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.797 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.797 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.799 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.799 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.800 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.802 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.802 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.802 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.896 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.036 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.109 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.110 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.110 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.111 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.111 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.112 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.112 I llama_model_loader: - type  f32:  194 tensors
0.00.031.112 I llama_model_loader: - type q8_0:   98 tensors
0.00.031.113 I print_info: file format = GGUF V3 (latest)
0.00.031.114 I print_info: file type   = Q8_0
0.00.031.115 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.050.526 I load: special tokens cache size = 25
0.00.056.484 I load: token to piece cache size = 0.2984 MB
0.00.056.489 I print_info: arch             = gptneox
0.00.056.489 I print_info: vocab_only       = 0
0.00.056.489 I print_info: n_ctx_train      = 2048
0.00.056.489 I print_info: n_embd           = 2048
0.00.056.490 I print_info: n_layer          = 24
0.00.056.499 I print_info: n_head           = 16
0.00.056.500 I print_info: n_head_kv        = 16
0.00.056.503 I print_info: n_rot            = 32
0.00.056.503 I print_info: n_swa            = 0
0.00.056.503 I print_info: n_embd_head_k    = 128
0.00.056.503 I print_info: n_embd_head_v    = 128
0.00.056.504 I print_info: n_gqa            = 1
0.00.056.505 I print_info: n_embd_k_gqa     = 2048
0.00.056.505 I print_info: n_embd_v_gqa     = 2048
0.00.056.506 I print_info: f_norm_eps       = 1.0e-05
0.00.056.506 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.507 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.507 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.507 I print_info: f_logit_scale    = 0.0e+00
0.00.056.508 I print_info: n_ff             = 8192
0.00.056.508 I print_info: n_expert         = 0
0.00.056.509 I print_info: n_expert_used    = 0
0.00.056.509 I print_info: causal attn      = 1
0.00.056.509 I print_info: pooling type     = 0
0.00.056.509 I print_info: rope type        = 2
0.00.056.509 I print_info: rope scaling     = linear
0.00.056.510 I print_info: freq_base_train  = 10000.0
0.00.056.511 I print_info: freq_scale_train = 1
0.00.056.511 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.511 I print_info: rope_finetuned   = unknown
0.00.056.511 I print_info: ssm_d_conv       = 0
0.00.056.511 I print_info: ssm_d_inner      = 0
0.00.056.512 I print_info: ssm_d_state      = 0
0.00.056.512 I print_info: ssm_dt_rank      = 0
0.00.056.512 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.512 I print_info: model type       = 1.4B
0.00.056.513 I print_info: model params     = 1.41 B
0.00.056.513 I print_info: general.name     = 1.4B
0.00.056.514 I print_info: vocab type       = BPE
0.00.056.514 I print_info: n_vocab          = 50304
0.00.056.514 I print_info: n_merges         = 50009
0.00.056.518 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.518 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.519 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.519 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.519 I print_info: LF token         = 128 'Ä'
0.00.056.520 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.520 I print_info: max token length = 1024
0.01.067.710 I load_tensors: offloading 24 repeating layers to GPU
0.01.067.716 I load_tensors: offloading output layer to GPU
0.01.067.717 I load_tensors: offloaded 25/25 layers to GPU
0.01.067.742 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.067.743 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.068.418 I llama_init_from_model: n_seq_max     = 1
0.01.068.420 I llama_init_from_model: n_ctx         = 2048
0.01.068.421 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.068.422 I llama_init_from_model: n_batch       = 2048
0.01.068.422 I llama_init_from_model: n_ubatch      = 512
0.01.068.422 I llama_init_from_model: flash_attn    = 0
0.01.068.423 I llama_init_from_model: freq_base     = 10000.0
0.01.068.424 I llama_init_from_model: freq_scale    = 1
0.01.068.425 I ggml_metal_init: allocating
0.01.068.443 I ggml_metal_init: found device: Apple M4
0.01.068.448 I ggml_metal_init: picking default device: Apple M4
0.01.069.677 I ggml_metal_init: using embedded metal library
0.01.074.942 I ggml_metal_init: GPU name:   Apple M4
0.01.074.945 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.074.946 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.074.946 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.074.947 I ggml_metal_init: simdgroup reduction   = true
0.01.074.947 I ggml_metal_init: simdgroup matrix mul. = true
0.01.074.948 I ggml_metal_init: has residency sets    = true
0.01.074.948 I ggml_metal_init: has bfloat            = true
0.01.074.948 I ggml_metal_init: use bfloat            = true
0.01.074.949 I ggml_metal_init: hasUnifiedMemory      = true
0.01.074.950 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.090.628 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.135.463 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.135.469 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.135.492 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.139.885 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.139.887 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.139.887 I llama_init_from_model: graph nodes  = 967
0.01.139.887 I llama_init_from_model: graph splits = 2
0.01.139.891 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.140.007 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.140.008 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.197.607 I main: llama threadpool init, n_threads = 4
0.01.197.651 I 
0.01.197.676 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.197.676 I 
0.01.197.905 I sampler seed: 1234
0.01.197.910 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.197.958 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.197.961 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.197.962 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.288.662 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51787.02 tokens per second)
0.02.288.663 I llama_perf_context_print:        load time =    1186.78 ms
0.02.288.663 I llama_perf_context_print: prompt eval time =      47.32 ms /     7 tokens (    6.76 ms per token,   147.94 tokens per second)
0.02.288.664 I llama_perf_context_print:        eval time =    1040.34 ms /    63 runs   (   16.51 ms per token,    60.56 tokens per second)
0.02.288.665 I llama_perf_context_print:       total time =    1091.95 ms /    70 tokens
0.02.288.876 I ggml_metal_free: deallocating

real	0m2.308s
user	0m0.119s
sys	0m0.242s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.011.917 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.487 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.492 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.494 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.494 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.495 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.495 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.496 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.497 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.497 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.498 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.498 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.498 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.499 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.499 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.501 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.501 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.502 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.436 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.540 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.507 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.509 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.509 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.510 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.510 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.510 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.511 I llama_model_loader: - type  f32:  194 tensors
0.00.028.511 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.512 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.512 I print_info: file format = GGUF V3 (latest)
0.00.028.513 I print_info: file type   = Q4_0
0.00.028.513 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.047.556 I load: special tokens cache size = 25
0.00.053.357 I load: token to piece cache size = 0.2984 MB
0.00.053.360 I print_info: arch             = gptneox
0.00.053.360 I print_info: vocab_only       = 0
0.00.053.361 I print_info: n_ctx_train      = 2048
0.00.053.361 I print_info: n_embd           = 2048
0.00.053.361 I print_info: n_layer          = 24
0.00.053.368 I print_info: n_head           = 16
0.00.053.369 I print_info: n_head_kv        = 16
0.00.053.369 I print_info: n_rot            = 32
0.00.053.369 I print_info: n_swa            = 0
0.00.053.369 I print_info: n_embd_head_k    = 128
0.00.053.369 I print_info: n_embd_head_v    = 128
0.00.053.370 I print_info: n_gqa            = 1
0.00.053.371 I print_info: n_embd_k_gqa     = 2048
0.00.053.372 I print_info: n_embd_v_gqa     = 2048
0.00.053.372 I print_info: f_norm_eps       = 1.0e-05
0.00.053.373 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.373 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.373 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.374 I print_info: f_logit_scale    = 0.0e+00
0.00.053.374 I print_info: n_ff             = 8192
0.00.053.374 I print_info: n_expert         = 0
0.00.053.374 I print_info: n_expert_used    = 0
0.00.053.375 I print_info: causal attn      = 1
0.00.053.375 I print_info: pooling type     = 0
0.00.053.375 I print_info: rope type        = 2
0.00.053.375 I print_info: rope scaling     = linear
0.00.053.375 I print_info: freq_base_train  = 10000.0
0.00.053.376 I print_info: freq_scale_train = 1
0.00.053.376 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.376 I print_info: rope_finetuned   = unknown
0.00.053.376 I print_info: ssm_d_conv       = 0
0.00.053.376 I print_info: ssm_d_inner      = 0
0.00.053.376 I print_info: ssm_d_state      = 0
0.00.053.376 I print_info: ssm_dt_rank      = 0
0.00.053.376 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.377 I print_info: model type       = 1.4B
0.00.053.379 I print_info: model params     = 1.41 B
0.00.053.380 I print_info: general.name     = 1.4B
0.00.053.380 I print_info: vocab type       = BPE
0.00.053.380 I print_info: n_vocab          = 50304
0.00.053.381 I print_info: n_merges         = 50009
0.00.053.381 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.381 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.381 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.381 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.382 I print_info: LF token         = 128 'Ä'
0.00.053.382 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.382 I print_info: max token length = 1024
0.00.598.839 I load_tensors: offloading 24 repeating layers to GPU
0.00.598.854 I load_tensors: offloading output layer to GPU
0.00.598.855 I load_tensors: offloaded 25/25 layers to GPU
0.00.598.888 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.598.889 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.600.469 I llama_init_from_model: n_seq_max     = 1
0.00.600.474 I llama_init_from_model: n_ctx         = 2048
0.00.600.475 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.600.475 I llama_init_from_model: n_batch       = 2048
0.00.600.476 I llama_init_from_model: n_ubatch      = 512
0.00.600.476 I llama_init_from_model: flash_attn    = 0
0.00.600.479 I llama_init_from_model: freq_base     = 10000.0
0.00.600.479 I llama_init_from_model: freq_scale    = 1
0.00.600.486 I ggml_metal_init: allocating
0.00.600.554 I ggml_metal_init: found device: Apple M4
0.00.600.563 I ggml_metal_init: picking default device: Apple M4
0.00.602.281 I ggml_metal_init: using embedded metal library
0.00.607.701 I ggml_metal_init: GPU name:   Apple M4
0.00.607.707 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.607.708 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.607.708 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.607.709 I ggml_metal_init: simdgroup reduction   = true
0.00.607.709 I ggml_metal_init: simdgroup matrix mul. = true
0.00.607.710 I ggml_metal_init: has residency sets    = true
0.00.607.710 I ggml_metal_init: has bfloat            = true
0.00.607.710 I ggml_metal_init: use bfloat            = true
0.00.607.712 I ggml_metal_init: hasUnifiedMemory      = true
0.00.607.723 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.627.639 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.683.871 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.683.877 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.683.905 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.688.048 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.688.050 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.688.051 I llama_init_from_model: graph nodes  = 967
0.00.688.051 I llama_init_from_model: graph splits = 2
0.00.688.056 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.688.179 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.688.180 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.744.615 I main: llama threadpool init, n_threads = 4
0.00.744.663 I 
0.00.744.690 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.744.692 I 
0.00.744.917 I sampler seed: 1234
0.00.744.922 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.744.968 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.744.972 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.744.972 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.433.230 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52129.22 tokens per second)
0.01.433.230 I llama_perf_context_print:        load time =     731.80 ms
0.01.433.231 I llama_perf_context_print: prompt eval time =      49.02 ms /     7 tokens (    7.00 ms per token,   142.79 tokens per second)
0.01.433.233 I llama_perf_context_print:        eval time =     636.36 ms /    63 runs   (   10.10 ms per token,    99.00 tokens per second)
0.01.433.233 I llama_perf_context_print:       total time =     689.51 ms /    70 tokens
0.01.433.501 I ggml_metal_free: deallocating

real	0m1.452s
user	0m0.121s
sys	0m0.202s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.691 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.466 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.470 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.472 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.473 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.474 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.474 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.474 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.475 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.476 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.476 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.476 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.477 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.477 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.478 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.480 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.480 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.481 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.439 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.556 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.523 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.524 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.525 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.525 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.525 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.526 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.526 I llama_model_loader: - type  f32:  194 tensors
0.00.025.527 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.527 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.527 I print_info: file format = GGUF V3 (latest)
0.00.025.528 I print_info: file type   = Q4_1
0.00.025.529 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.345 I load: special tokens cache size = 25
0.00.050.529 I load: token to piece cache size = 0.2984 MB
0.00.050.531 I print_info: arch             = gptneox
0.00.050.532 I print_info: vocab_only       = 0
0.00.050.532 I print_info: n_ctx_train      = 2048
0.00.050.532 I print_info: n_embd           = 2048
0.00.050.532 I print_info: n_layer          = 24
0.00.050.536 I print_info: n_head           = 16
0.00.050.537 I print_info: n_head_kv        = 16
0.00.050.537 I print_info: n_rot            = 32
0.00.050.537 I print_info: n_swa            = 0
0.00.050.537 I print_info: n_embd_head_k    = 128
0.00.050.537 I print_info: n_embd_head_v    = 128
0.00.050.538 I print_info: n_gqa            = 1
0.00.050.539 I print_info: n_embd_k_gqa     = 2048
0.00.050.540 I print_info: n_embd_v_gqa     = 2048
0.00.050.540 I print_info: f_norm_eps       = 1.0e-05
0.00.050.541 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.541 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.541 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.541 I print_info: f_logit_scale    = 0.0e+00
0.00.050.543 I print_info: n_ff             = 8192
0.00.050.543 I print_info: n_expert         = 0
0.00.050.543 I print_info: n_expert_used    = 0
0.00.050.543 I print_info: causal attn      = 1
0.00.050.544 I print_info: pooling type     = 0
0.00.050.545 I print_info: rope type        = 2
0.00.050.547 I print_info: rope scaling     = linear
0.00.050.547 I print_info: freq_base_train  = 10000.0
0.00.050.548 I print_info: freq_scale_train = 1
0.00.050.548 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.548 I print_info: rope_finetuned   = unknown
0.00.050.548 I print_info: ssm_d_conv       = 0
0.00.050.548 I print_info: ssm_d_inner      = 0
0.00.050.548 I print_info: ssm_d_state      = 0
0.00.050.549 I print_info: ssm_dt_rank      = 0
0.00.050.549 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.549 I print_info: model type       = 1.4B
0.00.050.549 I print_info: model params     = 1.41 B
0.00.050.549 I print_info: general.name     = 1.4B
0.00.050.550 I print_info: vocab type       = BPE
0.00.050.550 I print_info: n_vocab          = 50304
0.00.050.550 I print_info: n_merges         = 50009
0.00.050.551 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.551 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.551 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.551 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.551 I print_info: LF token         = 128 'Ä'
0.00.050.552 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.552 I print_info: max token length = 1024
0.00.644.860 I load_tensors: offloading 24 repeating layers to GPU
0.00.644.875 I load_tensors: offloading output layer to GPU
0.00.644.875 I load_tensors: offloaded 25/25 layers to GPU
0.00.644.908 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.644.916 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.646.446 I llama_init_from_model: n_seq_max     = 1
0.00.646.451 I llama_init_from_model: n_ctx         = 2048
0.00.646.452 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.646.452 I llama_init_from_model: n_batch       = 2048
0.00.646.452 I llama_init_from_model: n_ubatch      = 512
0.00.646.453 I llama_init_from_model: flash_attn    = 0
0.00.646.455 I llama_init_from_model: freq_base     = 10000.0
0.00.646.455 I llama_init_from_model: freq_scale    = 1
0.00.646.458 I ggml_metal_init: allocating
0.00.646.528 I ggml_metal_init: found device: Apple M4
0.00.646.537 I ggml_metal_init: picking default device: Apple M4
0.00.648.355 I ggml_metal_init: using embedded metal library
0.00.655.025 I ggml_metal_init: GPU name:   Apple M4
0.00.655.030 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.655.031 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.655.032 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.655.032 I ggml_metal_init: simdgroup reduction   = true
0.00.655.032 I ggml_metal_init: simdgroup matrix mul. = true
0.00.655.033 I ggml_metal_init: has residency sets    = true
0.00.655.033 I ggml_metal_init: has bfloat            = true
0.00.655.033 I ggml_metal_init: use bfloat            = true
0.00.655.034 I ggml_metal_init: hasUnifiedMemory      = true
0.00.655.045 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.673.039 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.726.949 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.726.956 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.726.981 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.731.771 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.731.773 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.731.774 I llama_init_from_model: graph nodes  = 967
0.00.731.774 I llama_init_from_model: graph splits = 2
0.00.731.785 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.731.915 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.731.916 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.788.492 I main: llama threadpool init, n_threads = 4
0.00.788.539 I 
0.00.788.564 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.788.564 I 
0.00.788.786 I sampler seed: 1234
0.00.788.791 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.788.801 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.788.802 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.788.804 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.524.016 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56082.15 tokens per second)
0.01.524.017 I llama_perf_context_print:        load time =     778.93 ms
0.01.524.017 I llama_perf_context_print: prompt eval time =      47.95 ms /     7 tokens (    6.85 ms per token,   145.98 tokens per second)
0.01.524.018 I llama_perf_context_print:        eval time =     684.43 ms /    63 runs   (   10.86 ms per token,    92.05 tokens per second)
0.01.524.018 I llama_perf_context_print:       total time =     736.40 ms /    70 tokens
0.01.524.217 I ggml_metal_free: deallocating

real	0m1.543s
user	0m0.121s
sys	0m0.206s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.011.277 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.810 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.814 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.815 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.816 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.816 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.816 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.817 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.818 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.818 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.818 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.819 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.819 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.820 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.820 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.822 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.822 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.822 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.830 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.943 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.972 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.974 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.974 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.974 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.974 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.975 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.975 I llama_model_loader: - type  f32:  194 tensors
0.00.026.976 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.976 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.976 I print_info: file format = GGUF V3 (latest)
0.00.026.977 I print_info: file type   = Q5_0
0.00.026.978 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.828 I load: special tokens cache size = 25
0.00.051.804 I load: token to piece cache size = 0.2984 MB
0.00.051.807 I print_info: arch             = gptneox
0.00.051.807 I print_info: vocab_only       = 0
0.00.051.807 I print_info: n_ctx_train      = 2048
0.00.051.808 I print_info: n_embd           = 2048
0.00.051.808 I print_info: n_layer          = 24
0.00.051.811 I print_info: n_head           = 16
0.00.051.811 I print_info: n_head_kv        = 16
0.00.051.811 I print_info: n_rot            = 32
0.00.051.813 I print_info: n_swa            = 0
0.00.051.813 I print_info: n_embd_head_k    = 128
0.00.051.815 I print_info: n_embd_head_v    = 128
0.00.051.816 I print_info: n_gqa            = 1
0.00.051.817 I print_info: n_embd_k_gqa     = 2048
0.00.051.818 I print_info: n_embd_v_gqa     = 2048
0.00.051.818 I print_info: f_norm_eps       = 1.0e-05
0.00.051.818 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.819 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.819 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.819 I print_info: f_logit_scale    = 0.0e+00
0.00.051.820 I print_info: n_ff             = 8192
0.00.051.820 I print_info: n_expert         = 0
0.00.051.820 I print_info: n_expert_used    = 0
0.00.051.820 I print_info: causal attn      = 1
0.00.051.821 I print_info: pooling type     = 0
0.00.051.821 I print_info: rope type        = 2
0.00.051.821 I print_info: rope scaling     = linear
0.00.051.821 I print_info: freq_base_train  = 10000.0
0.00.051.822 I print_info: freq_scale_train = 1
0.00.051.822 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.822 I print_info: rope_finetuned   = unknown
0.00.051.822 I print_info: ssm_d_conv       = 0
0.00.051.822 I print_info: ssm_d_inner      = 0
0.00.051.822 I print_info: ssm_d_state      = 0
0.00.051.823 I print_info: ssm_dt_rank      = 0
0.00.051.823 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.823 I print_info: model type       = 1.4B
0.00.051.823 I print_info: model params     = 1.41 B
0.00.051.824 I print_info: general.name     = 1.4B
0.00.051.824 I print_info: vocab type       = BPE
0.00.051.824 I print_info: n_vocab          = 50304
0.00.051.825 I print_info: n_merges         = 50009
0.00.051.825 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.825 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.825 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.825 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.826 I print_info: LF token         = 128 'Ä'
0.00.051.826 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.826 I print_info: max token length = 1024
0.00.660.593 I load_tensors: offloading 24 repeating layers to GPU
0.00.660.597 I load_tensors: offloading output layer to GPU
0.00.660.598 I load_tensors: offloaded 25/25 layers to GPU
0.00.660.620 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.660.621 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.661.941 I llama_init_from_model: n_seq_max     = 1
0.00.661.943 I llama_init_from_model: n_ctx         = 2048
0.00.661.944 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.661.944 I llama_init_from_model: n_batch       = 2048
0.00.661.945 I llama_init_from_model: n_ubatch      = 512
0.00.661.945 I llama_init_from_model: flash_attn    = 0
0.00.661.946 I llama_init_from_model: freq_base     = 10000.0
0.00.661.946 I llama_init_from_model: freq_scale    = 1
0.00.661.947 I ggml_metal_init: allocating
0.00.661.980 I ggml_metal_init: found device: Apple M4
0.00.661.988 I ggml_metal_init: picking default device: Apple M4
0.00.663.431 I ggml_metal_init: using embedded metal library
0.00.669.560 I ggml_metal_init: GPU name:   Apple M4
0.00.669.563 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.669.564 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.669.565 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.669.566 I ggml_metal_init: simdgroup reduction   = true
0.00.669.566 I ggml_metal_init: simdgroup matrix mul. = true
0.00.669.566 I ggml_metal_init: has residency sets    = true
0.00.669.567 I ggml_metal_init: has bfloat            = true
0.00.669.567 I ggml_metal_init: use bfloat            = true
0.00.669.568 I ggml_metal_init: hasUnifiedMemory      = true
0.00.669.569 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.686.570 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.743.153 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.743.159 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.743.183 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.747.699 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.747.702 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.747.702 I llama_init_from_model: graph nodes  = 967
0.00.747.702 I llama_init_from_model: graph splits = 2
0.00.747.707 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.747.823 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.747.824 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.803.269 I main: llama threadpool init, n_threads = 4
0.00.803.310 I 
0.00.803.334 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.335 I 
0.00.803.573 I sampler seed: 1234
0.00.803.577 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.803.588 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.803.588 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.803.588 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.596.363 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50426.14 tokens per second)
0.01.596.364 I llama_perf_context_print:        load time =     791.12 ms
0.01.596.365 I llama_perf_context_print: prompt eval time =      42.75 ms /     7 tokens (    6.11 ms per token,   163.74 tokens per second)
0.01.596.366 I llama_perf_context_print:        eval time =     747.19 ms /    63 runs   (   11.86 ms per token,    84.32 tokens per second)
0.01.596.366 I llama_perf_context_print:       total time =     793.97 ms /    70 tokens
0.01.596.633 I ggml_metal_free: deallocating

real	0m1.615s
user	0m0.120s
sys	0m0.204s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.149 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.377 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.382 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.388 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.388 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.389 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.389 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.390 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.391 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.391 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.391 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.392 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.392 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.392 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.393 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.395 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.395 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.396 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.487 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.649 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.811 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.814 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.814 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.814 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.815 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.815 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.816 I llama_model_loader: - type  f32:  194 tensors
0.00.025.816 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.816 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.817 I print_info: file format = GGUF V3 (latest)
0.00.025.821 I print_info: file type   = Q5_1
0.00.025.823 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.220 I load: special tokens cache size = 25
0.00.051.247 I load: token to piece cache size = 0.2984 MB
0.00.051.259 I print_info: arch             = gptneox
0.00.051.259 I print_info: vocab_only       = 0
0.00.051.259 I print_info: n_ctx_train      = 2048
0.00.051.261 I print_info: n_embd           = 2048
0.00.051.261 I print_info: n_layer          = 24
0.00.051.265 I print_info: n_head           = 16
0.00.051.267 I print_info: n_head_kv        = 16
0.00.051.267 I print_info: n_rot            = 32
0.00.051.267 I print_info: n_swa            = 0
0.00.051.267 I print_info: n_embd_head_k    = 128
0.00.051.267 I print_info: n_embd_head_v    = 128
0.00.051.268 I print_info: n_gqa            = 1
0.00.051.268 I print_info: n_embd_k_gqa     = 2048
0.00.051.269 I print_info: n_embd_v_gqa     = 2048
0.00.051.269 I print_info: f_norm_eps       = 1.0e-05
0.00.051.295 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.297 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.298 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.299 I print_info: f_logit_scale    = 0.0e+00
0.00.051.301 I print_info: n_ff             = 8192
0.00.051.301 I print_info: n_expert         = 0
0.00.051.302 I print_info: n_expert_used    = 0
0.00.051.302 I print_info: causal attn      = 1
0.00.051.302 I print_info: pooling type     = 0
0.00.051.302 I print_info: rope type        = 2
0.00.051.302 I print_info: rope scaling     = linear
0.00.051.303 I print_info: freq_base_train  = 10000.0
0.00.051.303 I print_info: freq_scale_train = 1
0.00.051.303 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.304 I print_info: rope_finetuned   = unknown
0.00.051.304 I print_info: ssm_d_conv       = 0
0.00.051.304 I print_info: ssm_d_inner      = 0
0.00.051.304 I print_info: ssm_d_state      = 0
0.00.051.304 I print_info: ssm_dt_rank      = 0
0.00.051.304 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.305 I print_info: model type       = 1.4B
0.00.051.305 I print_info: model params     = 1.41 B
0.00.051.305 I print_info: general.name     = 1.4B
0.00.051.306 I print_info: vocab type       = BPE
0.00.051.306 I print_info: n_vocab          = 50304
0.00.051.306 I print_info: n_merges         = 50009
0.00.051.306 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.306 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.307 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.307 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.307 I print_info: LF token         = 128 'Ä'
0.00.051.310 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.310 I print_info: max token length = 1024
0.00.687.745 I load_tensors: offloading 24 repeating layers to GPU
0.00.687.751 I load_tensors: offloading output layer to GPU
0.00.687.751 I load_tensors: offloaded 25/25 layers to GPU
0.00.687.771 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.687.772 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.688.555 I llama_init_from_model: n_seq_max     = 1
0.00.688.558 I llama_init_from_model: n_ctx         = 2048
0.00.688.559 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.688.559 I llama_init_from_model: n_batch       = 2048
0.00.688.559 I llama_init_from_model: n_ubatch      = 512
0.00.688.560 I llama_init_from_model: flash_attn    = 0
0.00.688.561 I llama_init_from_model: freq_base     = 10000.0
0.00.688.561 I llama_init_from_model: freq_scale    = 1
0.00.688.563 I ggml_metal_init: allocating
0.00.688.619 I ggml_metal_init: found device: Apple M4
0.00.688.627 I ggml_metal_init: picking default device: Apple M4
0.00.689.767 I ggml_metal_init: using embedded metal library
0.00.694.275 I ggml_metal_init: GPU name:   Apple M4
0.00.694.283 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.694.284 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.694.285 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.694.285 I ggml_metal_init: simdgroup reduction   = true
0.00.694.285 I ggml_metal_init: simdgroup matrix mul. = true
0.00.694.286 I ggml_metal_init: has residency sets    = true
0.00.694.286 I ggml_metal_init: has bfloat            = true
0.00.694.286 I ggml_metal_init: use bfloat            = true
0.00.694.288 I ggml_metal_init: hasUnifiedMemory      = true
0.00.694.290 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.705.461 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.736.096 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.736.102 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.736.125 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.740.352 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.740.353 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.740.354 I llama_init_from_model: graph nodes  = 967
0.00.740.354 I llama_init_from_model: graph splits = 2
0.00.740.358 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.740.488 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.740.489 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.799.623 I main: llama threadpool init, n_threads = 4
0.00.799.667 I 
0.00.799.690 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.799.691 I 
0.00.799.912 I sampler seed: 1234
0.00.799.917 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.799.936 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.799.936 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.799.936 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.644.084 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48299.32 tokens per second)
0.01.644.085 I llama_perf_context_print:        load time =     789.58 ms
0.01.644.085 I llama_perf_context_print: prompt eval time =      46.45 ms /     7 tokens (    6.64 ms per token,   150.69 tokens per second)
0.01.644.086 I llama_perf_context_print:        eval time =     795.21 ms /    63 runs   (   12.62 ms per token,    79.22 tokens per second)
0.01.644.086 I llama_perf_context_print:       total time =     845.35 ms /    70 tokens
0.01.644.323 I ggml_metal_free: deallocating

real	0m1.664s
user	0m0.113s
sys	0m0.168s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.011.782 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.126 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.019.131 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.133 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.134 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.134 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.134 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.135 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.135 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.136 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.136 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.138 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.139 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.139 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.140 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.143 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.143 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.099 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.195 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.159 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.160 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.161 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.161 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.161 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.162 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.028.162 I llama_model_loader: - type  f32:  194 tensors
0.00.028.162 I llama_model_loader: - type q2_K:   49 tensors
0.00.028.163 I llama_model_loader: - type q3_K:   48 tensors
0.00.028.163 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.163 I print_info: file format = GGUF V3 (latest)
0.00.028.164 I print_info: file type   = Q2_K - Medium
0.00.028.165 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.047.944 I load: special tokens cache size = 25
0.00.053.793 I load: token to piece cache size = 0.2984 MB
0.00.053.796 I print_info: arch             = gptneox
0.00.053.796 I print_info: vocab_only       = 0
0.00.053.797 I print_info: n_ctx_train      = 2048
0.00.053.797 I print_info: n_embd           = 2048
0.00.053.797 I print_info: n_layer          = 24
0.00.053.800 I print_info: n_head           = 16
0.00.053.801 I print_info: n_head_kv        = 16
0.00.053.801 I print_info: n_rot            = 32
0.00.053.801 I print_info: n_swa            = 0
0.00.053.801 I print_info: n_embd_head_k    = 128
0.00.053.801 I print_info: n_embd_head_v    = 128
0.00.053.802 I print_info: n_gqa            = 1
0.00.053.803 I print_info: n_embd_k_gqa     = 2048
0.00.053.803 I print_info: n_embd_v_gqa     = 2048
0.00.053.804 I print_info: f_norm_eps       = 1.0e-05
0.00.053.804 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.806 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.807 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.807 I print_info: f_logit_scale    = 0.0e+00
0.00.053.807 I print_info: n_ff             = 8192
0.00.053.808 I print_info: n_expert         = 0
0.00.053.808 I print_info: n_expert_used    = 0
0.00.053.809 I print_info: causal attn      = 1
0.00.053.809 I print_info: pooling type     = 0
0.00.053.810 I print_info: rope type        = 2
0.00.053.810 I print_info: rope scaling     = linear
0.00.053.810 I print_info: freq_base_train  = 10000.0
0.00.053.810 I print_info: freq_scale_train = 1
0.00.053.811 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.811 I print_info: rope_finetuned   = unknown
0.00.053.811 I print_info: ssm_d_conv       = 0
0.00.053.811 I print_info: ssm_d_inner      = 0
0.00.053.811 I print_info: ssm_d_state      = 0
0.00.053.811 I print_info: ssm_dt_rank      = 0
0.00.053.811 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.812 I print_info: model type       = 1.4B
0.00.053.812 I print_info: model params     = 1.41 B
0.00.053.812 I print_info: general.name     = 1.4B
0.00.053.813 I print_info: vocab type       = BPE
0.00.053.813 I print_info: n_vocab          = 50304
0.00.053.813 I print_info: n_merges         = 50009
0.00.053.813 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.813 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.814 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.814 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.815 I print_info: LF token         = 128 'Ä'
0.00.053.819 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.819 I print_info: max token length = 1024
0.00.391.098 I load_tensors: offloading 24 repeating layers to GPU
0.00.391.115 I load_tensors: offloading output layer to GPU
0.00.391.116 I load_tensors: offloaded 25/25 layers to GPU
0.00.391.150 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.391.151 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.392.552 I llama_init_from_model: n_seq_max     = 1
0.00.392.558 I llama_init_from_model: n_ctx         = 2048
0.00.392.558 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.392.559 I llama_init_from_model: n_batch       = 2048
0.00.392.559 I llama_init_from_model: n_ubatch      = 512
0.00.392.559 I llama_init_from_model: flash_attn    = 0
0.00.392.561 I llama_init_from_model: freq_base     = 10000.0
0.00.392.562 I llama_init_from_model: freq_scale    = 1
0.00.392.564 I ggml_metal_init: allocating
0.00.392.656 I ggml_metal_init: found device: Apple M4
0.00.392.665 I ggml_metal_init: picking default device: Apple M4
0.00.394.448 I ggml_metal_init: using embedded metal library
0.00.400.149 I ggml_metal_init: GPU name:   Apple M4
0.00.400.165 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.400.166 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.400.167 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.400.167 I ggml_metal_init: simdgroup reduction   = true
0.00.400.167 I ggml_metal_init: simdgroup matrix mul. = true
0.00.400.168 I ggml_metal_init: has residency sets    = true
0.00.400.168 I ggml_metal_init: has bfloat            = true
0.00.400.168 I ggml_metal_init: use bfloat            = true
0.00.400.170 I ggml_metal_init: hasUnifiedMemory      = true
0.00.400.175 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.420.714 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.480.260 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.480.268 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.480.295 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.484.592 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.484.594 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.484.594 I llama_init_from_model: graph nodes  = 967
0.00.484.594 I llama_init_from_model: graph splits = 2
0.00.484.601 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.484.732 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.484.732 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.540.464 I main: llama threadpool init, n_threads = 4
0.00.540.508 I 
0.00.540.534 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.540.534 I 
0.00.540.753 I sampler seed: 1234
0.00.540.758 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.540.782 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.540.783 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.540.783 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.217.922 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52475.98 tokens per second)
0.01.217.923 I llama_perf_context_print:        load time =     527.78 ms
0.01.217.924 I llama_perf_context_print: prompt eval time =      35.78 ms /     7 tokens (    5.11 ms per token,   195.66 tokens per second)
0.01.217.924 I llama_perf_context_print:        eval time =     638.36 ms /    63 runs   (   10.13 ms per token,    98.69 tokens per second)
0.01.217.925 I llama_perf_context_print:       total time =     678.36 ms /    70 tokens
0.01.218.151 I ggml_metal_free: deallocating

real	0m1.237s
user	0m0.123s
sys	0m0.170s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.771 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.689 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.695 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.697 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.697 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.698 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.698 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.699 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.700 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.700 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.700 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.701 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.701 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.702 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.702 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.703 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.704 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.704 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.534 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.533 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.343 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.344 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.345 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.345 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.345 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.346 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.346 I llama_model_loader: - type  f32:  194 tensors
0.00.025.346 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.347 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.347 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.347 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.348 I print_info: file format = GGUF V3 (latest)
0.00.025.348 I print_info: file type   = Q3_K - Medium
0.00.025.349 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.884 I load: special tokens cache size = 25
0.00.051.168 I load: token to piece cache size = 0.2984 MB
0.00.051.171 I print_info: arch             = gptneox
0.00.051.171 I print_info: vocab_only       = 0
0.00.051.171 I print_info: n_ctx_train      = 2048
0.00.051.172 I print_info: n_embd           = 2048
0.00.051.172 I print_info: n_layer          = 24
0.00.051.175 I print_info: n_head           = 16
0.00.051.176 I print_info: n_head_kv        = 16
0.00.051.176 I print_info: n_rot            = 32
0.00.051.176 I print_info: n_swa            = 0
0.00.051.176 I print_info: n_embd_head_k    = 128
0.00.051.176 I print_info: n_embd_head_v    = 128
0.00.051.177 I print_info: n_gqa            = 1
0.00.051.178 I print_info: n_embd_k_gqa     = 2048
0.00.051.180 I print_info: n_embd_v_gqa     = 2048
0.00.051.180 I print_info: f_norm_eps       = 1.0e-05
0.00.051.180 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.185 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.185 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.186 I print_info: f_logit_scale    = 0.0e+00
0.00.051.186 I print_info: n_ff             = 8192
0.00.051.187 I print_info: n_expert         = 0
0.00.051.187 I print_info: n_expert_used    = 0
0.00.051.187 I print_info: causal attn      = 1
0.00.051.187 I print_info: pooling type     = 0
0.00.051.187 I print_info: rope type        = 2
0.00.051.188 I print_info: rope scaling     = linear
0.00.051.188 I print_info: freq_base_train  = 10000.0
0.00.051.188 I print_info: freq_scale_train = 1
0.00.051.189 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.189 I print_info: rope_finetuned   = unknown
0.00.051.189 I print_info: ssm_d_conv       = 0
0.00.051.189 I print_info: ssm_d_inner      = 0
0.00.051.189 I print_info: ssm_d_state      = 0
0.00.051.189 I print_info: ssm_dt_rank      = 0
0.00.051.190 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.190 I print_info: model type       = 1.4B
0.00.051.190 I print_info: model params     = 1.41 B
0.00.051.190 I print_info: general.name     = 1.4B
0.00.051.191 I print_info: vocab type       = BPE
0.00.051.191 I print_info: n_vocab          = 50304
0.00.051.193 I print_info: n_merges         = 50009
0.00.051.193 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.193 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.193 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.193 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.194 I print_info: LF token         = 128 'Ä'
0.00.051.194 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.194 I print_info: max token length = 1024
0.00.474.360 I load_tensors: offloading 24 repeating layers to GPU
0.00.474.374 I load_tensors: offloading output layer to GPU
0.00.474.375 I load_tensors: offloaded 25/25 layers to GPU
0.00.474.409 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.474.411 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.475.942 I llama_init_from_model: n_seq_max     = 1
0.00.475.946 I llama_init_from_model: n_ctx         = 2048
0.00.475.947 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.475.947 I llama_init_from_model: n_batch       = 2048
0.00.475.947 I llama_init_from_model: n_ubatch      = 512
0.00.475.948 I llama_init_from_model: flash_attn    = 0
0.00.475.950 I llama_init_from_model: freq_base     = 10000.0
0.00.475.951 I llama_init_from_model: freq_scale    = 1
0.00.475.953 I ggml_metal_init: allocating
0.00.476.061 I ggml_metal_init: found device: Apple M4
0.00.476.072 I ggml_metal_init: picking default device: Apple M4
0.00.477.900 I ggml_metal_init: using embedded metal library
0.00.483.359 I ggml_metal_init: GPU name:   Apple M4
0.00.483.363 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.483.364 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.483.365 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.483.366 I ggml_metal_init: simdgroup reduction   = true
0.00.483.366 I ggml_metal_init: simdgroup matrix mul. = true
0.00.483.366 I ggml_metal_init: has residency sets    = true
0.00.483.367 I ggml_metal_init: has bfloat            = true
0.00.483.367 I ggml_metal_init: use bfloat            = true
0.00.483.368 I ggml_metal_init: hasUnifiedMemory      = true
0.00.483.369 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.502.597 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.560.221 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.560.227 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.560.254 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.564.416 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.564.419 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.564.419 I llama_init_from_model: graph nodes  = 967
0.00.564.419 I llama_init_from_model: graph splits = 2
0.00.564.425 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.564.555 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.564.555 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.910 I main: llama threadpool init, n_threads = 4
0.00.618.963 I 
0.00.618.990 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.992 I 
0.00.619.229 I sampler seed: 1234
0.00.619.233 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.619.278 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.619.282 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.619.283 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.361.530 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51862.67 tokens per second)
0.01.361.530 I llama_perf_context_print:        load time =     609.24 ms
0.01.361.531 I llama_perf_context_print: prompt eval time =      40.46 ms /     7 tokens (    5.78 ms per token,   173.00 tokens per second)
0.01.361.533 I llama_perf_context_print:        eval time =     698.80 ms /    63 runs   (   11.09 ms per token,    90.15 tokens per second)
0.01.361.533 I llama_perf_context_print:       total time =     743.51 ms /    70 tokens
0.01.361.735 I ggml_metal_free: deallocating

real	0m1.379s
user	0m0.122s
sys	0m0.183s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.846 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.507 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.512 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.514 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.515 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.515 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.515 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.516 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.517 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.517 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.517 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.518 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.519 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.519 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.520 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.521 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.522 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.522 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.413 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.505 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.401 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.403 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.403 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.403 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.404 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.404 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.405 I llama_model_loader: - type  f32:  194 tensors
0.00.024.405 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.405 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.405 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.406 I print_info: file format = GGUF V3 (latest)
0.00.024.406 I print_info: file type   = Q4_K - Medium
0.00.024.407 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.193 I load: special tokens cache size = 25
0.00.049.191 I load: token to piece cache size = 0.2984 MB
0.00.049.194 I print_info: arch             = gptneox
0.00.049.194 I print_info: vocab_only       = 0
0.00.049.194 I print_info: n_ctx_train      = 2048
0.00.049.195 I print_info: n_embd           = 2048
0.00.049.195 I print_info: n_layer          = 24
0.00.049.198 I print_info: n_head           = 16
0.00.049.199 I print_info: n_head_kv        = 16
0.00.049.199 I print_info: n_rot            = 32
0.00.049.199 I print_info: n_swa            = 0
0.00.049.199 I print_info: n_embd_head_k    = 128
0.00.049.199 I print_info: n_embd_head_v    = 128
0.00.049.200 I print_info: n_gqa            = 1
0.00.049.201 I print_info: n_embd_k_gqa     = 2048
0.00.049.204 I print_info: n_embd_v_gqa     = 2048
0.00.049.204 I print_info: f_norm_eps       = 1.0e-05
0.00.049.205 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.205 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.205 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.205 I print_info: f_logit_scale    = 0.0e+00
0.00.049.206 I print_info: n_ff             = 8192
0.00.049.206 I print_info: n_expert         = 0
0.00.049.207 I print_info: n_expert_used    = 0
0.00.049.207 I print_info: causal attn      = 1
0.00.049.207 I print_info: pooling type     = 0
0.00.049.207 I print_info: rope type        = 2
0.00.049.207 I print_info: rope scaling     = linear
0.00.049.208 I print_info: freq_base_train  = 10000.0
0.00.049.208 I print_info: freq_scale_train = 1
0.00.049.208 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.208 I print_info: rope_finetuned   = unknown
0.00.049.208 I print_info: ssm_d_conv       = 0
0.00.049.209 I print_info: ssm_d_inner      = 0
0.00.049.209 I print_info: ssm_d_state      = 0
0.00.049.209 I print_info: ssm_dt_rank      = 0
0.00.049.209 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.211 I print_info: model type       = 1.4B
0.00.049.211 I print_info: model params     = 1.41 B
0.00.049.211 I print_info: general.name     = 1.4B
0.00.049.212 I print_info: vocab type       = BPE
0.00.049.212 I print_info: n_vocab          = 50304
0.00.049.212 I print_info: n_merges         = 50009
0.00.049.213 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.213 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.213 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.213 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.213 I print_info: LF token         = 128 'Ä'
0.00.049.214 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.214 I print_info: max token length = 1024
0.00.527.229 I load_tensors: offloading 24 repeating layers to GPU
0.00.527.239 I load_tensors: offloading output layer to GPU
0.00.527.239 I load_tensors: offloaded 25/25 layers to GPU
0.00.527.278 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.527.280 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.528.906 I llama_init_from_model: n_seq_max     = 1
0.00.528.912 I llama_init_from_model: n_ctx         = 2048
0.00.528.912 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.528.913 I llama_init_from_model: n_batch       = 2048
0.00.528.913 I llama_init_from_model: n_ubatch      = 512
0.00.528.913 I llama_init_from_model: flash_attn    = 0
0.00.528.915 I llama_init_from_model: freq_base     = 10000.0
0.00.528.915 I llama_init_from_model: freq_scale    = 1
0.00.528.918 I ggml_metal_init: allocating
0.00.528.997 I ggml_metal_init: found device: Apple M4
0.00.529.006 I ggml_metal_init: picking default device: Apple M4
0.00.530.806 I ggml_metal_init: using embedded metal library
0.00.537.653 I ggml_metal_init: GPU name:   Apple M4
0.00.537.658 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.537.659 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.537.660 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.537.661 I ggml_metal_init: simdgroup reduction   = true
0.00.537.661 I ggml_metal_init: simdgroup matrix mul. = true
0.00.537.662 I ggml_metal_init: has residency sets    = true
0.00.537.662 I ggml_metal_init: has bfloat            = true
0.00.537.662 I ggml_metal_init: use bfloat            = true
0.00.537.663 I ggml_metal_init: hasUnifiedMemory      = true
0.00.537.673 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.555.747 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.608.723 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.608.730 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.608.756 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.613.838 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.613.840 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.613.840 I llama_init_from_model: graph nodes  = 967
0.00.613.841 I llama_init_from_model: graph splits = 2
0.00.613.847 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.613.976 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.613.976 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.533 I main: llama threadpool init, n_threads = 4
0.00.670.579 I 
0.00.670.604 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.605 I 
0.00.670.838 I sampler seed: 1234
0.00.670.842 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.670.853 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.670.853 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.670.853 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.431.804 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49685.09 tokens per second)
0.01.431.804 I llama_perf_context_print:        load time =     660.79 ms
0.01.431.805 I llama_perf_context_print: prompt eval time =      47.12 ms /     7 tokens (    6.73 ms per token,   148.57 tokens per second)
0.01.431.806 I llama_perf_context_print:        eval time =     710.76 ms /    63 runs   (   11.28 ms per token,    88.64 tokens per second)
0.01.431.806 I llama_perf_context_print:       total time =     762.17 ms /    70 tokens
0.01.432.033 I ggml_metal_free: deallocating

real	0m1.449s
user	0m0.121s
sys	0m0.191s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.239 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.953 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.958 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.959 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.960 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.960 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.960 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.961 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.962 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.962 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.962 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.963 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.963 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.963 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.964 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.965 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.965 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.966 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.886 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.033 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.995 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.996 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.997 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.997 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.997 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.998 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.998 I llama_model_loader: - type  f32:  194 tensors
0.00.024.999 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.999 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.999 I print_info: file format = GGUF V3 (latest)
0.00.025.000 I print_info: file type   = Q5_K - Medium
0.00.025.001 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.885 I load: special tokens cache size = 25
0.00.050.083 I load: token to piece cache size = 0.2984 MB
0.00.050.087 I print_info: arch             = gptneox
0.00.050.087 I print_info: vocab_only       = 0
0.00.050.087 I print_info: n_ctx_train      = 2048
0.00.050.087 I print_info: n_embd           = 2048
0.00.050.087 I print_info: n_layer          = 24
0.00.050.090 I print_info: n_head           = 16
0.00.050.091 I print_info: n_head_kv        = 16
0.00.050.091 I print_info: n_rot            = 32
0.00.050.091 I print_info: n_swa            = 0
0.00.050.092 I print_info: n_embd_head_k    = 128
0.00.050.092 I print_info: n_embd_head_v    = 128
0.00.050.093 I print_info: n_gqa            = 1
0.00.050.093 I print_info: n_embd_k_gqa     = 2048
0.00.050.094 I print_info: n_embd_v_gqa     = 2048
0.00.050.094 I print_info: f_norm_eps       = 1.0e-05
0.00.050.095 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.096 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.098 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.098 I print_info: f_logit_scale    = 0.0e+00
0.00.050.099 I print_info: n_ff             = 8192
0.00.050.099 I print_info: n_expert         = 0
0.00.050.099 I print_info: n_expert_used    = 0
0.00.050.099 I print_info: causal attn      = 1
0.00.050.101 I print_info: pooling type     = 0
0.00.050.101 I print_info: rope type        = 2
0.00.050.101 I print_info: rope scaling     = linear
0.00.050.101 I print_info: freq_base_train  = 10000.0
0.00.050.102 I print_info: freq_scale_train = 1
0.00.050.102 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.102 I print_info: rope_finetuned   = unknown
0.00.050.102 I print_info: ssm_d_conv       = 0
0.00.050.102 I print_info: ssm_d_inner      = 0
0.00.050.103 I print_info: ssm_d_state      = 0
0.00.050.103 I print_info: ssm_dt_rank      = 0
0.00.050.103 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.103 I print_info: model type       = 1.4B
0.00.050.104 I print_info: model params     = 1.41 B
0.00.050.104 I print_info: general.name     = 1.4B
0.00.050.104 I print_info: vocab type       = BPE
0.00.050.105 I print_info: n_vocab          = 50304
0.00.050.106 I print_info: n_merges         = 50009
0.00.050.106 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.106 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.107 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.107 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.107 I print_info: LF token         = 128 'Ä'
0.00.050.107 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.108 I print_info: max token length = 1024
0.00.604.184 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.201 I load_tensors: offloading output layer to GPU
0.00.604.201 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.235 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.604.237 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.605.789 I llama_init_from_model: n_seq_max     = 1
0.00.605.792 I llama_init_from_model: n_ctx         = 2048
0.00.605.792 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.605.793 I llama_init_from_model: n_batch       = 2048
0.00.605.794 I llama_init_from_model: n_ubatch      = 512
0.00.605.794 I llama_init_from_model: flash_attn    = 0
0.00.605.795 I llama_init_from_model: freq_base     = 10000.0
0.00.605.796 I llama_init_from_model: freq_scale    = 1
0.00.605.797 I ggml_metal_init: allocating
0.00.605.840 I ggml_metal_init: found device: Apple M4
0.00.605.848 I ggml_metal_init: picking default device: Apple M4
0.00.607.342 I ggml_metal_init: using embedded metal library
0.00.613.475 I ggml_metal_init: GPU name:   Apple M4
0.00.613.479 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.613.480 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.613.481 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.613.482 I ggml_metal_init: simdgroup reduction   = true
0.00.613.482 I ggml_metal_init: simdgroup matrix mul. = true
0.00.613.482 I ggml_metal_init: has residency sets    = true
0.00.613.482 I ggml_metal_init: has bfloat            = true
0.00.613.483 I ggml_metal_init: use bfloat            = true
0.00.613.483 I ggml_metal_init: hasUnifiedMemory      = true
0.00.613.485 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.630.636 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.683.882 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.683.888 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.683.956 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.688.136 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.688.138 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.688.138 I llama_init_from_model: graph nodes  = 967
0.00.688.138 I llama_init_from_model: graph splits = 2
0.00.688.144 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.688.272 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.688.273 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.668 I main: llama threadpool init, n_threads = 4
0.00.753.714 I 
0.00.753.740 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.740 I 
0.00.753.968 I sampler seed: 1234
0.00.753.972 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.983 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.984 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.984 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.602.879 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54531.49 tokens per second)
0.01.602.880 I llama_perf_context_print:        load time =     743.53 ms
0.01.602.881 I llama_perf_context_print: prompt eval time =      51.21 ms /     7 tokens (    7.32 ms per token,   136.68 tokens per second)
0.01.602.881 I llama_perf_context_print:        eval time =     794.74 ms /    63 runs   (   12.61 ms per token,    79.27 tokens per second)
0.01.602.882 I llama_perf_context_print:       total time =     850.11 ms /    70 tokens
0.01.603.133 I ggml_metal_free: deallocating

real	0m1.622s
user	0m0.120s
sys	0m0.210s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.654 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.284 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.289 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.290 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.291 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.291 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.292 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.292 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.293 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.293 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.294 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.294 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.294 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.295 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.295 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.297 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.297 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.298 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.230 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.308 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.126 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.127 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.127 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.128 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.128 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.128 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.129 I llama_model_loader: - type  f32:  194 tensors
0.00.024.129 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.130 I print_info: file format = GGUF V3 (latest)
0.00.024.130 I print_info: file type   = Q6_K
0.00.024.131 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.106 I load: special tokens cache size = 25
0.00.049.111 I load: token to piece cache size = 0.2984 MB
0.00.049.114 I print_info: arch             = gptneox
0.00.049.114 I print_info: vocab_only       = 0
0.00.049.115 I print_info: n_ctx_train      = 2048
0.00.049.115 I print_info: n_embd           = 2048
0.00.049.115 I print_info: n_layer          = 24
0.00.049.118 I print_info: n_head           = 16
0.00.049.119 I print_info: n_head_kv        = 16
0.00.049.120 I print_info: n_rot            = 32
0.00.049.120 I print_info: n_swa            = 0
0.00.049.120 I print_info: n_embd_head_k    = 128
0.00.049.120 I print_info: n_embd_head_v    = 128
0.00.049.121 I print_info: n_gqa            = 1
0.00.049.122 I print_info: n_embd_k_gqa     = 2048
0.00.049.122 I print_info: n_embd_v_gqa     = 2048
0.00.049.123 I print_info: f_norm_eps       = 1.0e-05
0.00.049.123 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.123 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.124 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.124 I print_info: f_logit_scale    = 0.0e+00
0.00.049.125 I print_info: n_ff             = 8192
0.00.049.125 I print_info: n_expert         = 0
0.00.049.125 I print_info: n_expert_used    = 0
0.00.049.125 I print_info: causal attn      = 1
0.00.049.125 I print_info: pooling type     = 0
0.00.049.125 I print_info: rope type        = 2
0.00.049.126 I print_info: rope scaling     = linear
0.00.049.126 I print_info: freq_base_train  = 10000.0
0.00.049.126 I print_info: freq_scale_train = 1
0.00.049.127 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.127 I print_info: rope_finetuned   = unknown
0.00.049.127 I print_info: ssm_d_conv       = 0
0.00.049.127 I print_info: ssm_d_inner      = 0
0.00.049.127 I print_info: ssm_d_state      = 0
0.00.049.127 I print_info: ssm_dt_rank      = 0
0.00.049.127 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.128 I print_info: model type       = 1.4B
0.00.049.128 I print_info: model params     = 1.41 B
0.00.049.128 I print_info: general.name     = 1.4B
0.00.049.129 I print_info: vocab type       = BPE
0.00.049.129 I print_info: n_vocab          = 50304
0.00.049.129 I print_info: n_merges         = 50009
0.00.049.129 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.130 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.133 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.133 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.133 I print_info: LF token         = 128 'Ä'
0.00.049.133 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.133 I print_info: max token length = 1024
0.00.650.434 I load_tensors: offloading 24 repeating layers to GPU
0.00.650.438 I load_tensors: offloading output layer to GPU
0.00.650.439 I load_tensors: offloaded 25/25 layers to GPU
0.00.650.467 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.650.469 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.651.803 I llama_init_from_model: n_seq_max     = 1
0.00.651.805 I llama_init_from_model: n_ctx         = 2048
0.00.651.805 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.651.805 I llama_init_from_model: n_batch       = 2048
0.00.651.806 I llama_init_from_model: n_ubatch      = 512
0.00.651.806 I llama_init_from_model: flash_attn    = 0
0.00.651.807 I llama_init_from_model: freq_base     = 10000.0
0.00.651.808 I llama_init_from_model: freq_scale    = 1
0.00.651.809 I ggml_metal_init: allocating
0.00.651.861 I ggml_metal_init: found device: Apple M4
0.00.651.867 I ggml_metal_init: picking default device: Apple M4
0.00.653.214 I ggml_metal_init: using embedded metal library
0.00.659.103 I ggml_metal_init: GPU name:   Apple M4
0.00.659.107 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.108 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.108 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.109 I ggml_metal_init: simdgroup reduction   = true
0.00.659.109 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.110 I ggml_metal_init: has residency sets    = true
0.00.659.110 I ggml_metal_init: has bfloat            = true
0.00.659.110 I ggml_metal_init: use bfloat            = true
0.00.659.111 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.115 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.650 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.730.647 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.730.655 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.730.677 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.735.171 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.735.173 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.735.174 I llama_init_from_model: graph nodes  = 967
0.00.735.174 I llama_init_from_model: graph splits = 2
0.00.735.181 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.735.301 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.735.302 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.803.141 I main: llama threadpool init, n_threads = 4
0.00.803.189 I 
0.00.803.212 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.213 I 
0.00.803.444 I sampler seed: 1234
0.00.803.449 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.803.460 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.803.460 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.803.462 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.686.269 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56709.27 tokens per second)
0.01.686.270 I llama_perf_context_print:        load time =     793.56 ms
0.01.686.270 I llama_perf_context_print: prompt eval time =      54.39 ms /     7 tokens (    7.77 ms per token,   128.70 tokens per second)
0.01.686.271 I llama_perf_context_print:        eval time =     825.51 ms /    63 runs   (   13.10 ms per token,    76.32 tokens per second)
0.01.686.271 I llama_perf_context_print:       total time =     884.06 ms /    70 tokens
0.01.686.500 I ggml_metal_free: deallocating

real	0m1.704s
user	0m0.120s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.529 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.106 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.871 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.885 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.889 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.890 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.890 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.891 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.891 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.894 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.894 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.895 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.896 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.896 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.897 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.898 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.903 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.904 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.904 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.394 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.824 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.492 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.495 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.496 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.496 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.496 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.497 I llama_model_loader: - type  f32:  194 tensors
0.00.055.498 I llama_model_loader: - type  f16:   98 tensors
0.00.055.499 I print_info: file format = GGUF V3 (latest)
0.00.055.500 I print_info: file type   = all F32 (guessed)
0.00.055.501 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.083.993 I load: special tokens cache size = 25
0.00.091.118 I load: token to piece cache size = 0.2984 MB
0.00.091.121 I print_info: arch             = gptneox
0.00.091.121 I print_info: vocab_only       = 0
0.00.091.121 I print_info: n_ctx_train      = 2048
0.00.091.121 I print_info: n_embd           = 2048
0.00.091.122 I print_info: n_layer          = 24
0.00.091.124 I print_info: n_head           = 16
0.00.091.125 I print_info: n_head_kv        = 16
0.00.091.125 I print_info: n_rot            = 32
0.00.091.126 I print_info: n_swa            = 0
0.00.091.126 I print_info: n_embd_head_k    = 128
0.00.091.126 I print_info: n_embd_head_v    = 128
0.00.091.127 I print_info: n_gqa            = 1
0.00.091.127 I print_info: n_embd_k_gqa     = 2048
0.00.091.128 I print_info: n_embd_v_gqa     = 2048
0.00.091.128 I print_info: f_norm_eps       = 1.0e-05
0.00.091.129 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.091.129 I print_info: f_clamp_kqv      = 0.0e+00
0.00.091.129 I print_info: f_max_alibi_bias = 0.0e+00
0.00.091.129 I print_info: f_logit_scale    = 0.0e+00
0.00.091.130 I print_info: n_ff             = 8192
0.00.091.130 I print_info: n_expert         = 0
0.00.091.130 I print_info: n_expert_used    = 0
0.00.091.130 I print_info: causal attn      = 1
0.00.091.130 I print_info: pooling type     = 0
0.00.091.131 I print_info: rope type        = 2
0.00.091.131 I print_info: rope scaling     = linear
0.00.091.131 I print_info: freq_base_train  = 10000.0
0.00.091.131 I print_info: freq_scale_train = 1
0.00.091.132 I print_info: n_ctx_orig_yarn  = 2048
0.00.091.132 I print_info: rope_finetuned   = unknown
0.00.091.132 I print_info: ssm_d_conv       = 0
0.00.091.132 I print_info: ssm_d_inner      = 0
0.00.091.132 I print_info: ssm_d_state      = 0
0.00.091.132 I print_info: ssm_dt_rank      = 0
0.00.091.132 I print_info: ssm_dt_b_c_rms   = 0
0.00.091.133 I print_info: model type       = 1.4B
0.00.091.133 I print_info: model params     = 1.41 B
0.00.091.133 I print_info: general.name     = 1.4B
0.00.091.134 I print_info: vocab type       = BPE
0.00.091.134 I print_info: n_vocab          = 50304
0.00.091.134 I print_info: n_merges         = 50009
0.00.091.134 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.091.134 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.091.135 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.091.135 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.091.135 I print_info: LF token         = 128 'Ä'
0.00.091.135 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.091.136 I print_info: max token length = 1024
0.01.326.016 I load_tensors: offloading 24 repeating layers to GPU
0.01.326.022 I load_tensors: offloading output layer to GPU
0.01.326.023 I load_tensors: offloaded 25/25 layers to GPU
0.01.326.045 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.326.046 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.326.570 I llama_init_from_model: n_seq_max     = 1
0.01.326.571 I llama_init_from_model: n_ctx         = 128
0.01.326.571 I llama_init_from_model: n_ctx_per_seq = 128
0.01.326.572 I llama_init_from_model: n_batch       = 128
0.01.326.572 I llama_init_from_model: n_ubatch      = 128
0.01.326.573 I llama_init_from_model: flash_attn    = 0
0.01.326.573 I llama_init_from_model: freq_base     = 10000.0
0.01.326.574 I llama_init_from_model: freq_scale    = 1
0.01.326.574 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.326.575 I ggml_metal_init: allocating
0.01.326.613 I ggml_metal_init: found device: Apple M4
0.01.326.617 I ggml_metal_init: picking default device: Apple M4
0.01.327.737 I ggml_metal_init: using embedded metal library
0.01.331.665 I ggml_metal_init: GPU name:   Apple M4
0.01.331.667 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.331.668 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.331.668 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.331.668 I ggml_metal_init: simdgroup reduction   = true
0.01.331.669 I ggml_metal_init: simdgroup matrix mul. = true
0.01.331.669 I ggml_metal_init: has residency sets    = true
0.01.331.669 I ggml_metal_init: has bfloat            = true
0.01.331.669 I ggml_metal_init: use bfloat            = true
0.01.331.670 I ggml_metal_init: hasUnifiedMemory      = true
0.01.331.674 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.342.191 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.343.875 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.343.878 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.343.891 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.345.530 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.345.532 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.345.532 I llama_init_from_model: graph nodes  = 967
0.01.345.533 I llama_init_from_model: graph splits = 2
0.01.345.534 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.345.534 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.379.944 I 
0.01.379.981 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.380.000 I perplexity: tokenizing the input ..
0.01.389.665 I perplexity: tokenization took 9.662 ms
0.01.389.689 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.508.818 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.510.339 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.510.354 I llama_perf_context_print:        load time =    1356.83 ms
0.01.510.354 I llama_perf_context_print: prompt eval time =     118.86 ms /   128 tokens (    0.93 ms per token,  1076.87 tokens per second)
0.01.510.355 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.510.356 I llama_perf_context_print:       total time =     130.41 ms /   129 tokens
0.01.510.711 I ggml_metal_free: deallocating

real	0m1.699s
user	0m0.117s
sys	0m0.258s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.544 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.653 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.658 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.660 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.661 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.661 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.662 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.662 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.663 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.663 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.663 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.664 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.664 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.664 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.665 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.667 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.667 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.668 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.754 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.804 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.831 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.832 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.832 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.833 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.833 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.833 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.834 I llama_model_loader: - type  f32:  194 tensors
0.00.026.834 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.835 I print_info: file format = GGUF V3 (latest)
0.00.026.835 I print_info: file type   = Q8_0
0.00.026.836 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.680 I load: special tokens cache size = 25
0.00.053.838 I load: token to piece cache size = 0.2984 MB
0.00.053.843 I print_info: arch             = gptneox
0.00.053.843 I print_info: vocab_only       = 0
0.00.053.843 I print_info: n_ctx_train      = 2048
0.00.053.843 I print_info: n_embd           = 2048
0.00.053.844 I print_info: n_layer          = 24
0.00.053.848 I print_info: n_head           = 16
0.00.053.848 I print_info: n_head_kv        = 16
0.00.053.849 I print_info: n_rot            = 32
0.00.053.849 I print_info: n_swa            = 0
0.00.053.849 I print_info: n_embd_head_k    = 128
0.00.053.849 I print_info: n_embd_head_v    = 128
0.00.053.850 I print_info: n_gqa            = 1
0.00.053.851 I print_info: n_embd_k_gqa     = 2048
0.00.053.851 I print_info: n_embd_v_gqa     = 2048
0.00.053.852 I print_info: f_norm_eps       = 1.0e-05
0.00.053.852 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.852 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.853 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.853 I print_info: f_logit_scale    = 0.0e+00
0.00.053.853 I print_info: n_ff             = 8192
0.00.053.853 I print_info: n_expert         = 0
0.00.053.854 I print_info: n_expert_used    = 0
0.00.053.854 I print_info: causal attn      = 1
0.00.053.854 I print_info: pooling type     = 0
0.00.053.854 I print_info: rope type        = 2
0.00.053.854 I print_info: rope scaling     = linear
0.00.053.855 I print_info: freq_base_train  = 10000.0
0.00.053.855 I print_info: freq_scale_train = 1
0.00.053.855 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.855 I print_info: rope_finetuned   = unknown
0.00.053.856 I print_info: ssm_d_conv       = 0
0.00.053.856 I print_info: ssm_d_inner      = 0
0.00.053.856 I print_info: ssm_d_state      = 0
0.00.053.856 I print_info: ssm_dt_rank      = 0
0.00.053.856 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.856 I print_info: model type       = 1.4B
0.00.053.857 I print_info: model params     = 1.41 B
0.00.053.857 I print_info: general.name     = 1.4B
0.00.053.857 I print_info: vocab type       = BPE
0.00.053.857 I print_info: n_vocab          = 50304
0.00.053.857 I print_info: n_merges         = 50009
0.00.053.858 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.858 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.858 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.858 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.858 I print_info: LF token         = 128 'Ä'
0.00.053.859 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.859 I print_info: max token length = 1024
0.00.799.354 I load_tensors: offloading 24 repeating layers to GPU
0.00.799.361 I load_tensors: offloading output layer to GPU
0.00.799.362 I load_tensors: offloaded 25/25 layers to GPU
0.00.799.385 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.799.386 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.800.149 I llama_init_from_model: n_seq_max     = 1
0.00.800.152 I llama_init_from_model: n_ctx         = 128
0.00.800.153 I llama_init_from_model: n_ctx_per_seq = 128
0.00.800.153 I llama_init_from_model: n_batch       = 128
0.00.800.153 I llama_init_from_model: n_ubatch      = 128
0.00.800.154 I llama_init_from_model: flash_attn    = 0
0.00.800.155 I llama_init_from_model: freq_base     = 10000.0
0.00.800.155 I llama_init_from_model: freq_scale    = 1
0.00.800.156 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.800.157 I ggml_metal_init: allocating
0.00.800.211 I ggml_metal_init: found device: Apple M4
0.00.800.218 I ggml_metal_init: picking default device: Apple M4
0.00.801.245 I ggml_metal_init: using embedded metal library
0.00.805.198 I ggml_metal_init: GPU name:   Apple M4
0.00.805.201 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.805.201 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.805.202 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.805.202 I ggml_metal_init: simdgroup reduction   = true
0.00.805.202 I ggml_metal_init: simdgroup matrix mul. = true
0.00.805.202 I ggml_metal_init: has residency sets    = true
0.00.805.202 I ggml_metal_init: has bfloat            = true
0.00.805.202 I ggml_metal_init: use bfloat            = true
0.00.805.203 I ggml_metal_init: hasUnifiedMemory      = true
0.00.805.205 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.814.674 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.816.307 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.816.312 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.816.329 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.817.957 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.817.959 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.817.959 I llama_init_from_model: graph nodes  = 967
0.00.817.959 I llama_init_from_model: graph splits = 2
0.00.817.961 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.817.961 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.841.469 I 
0.00.841.510 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.841.519 I perplexity: tokenizing the input ..
0.00.849.209 I perplexity: tokenization took 7.687 ms
0.00.849.224 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.973.026 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.974.364 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.974.377 I llama_perf_context_print:        load time =     831.92 ms
0.00.974.377 I llama_perf_context_print: prompt eval time =     123.57 ms /   128 tokens (    0.97 ms per token,  1035.83 tokens per second)
0.00.974.378 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.974.378 I llama_perf_context_print:       total time =     132.91 ms /   129 tokens
0.00.974.762 I ggml_metal_free: deallocating

real	0m0.990s
user	0m0.083s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.833 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.141 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.145 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.154 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.154 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.155 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.155 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.155 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.156 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.156 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.157 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.157 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.158 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.158 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.158 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.160 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.160 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.161 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.199 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.280 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.209 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.210 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.210 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.211 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.211 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.211 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.212 I llama_model_loader: - type  f32:  194 tensors
0.00.026.212 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.213 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.213 I print_info: file format = GGUF V3 (latest)
0.00.026.214 I print_info: file type   = Q4_0
0.00.026.214 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.083 I load: special tokens cache size = 25
0.00.051.194 I load: token to piece cache size = 0.2984 MB
0.00.051.197 I print_info: arch             = gptneox
0.00.051.197 I print_info: vocab_only       = 0
0.00.051.197 I print_info: n_ctx_train      = 2048
0.00.051.197 I print_info: n_embd           = 2048
0.00.051.198 I print_info: n_layer          = 24
0.00.051.201 I print_info: n_head           = 16
0.00.051.202 I print_info: n_head_kv        = 16
0.00.051.202 I print_info: n_rot            = 32
0.00.051.202 I print_info: n_swa            = 0
0.00.051.202 I print_info: n_embd_head_k    = 128
0.00.051.202 I print_info: n_embd_head_v    = 128
0.00.051.203 I print_info: n_gqa            = 1
0.00.051.204 I print_info: n_embd_k_gqa     = 2048
0.00.051.205 I print_info: n_embd_v_gqa     = 2048
0.00.051.205 I print_info: f_norm_eps       = 1.0e-05
0.00.051.205 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.206 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.206 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.206 I print_info: f_logit_scale    = 0.0e+00
0.00.051.207 I print_info: n_ff             = 8192
0.00.051.207 I print_info: n_expert         = 0
0.00.051.207 I print_info: n_expert_used    = 0
0.00.051.207 I print_info: causal attn      = 1
0.00.051.207 I print_info: pooling type     = 0
0.00.051.208 I print_info: rope type        = 2
0.00.051.208 I print_info: rope scaling     = linear
0.00.051.208 I print_info: freq_base_train  = 10000.0
0.00.051.209 I print_info: freq_scale_train = 1
0.00.051.211 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.212 I print_info: rope_finetuned   = unknown
0.00.051.212 I print_info: ssm_d_conv       = 0
0.00.051.212 I print_info: ssm_d_inner      = 0
0.00.051.212 I print_info: ssm_d_state      = 0
0.00.051.212 I print_info: ssm_dt_rank      = 0
0.00.051.212 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.213 I print_info: model type       = 1.4B
0.00.051.213 I print_info: model params     = 1.41 B
0.00.051.213 I print_info: general.name     = 1.4B
0.00.051.214 I print_info: vocab type       = BPE
0.00.051.214 I print_info: n_vocab          = 50304
0.00.051.214 I print_info: n_merges         = 50009
0.00.051.214 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.214 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.215 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.215 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.215 I print_info: LF token         = 128 'Ä'
0.00.051.216 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.216 I print_info: max token length = 1024
0.00.619.065 I load_tensors: offloading 24 repeating layers to GPU
0.00.619.083 I load_tensors: offloading output layer to GPU
0.00.619.084 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.118 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.619.119 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.620.732 I llama_init_from_model: n_seq_max     = 1
0.00.620.737 I llama_init_from_model: n_ctx         = 128
0.00.620.737 I llama_init_from_model: n_ctx_per_seq = 128
0.00.620.738 I llama_init_from_model: n_batch       = 128
0.00.620.738 I llama_init_from_model: n_ubatch      = 128
0.00.620.738 I llama_init_from_model: flash_attn    = 0
0.00.620.740 I llama_init_from_model: freq_base     = 10000.0
0.00.620.741 I llama_init_from_model: freq_scale    = 1
0.00.620.741 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.620.743 I ggml_metal_init: allocating
0.00.620.823 I ggml_metal_init: found device: Apple M4
0.00.620.832 I ggml_metal_init: picking default device: Apple M4
0.00.622.549 I ggml_metal_init: using embedded metal library
0.00.629.093 I ggml_metal_init: GPU name:   Apple M4
0.00.629.099 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.629.100 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.629.101 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.629.101 I ggml_metal_init: simdgroup reduction   = true
0.00.629.102 I ggml_metal_init: simdgroup matrix mul. = true
0.00.629.102 I ggml_metal_init: has residency sets    = true
0.00.629.102 I ggml_metal_init: has bfloat            = true
0.00.629.102 I ggml_metal_init: use bfloat            = true
0.00.629.104 I ggml_metal_init: hasUnifiedMemory      = true
0.00.629.115 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.647.780 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.651.202 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.651.208 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.651.244 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.654.324 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.654.325 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.654.326 I llama_init_from_model: graph nodes  = 967
0.00.654.326 I llama_init_from_model: graph splits = 2
0.00.654.330 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.654.330 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.682.387 I 
0.00.682.462 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.682.483 I perplexity: tokenizing the input ..
0.00.694.895 I perplexity: tokenization took 12.411 ms
0.00.694.908 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.825.989 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.827.326 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.827.341 I llama_perf_context_print:        load time =     672.54 ms
0.00.827.342 I llama_perf_context_print: prompt eval time =     130.85 ms /   128 tokens (    1.02 ms per token,   978.23 tokens per second)
0.00.827.343 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.827.343 I llama_perf_context_print:       total time =     144.96 ms /   129 tokens
0.00.827.792 I ggml_metal_free: deallocating

real	0m0.844s
user	0m0.096s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.944 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.139 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.144 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.151 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.151 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.151 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.152 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.152 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.153 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.153 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.154 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.154 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.155 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.155 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.155 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.157 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.157 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.158 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.282 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.321 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.348 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.350 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.350 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.350 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.350 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.351 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.351 I llama_model_loader: - type  f32:  194 tensors
0.00.025.352 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.352 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.352 I print_info: file format = GGUF V3 (latest)
0.00.025.353 I print_info: file type   = Q4_1
0.00.025.354 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.801 I load: special tokens cache size = 25
0.00.050.858 I load: token to piece cache size = 0.2984 MB
0.00.050.861 I print_info: arch             = gptneox
0.00.050.861 I print_info: vocab_only       = 0
0.00.050.861 I print_info: n_ctx_train      = 2048
0.00.050.861 I print_info: n_embd           = 2048
0.00.050.861 I print_info: n_layer          = 24
0.00.050.865 I print_info: n_head           = 16
0.00.050.865 I print_info: n_head_kv        = 16
0.00.050.866 I print_info: n_rot            = 32
0.00.050.866 I print_info: n_swa            = 0
0.00.050.866 I print_info: n_embd_head_k    = 128
0.00.050.866 I print_info: n_embd_head_v    = 128
0.00.050.869 I print_info: n_gqa            = 1
0.00.050.870 I print_info: n_embd_k_gqa     = 2048
0.00.050.871 I print_info: n_embd_v_gqa     = 2048
0.00.050.872 I print_info: f_norm_eps       = 1.0e-05
0.00.050.872 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.873 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.873 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.873 I print_info: f_logit_scale    = 0.0e+00
0.00.050.874 I print_info: n_ff             = 8192
0.00.050.874 I print_info: n_expert         = 0
0.00.050.874 I print_info: n_expert_used    = 0
0.00.050.874 I print_info: causal attn      = 1
0.00.050.876 I print_info: pooling type     = 0
0.00.050.876 I print_info: rope type        = 2
0.00.050.876 I print_info: rope scaling     = linear
0.00.050.876 I print_info: freq_base_train  = 10000.0
0.00.050.877 I print_info: freq_scale_train = 1
0.00.050.877 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.877 I print_info: rope_finetuned   = unknown
0.00.050.877 I print_info: ssm_d_conv       = 0
0.00.050.877 I print_info: ssm_d_inner      = 0
0.00.050.877 I print_info: ssm_d_state      = 0
0.00.050.878 I print_info: ssm_dt_rank      = 0
0.00.050.878 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.878 I print_info: model type       = 1.4B
0.00.050.878 I print_info: model params     = 1.41 B
0.00.050.882 I print_info: general.name     = 1.4B
0.00.050.883 I print_info: vocab type       = BPE
0.00.050.883 I print_info: n_vocab          = 50304
0.00.050.883 I print_info: n_merges         = 50009
0.00.050.883 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.883 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.884 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.884 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.885 I print_info: LF token         = 128 'Ä'
0.00.050.885 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.885 I print_info: max token length = 1024
0.00.642.975 I load_tensors: offloading 24 repeating layers to GPU
0.00.642.989 I load_tensors: offloading output layer to GPU
0.00.642.989 I load_tensors: offloaded 25/25 layers to GPU
0.00.643.023 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.643.024 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.644.420 I llama_init_from_model: n_seq_max     = 1
0.00.644.426 I llama_init_from_model: n_ctx         = 128
0.00.644.426 I llama_init_from_model: n_ctx_per_seq = 128
0.00.644.427 I llama_init_from_model: n_batch       = 128
0.00.644.427 I llama_init_from_model: n_ubatch      = 128
0.00.644.428 I llama_init_from_model: flash_attn    = 0
0.00.644.429 I llama_init_from_model: freq_base     = 10000.0
0.00.644.430 I llama_init_from_model: freq_scale    = 1
0.00.644.430 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.644.433 I ggml_metal_init: allocating
0.00.644.508 I ggml_metal_init: found device: Apple M4
0.00.644.517 I ggml_metal_init: picking default device: Apple M4
0.00.646.224 I ggml_metal_init: using embedded metal library
0.00.653.064 I ggml_metal_init: GPU name:   Apple M4
0.00.653.069 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.653.070 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.653.071 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.653.071 I ggml_metal_init: simdgroup reduction   = true
0.00.653.072 I ggml_metal_init: simdgroup matrix mul. = true
0.00.653.072 I ggml_metal_init: has residency sets    = true
0.00.653.072 I ggml_metal_init: has bfloat            = true
0.00.653.072 I ggml_metal_init: use bfloat            = true
0.00.653.073 I ggml_metal_init: hasUnifiedMemory      = true
0.00.653.075 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.670.797 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.674.320 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.674.327 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.674.378 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.677.874 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.677.876 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.677.876 I llama_init_from_model: graph nodes  = 967
0.00.677.877 I llama_init_from_model: graph splits = 2
0.00.677.880 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.677.883 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.135 I 
0.00.705.221 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.242 I perplexity: tokenizing the input ..
0.00.717.601 I perplexity: tokenization took 12.357 ms
0.00.717.616 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.845.478 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.846.812 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.846.826 I llama_perf_context_print:        load time =     696.18 ms
0.00.846.827 I llama_perf_context_print: prompt eval time =     127.63 ms /   128 tokens (    1.00 ms per token,  1002.88 tokens per second)
0.00.846.828 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.846.829 I llama_perf_context_print:       total time =     141.70 ms /   129 tokens
0.00.847.212 I ggml_metal_free: deallocating

real	0m0.861s
user	0m0.096s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.913 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.784 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.788 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.792 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.792 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.793 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.793 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.793 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.794 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.795 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.795 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.795 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.796 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.796 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.797 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.798 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.799 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.799 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.792 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.839 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.776 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.778 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.778 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.778 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.779 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.779 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.780 I llama_model_loader: - type  f32:  194 tensors
0.00.024.780 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.780 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.781 I print_info: file format = GGUF V3 (latest)
0.00.024.781 I print_info: file type   = Q5_0
0.00.024.782 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.505 I load: special tokens cache size = 25
0.00.049.656 I load: token to piece cache size = 0.2984 MB
0.00.049.659 I print_info: arch             = gptneox
0.00.049.659 I print_info: vocab_only       = 0
0.00.049.659 I print_info: n_ctx_train      = 2048
0.00.049.660 I print_info: n_embd           = 2048
0.00.049.660 I print_info: n_layer          = 24
0.00.049.663 I print_info: n_head           = 16
0.00.049.664 I print_info: n_head_kv        = 16
0.00.049.664 I print_info: n_rot            = 32
0.00.049.664 I print_info: n_swa            = 0
0.00.049.664 I print_info: n_embd_head_k    = 128
0.00.049.664 I print_info: n_embd_head_v    = 128
0.00.049.665 I print_info: n_gqa            = 1
0.00.049.666 I print_info: n_embd_k_gqa     = 2048
0.00.049.667 I print_info: n_embd_v_gqa     = 2048
0.00.049.667 I print_info: f_norm_eps       = 1.0e-05
0.00.049.668 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.668 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.668 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.668 I print_info: f_logit_scale    = 0.0e+00
0.00.049.669 I print_info: n_ff             = 8192
0.00.049.669 I print_info: n_expert         = 0
0.00.049.669 I print_info: n_expert_used    = 0
0.00.049.669 I print_info: causal attn      = 1
0.00.049.669 I print_info: pooling type     = 0
0.00.049.670 I print_info: rope type        = 2
0.00.049.670 I print_info: rope scaling     = linear
0.00.049.670 I print_info: freq_base_train  = 10000.0
0.00.049.671 I print_info: freq_scale_train = 1
0.00.049.671 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.671 I print_info: rope_finetuned   = unknown
0.00.049.671 I print_info: ssm_d_conv       = 0
0.00.049.671 I print_info: ssm_d_inner      = 0
0.00.049.671 I print_info: ssm_d_state      = 0
0.00.049.671 I print_info: ssm_dt_rank      = 0
0.00.049.674 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.674 I print_info: model type       = 1.4B
0.00.049.674 I print_info: model params     = 1.41 B
0.00.049.674 I print_info: general.name     = 1.4B
0.00.049.675 I print_info: vocab type       = BPE
0.00.049.675 I print_info: n_vocab          = 50304
0.00.049.675 I print_info: n_merges         = 50009
0.00.049.676 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.676 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.680 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.680 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.681 I print_info: LF token         = 128 'Ä'
0.00.049.681 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.681 I print_info: max token length = 1024
0.00.665.121 I load_tensors: offloading 24 repeating layers to GPU
0.00.665.135 I load_tensors: offloading output layer to GPU
0.00.665.136 I load_tensors: offloaded 25/25 layers to GPU
0.00.665.166 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.665.167 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.666.617 I llama_init_from_model: n_seq_max     = 1
0.00.666.624 I llama_init_from_model: n_ctx         = 128
0.00.666.624 I llama_init_from_model: n_ctx_per_seq = 128
0.00.666.624 I llama_init_from_model: n_batch       = 128
0.00.666.625 I llama_init_from_model: n_ubatch      = 128
0.00.666.625 I llama_init_from_model: flash_attn    = 0
0.00.666.626 I llama_init_from_model: freq_base     = 10000.0
0.00.666.627 I llama_init_from_model: freq_scale    = 1
0.00.666.627 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.666.632 I ggml_metal_init: allocating
0.00.666.682 I ggml_metal_init: found device: Apple M4
0.00.666.690 I ggml_metal_init: picking default device: Apple M4
0.00.668.300 I ggml_metal_init: using embedded metal library
0.00.674.958 I ggml_metal_init: GPU name:   Apple M4
0.00.674.965 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.674.966 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.674.967 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.674.967 I ggml_metal_init: simdgroup reduction   = true
0.00.674.968 I ggml_metal_init: simdgroup matrix mul. = true
0.00.674.968 I ggml_metal_init: has residency sets    = true
0.00.674.968 I ggml_metal_init: has bfloat            = true
0.00.674.969 I ggml_metal_init: use bfloat            = true
0.00.674.970 I ggml_metal_init: hasUnifiedMemory      = true
0.00.674.972 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.693.439 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.697.090 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.697.096 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.697.142 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.700.522 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.700.524 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.700.525 I llama_init_from_model: graph nodes  = 967
0.00.700.525 I llama_init_from_model: graph splits = 2
0.00.700.527 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.700.528 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.786 I 
0.00.730.877 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.900 I perplexity: tokenizing the input ..
0.00.740.799 I perplexity: tokenization took 9.897 ms
0.00.740.813 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.877.149 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.878.482 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.878.496 I llama_perf_context_print:        load time =     721.87 ms
0.00.878.497 I llama_perf_context_print: prompt eval time =     136.10 ms /   128 tokens (    1.06 ms per token,   940.47 tokens per second)
0.00.878.498 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.878.498 I llama_perf_context_print:       total time =     147.71 ms /   129 tokens
0.00.878.906 I ggml_metal_free: deallocating

real	0m0.894s
user	0m0.093s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.061 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.912 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.917 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.924 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.925 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.925 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.925 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.926 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.927 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.927 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.927 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.928 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.928 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.928 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.929 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.930 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.931 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.931 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.991 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.042 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.062 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.063 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.063 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.064 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.064 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.064 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.065 I llama_model_loader: - type  f32:  194 tensors
0.00.026.065 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.065 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.066 I print_info: file format = GGUF V3 (latest)
0.00.026.067 I print_info: file type   = Q5_1
0.00.026.067 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.841 I load: special tokens cache size = 25
0.00.050.873 I load: token to piece cache size = 0.2984 MB
0.00.050.876 I print_info: arch             = gptneox
0.00.050.876 I print_info: vocab_only       = 0
0.00.050.877 I print_info: n_ctx_train      = 2048
0.00.050.877 I print_info: n_embd           = 2048
0.00.050.877 I print_info: n_layer          = 24
0.00.050.880 I print_info: n_head           = 16
0.00.050.881 I print_info: n_head_kv        = 16
0.00.050.881 I print_info: n_rot            = 32
0.00.050.881 I print_info: n_swa            = 0
0.00.050.881 I print_info: n_embd_head_k    = 128
0.00.050.881 I print_info: n_embd_head_v    = 128
0.00.050.882 I print_info: n_gqa            = 1
0.00.050.883 I print_info: n_embd_k_gqa     = 2048
0.00.050.883 I print_info: n_embd_v_gqa     = 2048
0.00.050.884 I print_info: f_norm_eps       = 1.0e-05
0.00.050.884 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.884 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.885 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.885 I print_info: f_logit_scale    = 0.0e+00
0.00.050.885 I print_info: n_ff             = 8192
0.00.050.886 I print_info: n_expert         = 0
0.00.050.886 I print_info: n_expert_used    = 0
0.00.050.886 I print_info: causal attn      = 1
0.00.050.886 I print_info: pooling type     = 0
0.00.050.886 I print_info: rope type        = 2
0.00.050.887 I print_info: rope scaling     = linear
0.00.050.887 I print_info: freq_base_train  = 10000.0
0.00.050.887 I print_info: freq_scale_train = 1
0.00.050.887 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.888 I print_info: rope_finetuned   = unknown
0.00.050.888 I print_info: ssm_d_conv       = 0
0.00.050.888 I print_info: ssm_d_inner      = 0
0.00.050.888 I print_info: ssm_d_state      = 0
0.00.050.888 I print_info: ssm_dt_rank      = 0
0.00.050.888 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.890 I print_info: model type       = 1.4B
0.00.050.891 I print_info: model params     = 1.41 B
0.00.050.891 I print_info: general.name     = 1.4B
0.00.050.891 I print_info: vocab type       = BPE
0.00.050.892 I print_info: n_vocab          = 50304
0.00.050.892 I print_info: n_merges         = 50009
0.00.050.892 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.892 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.892 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.893 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.893 I print_info: LF token         = 128 'Ä'
0.00.050.894 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.894 I print_info: max token length = 1024
0.00.682.922 I load_tensors: offloading 24 repeating layers to GPU
0.00.682.939 I load_tensors: offloading output layer to GPU
0.00.682.940 I load_tensors: offloaded 25/25 layers to GPU
0.00.682.976 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.682.977 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.684.565 I llama_init_from_model: n_seq_max     = 1
0.00.684.568 I llama_init_from_model: n_ctx         = 128
0.00.684.569 I llama_init_from_model: n_ctx_per_seq = 128
0.00.684.569 I llama_init_from_model: n_batch       = 128
0.00.684.570 I llama_init_from_model: n_ubatch      = 128
0.00.684.570 I llama_init_from_model: flash_attn    = 0
0.00.684.571 I llama_init_from_model: freq_base     = 10000.0
0.00.684.571 I llama_init_from_model: freq_scale    = 1
0.00.684.572 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.684.577 I ggml_metal_init: allocating
0.00.684.628 I ggml_metal_init: found device: Apple M4
0.00.684.636 I ggml_metal_init: picking default device: Apple M4
0.00.686.107 I ggml_metal_init: using embedded metal library
0.00.692.450 I ggml_metal_init: GPU name:   Apple M4
0.00.692.454 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.692.455 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.692.456 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.692.457 I ggml_metal_init: simdgroup reduction   = true
0.00.692.457 I ggml_metal_init: simdgroup matrix mul. = true
0.00.692.457 I ggml_metal_init: has residency sets    = true
0.00.692.457 I ggml_metal_init: has bfloat            = true
0.00.692.458 I ggml_metal_init: use bfloat            = true
0.00.692.458 I ggml_metal_init: hasUnifiedMemory      = true
0.00.692.463 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.709.171 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.712.518 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.712.521 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.712.570 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.715.762 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.715.764 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.715.764 I llama_init_from_model: graph nodes  = 967
0.00.715.765 I llama_init_from_model: graph splits = 2
0.00.715.767 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.715.767 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.193 I 
0.00.747.281 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.300 I perplexity: tokenizing the input ..
0.00.758.744 I perplexity: tokenization took 11.442 ms
0.00.758.757 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.902.832 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.904.158 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.904.177 I llama_perf_context_print:        load time =     737.12 ms
0.00.904.178 I llama_perf_context_print: prompt eval time =     143.84 ms /   128 tokens (    1.12 ms per token,   889.85 tokens per second)
0.00.904.181 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.904.182 I llama_perf_context_print:       total time =     156.99 ms /   129 tokens
0.00.904.574 I ggml_metal_free: deallocating

real	0m0.921s
user	0m0.094s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.968 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.947 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.954 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.955 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.957 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.957 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.957 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.958 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.959 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.959 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.963 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.963 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.963 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.964 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.964 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.966 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.966 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.966 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.951 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.988 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.839 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.840 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.841 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.841 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.841 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.842 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.842 I llama_model_loader: - type  f32:  194 tensors
0.00.024.843 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.843 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.843 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.844 I print_info: file format = GGUF V3 (latest)
0.00.024.844 I print_info: file type   = Q2_K - Medium
0.00.024.845 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.381 I load: special tokens cache size = 25
0.00.050.405 I load: token to piece cache size = 0.2984 MB
0.00.050.408 I print_info: arch             = gptneox
0.00.050.409 I print_info: vocab_only       = 0
0.00.050.409 I print_info: n_ctx_train      = 2048
0.00.050.409 I print_info: n_embd           = 2048
0.00.050.409 I print_info: n_layer          = 24
0.00.050.412 I print_info: n_head           = 16
0.00.050.413 I print_info: n_head_kv        = 16
0.00.050.413 I print_info: n_rot            = 32
0.00.050.414 I print_info: n_swa            = 0
0.00.050.414 I print_info: n_embd_head_k    = 128
0.00.050.416 I print_info: n_embd_head_v    = 128
0.00.050.417 I print_info: n_gqa            = 1
0.00.050.418 I print_info: n_embd_k_gqa     = 2048
0.00.050.418 I print_info: n_embd_v_gqa     = 2048
0.00.050.419 I print_info: f_norm_eps       = 1.0e-05
0.00.050.421 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.421 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.421 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.421 I print_info: f_logit_scale    = 0.0e+00
0.00.050.422 I print_info: n_ff             = 8192
0.00.050.422 I print_info: n_expert         = 0
0.00.050.422 I print_info: n_expert_used    = 0
0.00.050.422 I print_info: causal attn      = 1
0.00.050.422 I print_info: pooling type     = 0
0.00.050.422 I print_info: rope type        = 2
0.00.050.423 I print_info: rope scaling     = linear
0.00.050.423 I print_info: freq_base_train  = 10000.0
0.00.050.423 I print_info: freq_scale_train = 1
0.00.050.423 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.424 I print_info: rope_finetuned   = unknown
0.00.050.424 I print_info: ssm_d_conv       = 0
0.00.050.424 I print_info: ssm_d_inner      = 0
0.00.050.424 I print_info: ssm_d_state      = 0
0.00.050.428 I print_info: ssm_dt_rank      = 0
0.00.050.428 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.429 I print_info: model type       = 1.4B
0.00.050.429 I print_info: model params     = 1.41 B
0.00.050.429 I print_info: general.name     = 1.4B
0.00.050.431 I print_info: vocab type       = BPE
0.00.050.431 I print_info: n_vocab          = 50304
0.00.050.431 I print_info: n_merges         = 50009
0.00.050.431 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.432 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.432 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.432 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.432 I print_info: LF token         = 128 'Ä'
0.00.050.432 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.432 I print_info: max token length = 1024
0.00.381.244 I load_tensors: offloading 24 repeating layers to GPU
0.00.381.258 I load_tensors: offloading output layer to GPU
0.00.381.258 I load_tensors: offloaded 25/25 layers to GPU
0.00.381.294 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.381.295 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.382.761 I llama_init_from_model: n_seq_max     = 1
0.00.382.766 I llama_init_from_model: n_ctx         = 128
0.00.382.767 I llama_init_from_model: n_ctx_per_seq = 128
0.00.382.768 I llama_init_from_model: n_batch       = 128
0.00.382.768 I llama_init_from_model: n_ubatch      = 128
0.00.382.768 I llama_init_from_model: flash_attn    = 0
0.00.382.771 I llama_init_from_model: freq_base     = 10000.0
0.00.382.771 I llama_init_from_model: freq_scale    = 1
0.00.382.772 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.382.780 I ggml_metal_init: allocating
0.00.382.858 I ggml_metal_init: found device: Apple M4
0.00.382.867 I ggml_metal_init: picking default device: Apple M4
0.00.384.628 I ggml_metal_init: using embedded metal library
0.00.390.358 I ggml_metal_init: GPU name:   Apple M4
0.00.390.370 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.390.370 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.390.372 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.390.372 I ggml_metal_init: simdgroup reduction   = true
0.00.390.372 I ggml_metal_init: simdgroup matrix mul. = true
0.00.390.373 I ggml_metal_init: has residency sets    = true
0.00.390.373 I ggml_metal_init: has bfloat            = true
0.00.390.373 I ggml_metal_init: use bfloat            = true
0.00.390.375 I ggml_metal_init: hasUnifiedMemory      = true
0.00.390.380 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.410.985 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.414.566 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.414.569 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.414.608 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.417.910 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.417.912 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.417.912 I llama_init_from_model: graph nodes  = 967
0.00.417.913 I llama_init_from_model: graph splits = 2
0.00.417.916 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.417.917 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.448.430 I 
0.00.448.521 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.448.540 I perplexity: tokenizing the input ..
0.00.459.659 I perplexity: tokenization took 11.118 ms
0.00.459.671 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.596.415 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.597.837 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.597.853 I llama_perf_context_print:        load time =     439.46 ms
0.00.597.854 I llama_perf_context_print: prompt eval time =     136.51 ms /   128 tokens (    1.07 ms per token,   937.65 tokens per second)
0.00.597.855 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.597.855 I llama_perf_context_print:       total time =     149.42 ms /   129 tokens
0.00.598.237 I ggml_metal_free: deallocating

real	0m0.612s
user	0m0.095s
sys	0m0.089s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.881 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.862 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.868 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.874 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.874 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.876 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.876 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.877 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.878 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.878 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.878 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.879 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.879 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.880 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.880 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.881 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.882 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.888 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.803 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.937 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.955 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.957 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.957 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.958 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.958 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.958 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.959 I llama_model_loader: - type  f32:  194 tensors
0.00.024.959 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.959 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.960 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.960 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.960 I print_info: file format = GGUF V3 (latest)
0.00.024.965 I print_info: file type   = Q3_K - Medium
0.00.024.965 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.393 I load: special tokens cache size = 25
0.00.050.530 I load: token to piece cache size = 0.2984 MB
0.00.050.534 I print_info: arch             = gptneox
0.00.050.534 I print_info: vocab_only       = 0
0.00.050.534 I print_info: n_ctx_train      = 2048
0.00.050.534 I print_info: n_embd           = 2048
0.00.050.535 I print_info: n_layer          = 24
0.00.050.538 I print_info: n_head           = 16
0.00.050.538 I print_info: n_head_kv        = 16
0.00.050.538 I print_info: n_rot            = 32
0.00.050.539 I print_info: n_swa            = 0
0.00.050.539 I print_info: n_embd_head_k    = 128
0.00.050.539 I print_info: n_embd_head_v    = 128
0.00.050.540 I print_info: n_gqa            = 1
0.00.050.541 I print_info: n_embd_k_gqa     = 2048
0.00.050.541 I print_info: n_embd_v_gqa     = 2048
0.00.050.542 I print_info: f_norm_eps       = 1.0e-05
0.00.050.542 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.542 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.543 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.543 I print_info: f_logit_scale    = 0.0e+00
0.00.050.543 I print_info: n_ff             = 8192
0.00.050.544 I print_info: n_expert         = 0
0.00.050.544 I print_info: n_expert_used    = 0
0.00.050.544 I print_info: causal attn      = 1
0.00.050.544 I print_info: pooling type     = 0
0.00.050.544 I print_info: rope type        = 2
0.00.050.544 I print_info: rope scaling     = linear
0.00.050.545 I print_info: freq_base_train  = 10000.0
0.00.050.545 I print_info: freq_scale_train = 1
0.00.050.545 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.546 I print_info: rope_finetuned   = unknown
0.00.050.546 I print_info: ssm_d_conv       = 0
0.00.050.546 I print_info: ssm_d_inner      = 0
0.00.050.546 I print_info: ssm_d_state      = 0
0.00.050.548 I print_info: ssm_dt_rank      = 0
0.00.050.548 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.548 I print_info: model type       = 1.4B
0.00.050.549 I print_info: model params     = 1.41 B
0.00.050.549 I print_info: general.name     = 1.4B
0.00.050.549 I print_info: vocab type       = BPE
0.00.050.550 I print_info: n_vocab          = 50304
0.00.050.550 I print_info: n_merges         = 50009
0.00.050.550 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.550 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.550 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.550 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.551 I print_info: LF token         = 128 'Ä'
0.00.050.551 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.551 I print_info: max token length = 1024
0.00.457.547 I load_tensors: offloading 24 repeating layers to GPU
0.00.457.563 I load_tensors: offloading output layer to GPU
0.00.457.564 I load_tensors: offloaded 25/25 layers to GPU
0.00.457.598 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.457.600 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.458.936 I llama_init_from_model: n_seq_max     = 1
0.00.458.940 I llama_init_from_model: n_ctx         = 128
0.00.458.941 I llama_init_from_model: n_ctx_per_seq = 128
0.00.458.941 I llama_init_from_model: n_batch       = 128
0.00.458.941 I llama_init_from_model: n_ubatch      = 128
0.00.458.942 I llama_init_from_model: flash_attn    = 0
0.00.458.944 I llama_init_from_model: freq_base     = 10000.0
0.00.458.944 I llama_init_from_model: freq_scale    = 1
0.00.458.945 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.458.947 I ggml_metal_init: allocating
0.00.459.026 I ggml_metal_init: found device: Apple M4
0.00.459.035 I ggml_metal_init: picking default device: Apple M4
0.00.460.742 I ggml_metal_init: using embedded metal library
0.00.466.339 I ggml_metal_init: GPU name:   Apple M4
0.00.466.344 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.466.345 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.466.346 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.466.347 I ggml_metal_init: simdgroup reduction   = true
0.00.466.347 I ggml_metal_init: simdgroup matrix mul. = true
0.00.466.348 I ggml_metal_init: has residency sets    = true
0.00.466.348 I ggml_metal_init: has bfloat            = true
0.00.466.348 I ggml_metal_init: use bfloat            = true
0.00.466.349 I ggml_metal_init: hasUnifiedMemory      = true
0.00.466.359 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.485.963 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.489.433 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.489.443 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.489.474 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.492.793 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.492.795 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.492.796 I llama_init_from_model: graph nodes  = 967
0.00.492.796 I llama_init_from_model: graph splits = 2
0.00.492.799 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.492.799 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.520.046 I 
0.00.520.131 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.520.152 I perplexity: tokenizing the input ..
0.00.530.016 I perplexity: tokenization took 9.861 ms
0.00.530.029 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.661.146 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.662.459 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.662.471 I llama_perf_context_print:        load time =     511.16 ms
0.00.662.472 I llama_perf_context_print: prompt eval time =     130.89 ms /   128 tokens (    1.02 ms per token,   977.95 tokens per second)
0.00.662.473 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.662.473 I llama_perf_context_print:       total time =     142.43 ms /   129 tokens
0.00.662.868 I ggml_metal_free: deallocating

real	0m0.677s
user	0m0.094s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.066 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.715 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.720 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.722 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.723 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.723 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.723 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.724 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.725 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.725 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.725 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.726 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.726 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.726 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.727 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.730 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.730 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.730 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.682 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.725 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.584 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.586 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.586 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.586 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.586 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.587 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.587 I llama_model_loader: - type  f32:  194 tensors
0.00.025.588 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.588 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.588 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.589 I print_info: file format = GGUF V3 (latest)
0.00.025.589 I print_info: file type   = Q4_K - Medium
0.00.025.590 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.427 I load: special tokens cache size = 25
0.00.050.409 I load: token to piece cache size = 0.2984 MB
0.00.050.411 I print_info: arch             = gptneox
0.00.050.412 I print_info: vocab_only       = 0
0.00.050.412 I print_info: n_ctx_train      = 2048
0.00.050.412 I print_info: n_embd           = 2048
0.00.050.412 I print_info: n_layer          = 24
0.00.050.415 I print_info: n_head           = 16
0.00.050.416 I print_info: n_head_kv        = 16
0.00.050.416 I print_info: n_rot            = 32
0.00.050.417 I print_info: n_swa            = 0
0.00.050.418 I print_info: n_embd_head_k    = 128
0.00.050.418 I print_info: n_embd_head_v    = 128
0.00.050.419 I print_info: n_gqa            = 1
0.00.050.419 I print_info: n_embd_k_gqa     = 2048
0.00.050.420 I print_info: n_embd_v_gqa     = 2048
0.00.050.421 I print_info: f_norm_eps       = 1.0e-05
0.00.050.421 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.421 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.421 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.421 I print_info: f_logit_scale    = 0.0e+00
0.00.050.422 I print_info: n_ff             = 8192
0.00.050.422 I print_info: n_expert         = 0
0.00.050.423 I print_info: n_expert_used    = 0
0.00.050.423 I print_info: causal attn      = 1
0.00.050.423 I print_info: pooling type     = 0
0.00.050.423 I print_info: rope type        = 2
0.00.050.423 I print_info: rope scaling     = linear
0.00.050.424 I print_info: freq_base_train  = 10000.0
0.00.050.424 I print_info: freq_scale_train = 1
0.00.050.424 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.426 I print_info: rope_finetuned   = unknown
0.00.050.426 I print_info: ssm_d_conv       = 0
0.00.050.426 I print_info: ssm_d_inner      = 0
0.00.050.427 I print_info: ssm_d_state      = 0
0.00.050.427 I print_info: ssm_dt_rank      = 0
0.00.050.427 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.427 I print_info: model type       = 1.4B
0.00.050.427 I print_info: model params     = 1.41 B
0.00.050.428 I print_info: general.name     = 1.4B
0.00.050.428 I print_info: vocab type       = BPE
0.00.050.428 I print_info: n_vocab          = 50304
0.00.050.428 I print_info: n_merges         = 50009
0.00.050.429 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.429 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.429 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.429 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.429 I print_info: LF token         = 128 'Ä'
0.00.050.430 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.431 I print_info: max token length = 1024
0.00.524.121 I load_tensors: offloading 24 repeating layers to GPU
0.00.524.134 I load_tensors: offloading output layer to GPU
0.00.524.135 I load_tensors: offloaded 25/25 layers to GPU
0.00.524.166 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.524.167 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.525.488 I llama_init_from_model: n_seq_max     = 1
0.00.525.495 I llama_init_from_model: n_ctx         = 128
0.00.525.496 I llama_init_from_model: n_ctx_per_seq = 128
0.00.525.496 I llama_init_from_model: n_batch       = 128
0.00.525.497 I llama_init_from_model: n_ubatch      = 128
0.00.525.497 I llama_init_from_model: flash_attn    = 0
0.00.525.500 I llama_init_from_model: freq_base     = 10000.0
0.00.525.500 I llama_init_from_model: freq_scale    = 1
0.00.525.501 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.525.504 I ggml_metal_init: allocating
0.00.525.580 I ggml_metal_init: found device: Apple M4
0.00.525.589 I ggml_metal_init: picking default device: Apple M4
0.00.527.776 I ggml_metal_init: using embedded metal library
0.00.533.509 I ggml_metal_init: GPU name:   Apple M4
0.00.533.535 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.533.535 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.533.536 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.533.537 I ggml_metal_init: simdgroup reduction   = true
0.00.533.537 I ggml_metal_init: simdgroup matrix mul. = true
0.00.533.538 I ggml_metal_init: has residency sets    = true
0.00.533.538 I ggml_metal_init: has bfloat            = true
0.00.533.538 I ggml_metal_init: use bfloat            = true
0.00.533.541 I ggml_metal_init: hasUnifiedMemory      = true
0.00.533.546 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.554.727 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.558.231 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.558.238 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.558.279 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.561.596 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.561.598 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.561.598 I llama_init_from_model: graph nodes  = 967
0.00.561.598 I llama_init_from_model: graph splits = 2
0.00.561.602 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.561.602 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.588.704 I 
0.00.588.773 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.588.791 I perplexity: tokenizing the input ..
0.00.598.020 I perplexity: tokenization took 9.227 ms
0.00.598.035 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.731.302 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.732.643 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.732.658 I llama_perf_context_print:        load time =     578.63 ms
0.00.732.659 I llama_perf_context_print: prompt eval time =     133.04 ms /   128 tokens (    1.04 ms per token,   962.12 tokens per second)
0.00.732.660 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.732.660 I llama_perf_context_print:       total time =     143.96 ms /   129 tokens
0.00.733.052 I ggml_metal_free: deallocating

real	0m0.749s
user	0m0.093s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.927 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.696 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.701 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.708 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.709 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.709 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.711 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.711 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.712 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.713 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.713 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.713 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.714 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.717 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.718 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.720 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.720 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.720 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.529 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.521 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.324 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.324 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.325 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.325 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.325 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.326 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.326 I llama_model_loader: - type  f32:  194 tensors
0.00.024.327 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.327 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.327 I print_info: file format = GGUF V3 (latest)
0.00.024.328 I print_info: file type   = Q5_K - Medium
0.00.024.329 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.051 I load: special tokens cache size = 25
0.00.049.163 I load: token to piece cache size = 0.2984 MB
0.00.049.166 I print_info: arch             = gptneox
0.00.049.166 I print_info: vocab_only       = 0
0.00.049.167 I print_info: n_ctx_train      = 2048
0.00.049.167 I print_info: n_embd           = 2048
0.00.049.167 I print_info: n_layer          = 24
0.00.049.170 I print_info: n_head           = 16
0.00.049.171 I print_info: n_head_kv        = 16
0.00.049.171 I print_info: n_rot            = 32
0.00.049.171 I print_info: n_swa            = 0
0.00.049.171 I print_info: n_embd_head_k    = 128
0.00.049.171 I print_info: n_embd_head_v    = 128
0.00.049.172 I print_info: n_gqa            = 1
0.00.049.173 I print_info: n_embd_k_gqa     = 2048
0.00.049.173 I print_info: n_embd_v_gqa     = 2048
0.00.049.174 I print_info: f_norm_eps       = 1.0e-05
0.00.049.174 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.175 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.175 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.175 I print_info: f_logit_scale    = 0.0e+00
0.00.049.176 I print_info: n_ff             = 8192
0.00.049.176 I print_info: n_expert         = 0
0.00.049.176 I print_info: n_expert_used    = 0
0.00.049.176 I print_info: causal attn      = 1
0.00.049.177 I print_info: pooling type     = 0
0.00.049.177 I print_info: rope type        = 2
0.00.049.177 I print_info: rope scaling     = linear
0.00.049.180 I print_info: freq_base_train  = 10000.0
0.00.049.180 I print_info: freq_scale_train = 1
0.00.049.180 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.181 I print_info: rope_finetuned   = unknown
0.00.049.181 I print_info: ssm_d_conv       = 0
0.00.049.181 I print_info: ssm_d_inner      = 0
0.00.049.181 I print_info: ssm_d_state      = 0
0.00.049.181 I print_info: ssm_dt_rank      = 0
0.00.049.181 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.181 I print_info: model type       = 1.4B
0.00.049.182 I print_info: model params     = 1.41 B
0.00.049.182 I print_info: general.name     = 1.4B
0.00.049.182 I print_info: vocab type       = BPE
0.00.049.183 I print_info: n_vocab          = 50304
0.00.049.183 I print_info: n_merges         = 50009
0.00.049.183 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.183 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.183 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.184 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.184 I print_info: LF token         = 128 'Ä'
0.00.049.188 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.188 I print_info: max token length = 1024
0.00.622.924 I load_tensors: offloading 24 repeating layers to GPU
0.00.622.935 I load_tensors: offloading output layer to GPU
0.00.622.936 I load_tensors: offloaded 25/25 layers to GPU
0.00.622.962 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.622.963 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.624.373 I llama_init_from_model: n_seq_max     = 1
0.00.624.378 I llama_init_from_model: n_ctx         = 128
0.00.624.379 I llama_init_from_model: n_ctx_per_seq = 128
0.00.624.379 I llama_init_from_model: n_batch       = 128
0.00.624.380 I llama_init_from_model: n_ubatch      = 128
0.00.624.380 I llama_init_from_model: flash_attn    = 0
0.00.624.382 I llama_init_from_model: freq_base     = 10000.0
0.00.624.382 I llama_init_from_model: freq_scale    = 1
0.00.624.383 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.624.386 I ggml_metal_init: allocating
0.00.624.435 I ggml_metal_init: found device: Apple M4
0.00.624.444 I ggml_metal_init: picking default device: Apple M4
0.00.625.986 I ggml_metal_init: using embedded metal library
0.00.631.666 I ggml_metal_init: GPU name:   Apple M4
0.00.631.677 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.631.678 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.631.678 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.631.679 I ggml_metal_init: simdgroup reduction   = true
0.00.631.679 I ggml_metal_init: simdgroup matrix mul. = true
0.00.631.679 I ggml_metal_init: has residency sets    = true
0.00.631.680 I ggml_metal_init: has bfloat            = true
0.00.631.680 I ggml_metal_init: use bfloat            = true
0.00.631.682 I ggml_metal_init: hasUnifiedMemory      = true
0.00.631.688 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.651.534 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.655.069 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.655.077 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.655.121 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.658.318 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.658.320 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.658.321 I llama_init_from_model: graph nodes  = 967
0.00.658.321 I llama_init_from_model: graph splits = 2
0.00.658.324 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.658.324 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.898 I 
0.00.692.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.006 I perplexity: tokenizing the input ..
0.00.702.193 I perplexity: tokenization took 9.185 ms
0.00.702.206 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.842.027 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.843.370 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.843.382 I llama_perf_context_print:        load time =     683.96 ms
0.00.843.383 I llama_perf_context_print: prompt eval time =     139.59 ms /   128 tokens (    1.09 ms per token,   916.98 tokens per second)
0.00.843.385 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.843.386 I llama_perf_context_print:       total time =     150.49 ms /   129 tokens
0.00.843.770 I ggml_metal_free: deallocating

real	0m0.858s
user	0m0.092s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.796 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.465 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.471 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.477 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.477 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.478 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.478 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.478 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.479 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.480 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.480 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.482 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.482 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.482 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.483 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.485 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.485 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.485 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.498 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.592 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.560 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.561 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.561 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.561 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.562 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.562 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.562 I llama_model_loader: - type  f32:  194 tensors
0.00.024.562 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.563 I print_info: file format = GGUF V3 (latest)
0.00.024.563 I print_info: file type   = Q6_K
0.00.024.564 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.387 I load: special tokens cache size = 25
0.00.049.649 I load: token to piece cache size = 0.2984 MB
0.00.049.652 I print_info: arch             = gptneox
0.00.049.653 I print_info: vocab_only       = 0
0.00.049.653 I print_info: n_ctx_train      = 2048
0.00.049.653 I print_info: n_embd           = 2048
0.00.049.653 I print_info: n_layer          = 24
0.00.049.656 I print_info: n_head           = 16
0.00.049.657 I print_info: n_head_kv        = 16
0.00.049.657 I print_info: n_rot            = 32
0.00.049.658 I print_info: n_swa            = 0
0.00.049.658 I print_info: n_embd_head_k    = 128
0.00.049.659 I print_info: n_embd_head_v    = 128
0.00.049.659 I print_info: n_gqa            = 1
0.00.049.660 I print_info: n_embd_k_gqa     = 2048
0.00.049.661 I print_info: n_embd_v_gqa     = 2048
0.00.049.661 I print_info: f_norm_eps       = 1.0e-05
0.00.049.662 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.662 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.662 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.666 I print_info: f_logit_scale    = 0.0e+00
0.00.049.667 I print_info: n_ff             = 8192
0.00.049.667 I print_info: n_expert         = 0
0.00.049.668 I print_info: n_expert_used    = 0
0.00.049.668 I print_info: causal attn      = 1
0.00.049.668 I print_info: pooling type     = 0
0.00.049.668 I print_info: rope type        = 2
0.00.049.670 I print_info: rope scaling     = linear
0.00.049.670 I print_info: freq_base_train  = 10000.0
0.00.049.670 I print_info: freq_scale_train = 1
0.00.049.671 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.671 I print_info: rope_finetuned   = unknown
0.00.049.671 I print_info: ssm_d_conv       = 0
0.00.049.671 I print_info: ssm_d_inner      = 0
0.00.049.672 I print_info: ssm_d_state      = 0
0.00.049.672 I print_info: ssm_dt_rank      = 0
0.00.049.672 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.672 I print_info: model type       = 1.4B
0.00.049.673 I print_info: model params     = 1.41 B
0.00.049.675 I print_info: general.name     = 1.4B
0.00.049.676 I print_info: vocab type       = BPE
0.00.049.676 I print_info: n_vocab          = 50304
0.00.049.676 I print_info: n_merges         = 50009
0.00.049.676 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.676 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.676 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.677 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.677 I print_info: LF token         = 128 'Ä'
0.00.049.677 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.677 I print_info: max token length = 1024
0.00.513.425 I load_tensors: offloading 24 repeating layers to GPU
0.00.513.432 I load_tensors: offloading output layer to GPU
0.00.513.433 I load_tensors: offloaded 25/25 layers to GPU
0.00.513.458 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.513.459 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.514.900 I llama_init_from_model: n_seq_max     = 1
0.00.514.902 I llama_init_from_model: n_ctx         = 128
0.00.514.903 I llama_init_from_model: n_ctx_per_seq = 128
0.00.514.903 I llama_init_from_model: n_batch       = 128
0.00.514.904 I llama_init_from_model: n_ubatch      = 128
0.00.514.904 I llama_init_from_model: flash_attn    = 0
0.00.514.905 I llama_init_from_model: freq_base     = 10000.0
0.00.514.905 I llama_init_from_model: freq_scale    = 1
0.00.514.906 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.514.907 I ggml_metal_init: allocating
0.00.514.934 I ggml_metal_init: found device: Apple M4
0.00.514.938 I ggml_metal_init: picking default device: Apple M4
0.00.516.175 I ggml_metal_init: using embedded metal library
0.00.522.098 I ggml_metal_init: GPU name:   Apple M4
0.00.522.101 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.522.102 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.522.103 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.522.104 I ggml_metal_init: simdgroup reduction   = true
0.00.522.104 I ggml_metal_init: simdgroup matrix mul. = true
0.00.522.104 I ggml_metal_init: has residency sets    = true
0.00.522.105 I ggml_metal_init: has bfloat            = true
0.00.522.105 I ggml_metal_init: use bfloat            = true
0.00.522.106 I ggml_metal_init: hasUnifiedMemory      = true
0.00.522.107 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.538.546 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.541.990 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.541.996 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.542.024 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.545.195 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.545.197 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.545.197 I llama_init_from_model: graph nodes  = 967
0.00.545.198 I llama_init_from_model: graph splits = 2
0.00.545.200 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.545.200 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.582.720 I 
0.00.582.808 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.582.830 I perplexity: tokenizing the input ..
0.00.594.932 I perplexity: tokenization took 12.1 ms
0.00.594.953 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.734.923 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.736.258 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.736.273 I llama_perf_context_print:        load time =     573.91 ms
0.00.736.274 I llama_perf_context_print: prompt eval time =     139.57 ms /   128 tokens (    1.09 ms per token,   917.12 tokens per second)
0.00.736.275 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.736.275 I llama_perf_context_print:       total time =     153.56 ms /   129 tokens
0.00.736.651 I ggml_metal_free: deallocating

real	0m0.751s
user	0m0.093s
sys	0m0.120s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.244 I build: 4562 (9dc5ef45) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.638 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.078 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.087 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.099 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.100 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.101 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.102 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.104 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.108 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.109 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.110 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.111 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.114 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.115 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.116 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.119 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.120 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.120 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.842 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.888 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.364 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.366 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.366 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.367 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.367 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.368 I llama_model_loader: - type  f32:  194 tensors
0.00.055.368 I llama_model_loader: - type  f16:   98 tensors
0.00.055.369 I print_info: file format = GGUF V3 (latest)
0.00.055.370 I print_info: file type   = all F32 (guessed)
0.00.055.371 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.082.710 I load: special tokens cache size = 25
0.00.089.581 I load: token to piece cache size = 0.2984 MB
0.00.089.584 I print_info: arch             = gptneox
0.00.089.584 I print_info: vocab_only       = 0
0.00.089.585 I print_info: n_ctx_train      = 2048
0.00.089.585 I print_info: n_embd           = 2048
0.00.089.585 I print_info: n_layer          = 24
0.00.089.588 I print_info: n_head           = 16
0.00.089.589 I print_info: n_head_kv        = 16
0.00.089.589 I print_info: n_rot            = 32
0.00.089.589 I print_info: n_swa            = 0
0.00.089.590 I print_info: n_embd_head_k    = 128
0.00.089.592 I print_info: n_embd_head_v    = 128
0.00.089.593 I print_info: n_gqa            = 1
0.00.089.594 I print_info: n_embd_k_gqa     = 2048
0.00.089.594 I print_info: n_embd_v_gqa     = 2048
0.00.089.600 I print_info: f_norm_eps       = 1.0e-05
0.00.089.600 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.089.600 I print_info: f_clamp_kqv      = 0.0e+00
0.00.089.600 I print_info: f_max_alibi_bias = 0.0e+00
0.00.089.601 I print_info: f_logit_scale    = 0.0e+00
0.00.089.601 I print_info: n_ff             = 8192
0.00.089.602 I print_info: n_expert         = 0
0.00.089.602 I print_info: n_expert_used    = 0
0.00.089.604 I print_info: causal attn      = 1
0.00.089.604 I print_info: pooling type     = 0
0.00.089.604 I print_info: rope type        = 2
0.00.089.604 I print_info: rope scaling     = linear
0.00.089.604 I print_info: freq_base_train  = 10000.0
0.00.089.605 I print_info: freq_scale_train = 1
0.00.089.605 I print_info: n_ctx_orig_yarn  = 2048
0.00.089.605 I print_info: rope_finetuned   = unknown
0.00.089.606 I print_info: ssm_d_conv       = 0
0.00.089.606 I print_info: ssm_d_inner      = 0
0.00.089.606 I print_info: ssm_d_state      = 0
0.00.089.606 I print_info: ssm_dt_rank      = 0
0.00.089.606 I print_info: ssm_dt_b_c_rms   = 0
0.00.089.607 I print_info: model type       = 1.4B
0.00.089.609 I print_info: model params     = 1.41 B
0.00.089.609 I print_info: general.name     = 1.4B
0.00.089.610 I print_info: vocab type       = BPE
0.00.089.610 I print_info: n_vocab          = 50304
0.00.089.610 I print_info: n_merges         = 50009
0.00.089.610 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.089.611 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.089.611 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.089.611 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.089.611 I print_info: LF token         = 128 'Ä'
0.00.089.612 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.089.612 I print_info: max token length = 1024
0.01.374.988 I load_tensors: offloading 24 repeating layers to GPU
0.01.374.994 I load_tensors: offloading output layer to GPU
0.01.374.994 I load_tensors: offloaded 25/25 layers to GPU
0.01.375.024 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.375.026 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.376.161 I llama_init_from_model: n_seq_max     = 1
0.01.376.163 I llama_init_from_model: n_ctx         = 128
0.01.376.163 I llama_init_from_model: n_ctx_per_seq = 128
0.01.376.163 I llama_init_from_model: n_batch       = 128
0.01.376.163 I llama_init_from_model: n_ubatch      = 128
0.01.376.164 I llama_init_from_model: flash_attn    = 0
0.01.376.164 I llama_init_from_model: freq_base     = 10000.0
0.01.376.164 I llama_init_from_model: freq_scale    = 1
0.01.376.165 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.376.169 I ggml_metal_init: allocating
0.01.376.252 I ggml_metal_init: found device: Apple M4
0.01.376.257 I ggml_metal_init: picking default device: Apple M4
0.01.377.408 I ggml_metal_init: using embedded metal library
0.01.381.447 I ggml_metal_init: GPU name:   Apple M4
0.01.381.450 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.381.450 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.381.450 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.381.451 I ggml_metal_init: simdgroup reduction   = true
0.01.381.451 I ggml_metal_init: simdgroup matrix mul. = true
0.01.381.451 I ggml_metal_init: has residency sets    = true
0.01.381.451 I ggml_metal_init: has bfloat            = true
0.01.381.451 I ggml_metal_init: use bfloat            = true
0.01.381.452 I ggml_metal_init: hasUnifiedMemory      = true
0.01.381.453 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.392.028 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.393.770 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.393.775 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.393.789 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.395.440 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.395.441 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.395.442 I llama_init_from_model: graph nodes  = 967
0.01.395.442 I llama_init_from_model: graph splits = 2
0.01.395.443 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.395.443 I 
0.01.395.479 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.395.480 I compute_imatrix: tokenizing the input ..
0.01.403.232 I compute_imatrix: tokenization took 7.751 ms
0.01.403.234 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.668.602 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.671.254 I llama_perf_context_print:        load time =    1644.96 ms
0.01.671.255 I llama_perf_context_print: prompt eval time =     263.62 ms /   128 tokens (    2.06 ms per token,   485.55 tokens per second)
0.01.671.255 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.671.256 I llama_perf_context_print:       total time =    1647.61 ms /   129 tokens
0.01.671.757 I ggml_metal_free: deallocating

real	0m1.856s
user	0m0.146s
sys	0m0.280s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4562 (9dc5ef45)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x130b091d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x130b098e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x130b09e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x130b0a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x130b0a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x130b0afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x130b0b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x130b0bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x130b0c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x130b0c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x130b0cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x130b0cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x130b0dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x130b0e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x130b0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x130b0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x130b0f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x130b0fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x130b10710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x130b10ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x130b11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x130b11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x130b12440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x130b12ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x130b13400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x130b136c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x130b13cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x130b14940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x130b14e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x130b15140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x130b155e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x130b158a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x130b16130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x130b16670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x130b16930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x130b16dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x130b17270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x130b17710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x130b17bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x130b18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x130b184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x130b18990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x130b18e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x130b192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x130b19590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x130b19ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x130b1a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x130b1aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x130b1b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x130b1b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x130b1bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x130b1c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x130b1c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x130b1cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x130b1d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x130b1dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x130b1e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x130b1e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x130b1e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x130b1f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x130b1f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x130b1f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x130b1fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x130b201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x130b20660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x130b20b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x130b20fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x130b21440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x130b218e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x130b21d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x130b22220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x130b226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x130b22b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x130b230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x130b23600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x130b23b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x130b240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x130b245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x130b24b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x130b25090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x130b255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x130b25b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x130b26080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x130b265d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x130b26b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x130b27070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x130b275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x130b27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x130b28060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x130b285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x130b28b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x130b29050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x130b295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x130b29af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x130b2a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x130b2a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x130b2aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x130b1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x130b2af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x130b2b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x130b2bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x130b2c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x130b2c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x130b2cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x130b2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x130b2d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x130b2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x130b2e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x130b2e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x130b2ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x130b2f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x130b2f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x130b2fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x130b300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x130b30550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x130b309f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x130b30e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x130b31330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x130b317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x130b31c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x130b32110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x130b325b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x130b32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x130b32ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x130b33390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x130b33830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x130b33cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x130b34170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x130b34610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x130b34ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x130b34f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x130b353f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x130b35890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x130b35d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x130b361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x130b36670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x130b36b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x130b36fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x130b37450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x130b378f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x130b37d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x130b38230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x130b386d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x130b38b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x130b39010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x130b394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x130b39950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x130b39df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x130b3a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x130b3a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x130b3abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x130b3b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x130b3b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x130b3b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x130b3be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x130b3c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x130b3c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x130b3cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x130b3d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x130b3d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x130b3da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x130b3deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x130b3e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x130b3e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x130b3ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x130b3f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x130b3f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x130b3fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x130b3ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x130b403b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x130b40850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x130b40cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x130b41190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x130b41630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x130b41ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x130b41f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x130b42410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x130b428b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x130b42d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x130b431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x130b43690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x130b43b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x130b43fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x130b44470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x130b44910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x130b44db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x130b45250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x130b456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x130b45b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x130b46030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x130b464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x130b46970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x130b46e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x130b47360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x130b478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x130b47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x130b48350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x130b48610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x130b48c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x130b49230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x130b49840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x130b4a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x130b4a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x130b4a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x130b4ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x130b4b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x130b4bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x130b4c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x130b4c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x130b4c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x130b4d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x130b4d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x130b4dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x130b4e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x130b4e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x130b4ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x130b4f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x130b4f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x130b4fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x130b50100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x130b50650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x130b50ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x130b510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x130b51640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x130b51b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x130b520e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x130b52630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x130b52b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x130b530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x130b53620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x130b53b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x130b540c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x130b54610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x130b54b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x130b550b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x130b55600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x130b55b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x130b560a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x130b565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x130b56b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x130b57090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x130b575e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x130b57b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x130b58080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x130b585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x130b58b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x130b59070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x130b595c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x130b59b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x130b5a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x130b5a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x130b5ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x130b5b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x130b5b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x130b5baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x130b5c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x130b5c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x130b5cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x130b5d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x130b5d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x130b5dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x130b5e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x130b5e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x130b5eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x130b5f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x130b5f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x130b5fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x130b5ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x130b603f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x130b60890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x130b60d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x130b611d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x130b61670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x130b61b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x130b61fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x130b62450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x130b628f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x130b62d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x130b63230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x130b636d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x130b63b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x130b64010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x130b64560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x130b64c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x130b653a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x130b65ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x130b661e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x130b664a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x130b66c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x130b66f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x130b67560 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.729.072 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.729.076 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x130c09260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x130c096d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x130c09b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x130c09fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x130c0a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x130c0a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x130c0ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x130c0b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x130c0b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x130c0ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x130c0bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x130c0c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x130c0d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x130c0d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x130c0e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x130c0e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x130c0eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x130c0f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x130c0fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x130c104b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x130c10bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x130c112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x130c11a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x130c12130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x130c12850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x130c12b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x130a04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x130a05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x130a056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x130a05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x130a05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x130a06520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x130a06990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x130a06e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x130a07270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x130a076e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x130a07b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x130a07fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x130a08430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x130a088a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x130a08d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x130a09180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x130a095f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x130a09a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x130a09ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x130a0a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x130a0a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x130a0ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x130a0b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x130a0b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x130a0b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x130a0bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x130a0c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x130a0c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x130a0cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x130a0cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x130a0d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x130a0da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x130a0deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x130a0e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x130a0e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x130a0ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x130a0f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x130a0f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x130a0f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x130a0fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x130a10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x130a106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x130a10b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x130a10f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x130a113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x130a11860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x130a11cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x130a12140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x130a125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x130a12a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x130a12e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x130a13300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x130a13770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x130a13be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x130a14050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x130a144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x130a14930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x130a14da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x130a15210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x130a15680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x130a15af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x130a15f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x130a163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x130a16840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x130a16cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x130a17120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x130a17590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x130a17a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x130a17e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x130a182e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x130a18750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x130a18bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x130a19030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x130a194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x130a19910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x130a19d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x130a1a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x130a1a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x130a1aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x130a1af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x130a1b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x130a1b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x130a1bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x130a1c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x130a1c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x130a1c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x130a1ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x130a1d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x130a1d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x130a1dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x130a1e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x130a1e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x130a1e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x130a1ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x130a1f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x130a1f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x130a1fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x130a1ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x130a20390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x130a20800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x130a20c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x130a210e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x130a21550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x130a219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x130a21e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x130a222a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x130a22710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x130a22b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x130a22ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x130a23460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x130a238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x130a23d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x130a241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x130a24620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x130a24a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x130a24f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x130a25370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x130a257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x130a25c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x130a260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x130a26530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x130a269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x130a26e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x130a27280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x130a276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x130a27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x130a27fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x130a28440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x130a288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x130a28d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x130a29190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x130a29600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x130a29a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x130a29ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x130a2a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x130a2a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x130a2ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x130a2b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x130a2b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x130a2b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x130a2c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x130a2c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x130a2c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x130a2ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x130a2d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x130a2d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x130a2da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x130a2de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x130a2e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x130a2e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x130a2ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x130a2f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x130a2f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x130a2f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x130a2fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x130a30210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x130a30680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x130a30af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x130a30f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x130a313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x130a31840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x130a31cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x130a32120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x130a32590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x130a32a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x130a32e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x130a332e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x130a33750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x130a33bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x130a34030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x130a344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x130a34910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x130a34d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x130a351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x130a35660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x130a35ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x130a35f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x130a363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x130a36820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x130a36c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x130a37100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x130a37570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x130a379e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x130a37e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x130a38a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x130a38cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x130a38f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x130a393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x130a39860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x130a39cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x130a3a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x130a3a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x130a3aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x130a3ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x130a3b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x130a3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x130a3bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x130a3c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x130a3c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x130a3c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x130a3cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x130a3d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x130a3d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x130a3daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x130a3df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x130a3e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x130a3e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x130a3ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x130a3f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x130a3f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x130a3fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x130a3fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x130a402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x130a40750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x130a40bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x130a41030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x130a414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x130a41910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x130a41d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x130a421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x130a42660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x130a42ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x130a42f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x130a433b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x130a43820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x130a43c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x130a44100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x130a44570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x130a449e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x130a44e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x130a452c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x130a45730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x130a45ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x130a46010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x130a46480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x130a468f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x130a46d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x130a471d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x130a47640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x130a47ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x130a47f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x130a48390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x130a48800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x130a48c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x130a490e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x130a49550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x130a499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x130a49e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x130a4a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x130a4a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x130a4ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x130a4aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x130a4b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x130a4b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x130a4bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x130a4c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x130a4c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x130a4d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x130a4d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x130a4ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x130a4e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x130a4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x130a4ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x130a4f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x130a4f930 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10ff044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10ff04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10ff04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10ff05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10ff056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10ff05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10ff05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10ff063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10ff06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10ff06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10ff07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10ff078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10ff083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10ff08b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10ff09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10ff09aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10ff0a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10ff0a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10ff0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10ff0b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10ff0bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10ff0c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10ff0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10ff0d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10ff0db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10ff0de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10ff0e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10ff0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10ff0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10ff0ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10ff0f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10ff0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10ff0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10ff0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10ff10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10ff107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10ff10c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10ff110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10ff11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10ff119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10ff11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10ff12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10ff12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10ff12b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10ff12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10ff13450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10ff138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10ff13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10ff141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10ff14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10ff14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10ff14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10ff15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10ff157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10ff15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10ff160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10ff16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10ff16b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10ff16f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10ff17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10ff17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10ff17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10ff18150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10ff185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10ff18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10ff18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10ff19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10ff19780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10ff19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10ff1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10ff1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10ff1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10ff1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10ff1b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10ff1b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10ff1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10ff1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10ff1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10ff1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10ff1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10ff1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10ff1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10ff1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10ff1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10ff1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10ff1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10ff1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10ff1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10ff1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10ff1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10ff1fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10ff20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10ff20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10ff20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10ff20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10ff213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10ff21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10ff21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10ff22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10ff22580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10ff229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10ff22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10ff232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10ff23b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10ff23e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10ff24290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10ff24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10ff24b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10ff24fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10ff25450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10ff258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10ff25d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10ff261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10ff26610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10ff26a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10ff26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10ff27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10ff277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10ff27c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10ff280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10ff28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10ff28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10ff28e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10ff29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10ff296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10ff29b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10ff29fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10ff2a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10ff2a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10ff2ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10ff2b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10ff2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10ff2ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10ff2bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10ff2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10ff2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10ff2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10ff2d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10ff2d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10ff2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10ff2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10ff2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10ff2e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10ff2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10ff2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10ff2f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10ff2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10ff2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10ff30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10ff305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10ff30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10ff30eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10ff31320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10ff31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10ff31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10ff32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10ff324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10ff32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10ff32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10ff33230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10ff336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10ff33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10ff33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10ff343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10ff34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10ff34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10ff35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10ff355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10ff35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10ff35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10ff36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10ff36770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10ff36be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10ff37050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10ff374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10ff37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10ff37da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10ff38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10ff38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10ff38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10ff38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10ff393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10ff39840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10ff39cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10ff3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10ff3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10ff3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10ff3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10ff3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10ff3b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10ff3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10ff3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10ff3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10ff3c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10ff3cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x130c0c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x130c08e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x130c0c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x130c12f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x130c13230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x130c134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x130c137b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x130c13a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x130c13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x130c13ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x130c142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x130c14570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x130c14830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x130c14af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x130c14db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x130c15380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x130c15640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x130c15900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x130c15bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x130c15e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x130c16140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x130c16400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x130c166c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x130c16c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x130a4c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x130a2bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x130a38110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x130a4fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x130a502b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x130a50570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x130a50830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x130a50af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x130a50db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x130a51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x130a51330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x130a515f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x130a518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x130a51b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x130a51e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x130a520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x130a523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x130a52670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x130a52930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x130a52bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x130a52eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x130a53170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x130a53430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x130a536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x130a539b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x130a53c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x130a53f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x130a541f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x130a544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x130a54770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x130a54a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x130a54cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x130a54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x130a55270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x130a55530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x130a557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x130a55ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x130a55d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x130a56030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x130a562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x130a565b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x130a56870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x130a56b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x130a56df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x130a570b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x130a57370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x130a57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x130a578f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x130a57bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x130a57e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x130a58130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x130a583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x130a586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x130a58970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x130a58c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x130a58ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x130a591b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x130a59470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x130a59730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x130a599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x130a59cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x130a59f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x130a5a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x130a5a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x130a5a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x130a5acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x130a5b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x130a5b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x130a5bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x130a5bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x130a5c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x130a5c4f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.780s
user	0m0.299s
sys	0m0.319s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4562 (9dc5ef45)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1366101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136610900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136610eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136611460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136611a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136611fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136612570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136612b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1366130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1366135d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136613ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136613fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136614af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1366152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136615ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1366161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1366168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136617010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136617730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136617f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136618620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136618d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136619460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136619d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13661a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13661a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13661acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13661b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13661bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13661c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13661c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13661c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13661d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13661d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13661d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13661ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13661e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13661e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13661ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13661f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13661f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13661f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13661fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1366202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1366205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136620bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1366211d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136621af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136622100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136622710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136622d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136623330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136623940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136623f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136624740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136624be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136625080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136625340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136625950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136626140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136626400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1366268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136626d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1366271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136627680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136627b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136627fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136628460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136628900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136628da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136629240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1366296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136629b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13662a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13662a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13662ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13662b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13662b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13662bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13662c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13662c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13662cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13662d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13662d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13662db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13662e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13662e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13662eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13662f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13662f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13662fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136630070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1366305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136630b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136631060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1366315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136631b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1366217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136631f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136632720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136632c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1366331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136633710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136633c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1366341b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136634700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136634c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1366351a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1366356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136635c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136636190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1366366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136636c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1366370d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136637570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136637a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136637eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136638350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1366387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136638c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136639130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1366395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136639a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136639f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13663a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13663a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13663acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13663b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13663b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13663bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13663bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13663c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13663c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13663cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13663d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13663d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13663db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13663dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13663e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13663e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13663edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13663f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13663f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13663fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136640030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1366404d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136640970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136640e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1366412b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136641750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136641bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136642090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136642530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1366429d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136642e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136643310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1366437b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136643c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1366440f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136644590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136644a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136644ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136645370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136645810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136645cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136646150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1366465f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136646a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136646f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1366473d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136647870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136647d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1366481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136648650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136648af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136648f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136649430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1366498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136649d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13664a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13664a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13664ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13664aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13664b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13664b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13664bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13664c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13664c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13664cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13664d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13664d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13664d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13664de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13664e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13664e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13664ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13664f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13664f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13664fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136650250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136650860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136651050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1366514f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1366517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136651dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1366523d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136652bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136653060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136653500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1366539a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136654150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1366546a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136654bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136655140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136655690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136655be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136656130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136656680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136656bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136657120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136657670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136657bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136658110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136658660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136658bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136659100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136659650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136659ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13665a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13665a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13665ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13665b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13665b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13665bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13665c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13665c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13665cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13665d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13665d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13665db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13665e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13665e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13665eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13665f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13665f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13665fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136660090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1366605e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136660b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136661080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1366615d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136661b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136662070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1366625c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136662b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136663060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1366635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136663b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136664050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1366645a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136664af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136665040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136665590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136665ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136666030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136666580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136666ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136666f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136667410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1366678b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136667d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1366681f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136668690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136668b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136668fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136669470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136669910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136669db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13666a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13666a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13666ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13666b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13666b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13666bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13666c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13666cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13666d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13666d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13666dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13666df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13666e580 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.112.115 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.112.118 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127f054f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127f05960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127f05dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127f06240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127f066b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127f06b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127f06f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127f07400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127f07870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127f07ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127f08150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127f08840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127f09360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127f09b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127f0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127f0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127f0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127f0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127f0bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127f0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127f0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127f0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127f0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127f0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127f0ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127f0ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127f0eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127f0f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127f0f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127f0fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127f101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127f106e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127f10b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127f10e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127f11280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127f116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127f11b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127f11fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127f12440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127f128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127f12d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127f13190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127f13600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127f13a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127f13ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127f14350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127f147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127f14c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127f150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127f15510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127f15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127f15df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127f16260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127f166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127f16b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127f16fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127f17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127f17a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127f17e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127f18300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127f18770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127f18be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127f19050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127f194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127f19930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127f19da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127f1a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127f1a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127f1aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127f1af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127f1b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127f1b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127f1bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127f1c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127f1c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127f1ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127f1ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127f1d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127f1d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127f1dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127f1e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127f1e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127f1e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127f1ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127f1f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127f1f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127f1fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127f1ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127f203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127f20820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127f20c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127f21100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127f21570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127f219e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127f21e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127f222c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127f22730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127f22ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127f23010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127f23480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127f238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127f23d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127f241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127f24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127f24ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127f24f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127f25390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127f25800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127f25c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127f260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127f26550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127f269c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127f26e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127f272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127f27710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127f27b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127f27ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127f28460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127f288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127f28d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127f291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127f29620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127f29a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127f29f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127f2a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127f2a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127f2ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127f2b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127f2b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127f2b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127f2be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127f2c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127f2c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127f2cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127f2cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127f2d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127f2d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127f2dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127f2e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127f2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127f2ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127f2eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127f2f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127f2f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127f2fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127f300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127f30510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127f30980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127f30df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127f31260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127f316d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127f31b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127f31fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127f32420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127f32890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127f32d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127f33170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127f335e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127f33a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127f33ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127f34330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127f347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127f34c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127f35080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127f354f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127f35960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127f36590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127f36850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127f36b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127f36f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127f373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127f37860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127f37cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127f38140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127f385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127f38a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127f38e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127f39300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127f39770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127f39be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127f3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127f3a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127f3a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127f3ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127f3b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127f3b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127f3baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127f3bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127f3c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127f3c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127f3ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127f3d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127f3d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127f3da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127f3de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127f3e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127f3e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127f3ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127f3f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127f3f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127f3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127f3fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127f402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127f407f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127f40c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127f410d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127f41540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127f419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127f41ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127f423e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127f42f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127f43210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127f437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127f43d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127f44350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127f44910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127f44ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127f45490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127f45a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127f46010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127f465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127f46b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127f47150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127f47710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127f47cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127f48290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127f48850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127f48e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127f493d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127f49990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127f49f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127f4a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127f4aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127f4b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127f4b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127f4bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127f4c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127f4c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127f4cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127f4d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127f4d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127f4de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127f4e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127f4ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127f4efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127f4f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127f4fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127f50110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127f506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127f50c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127f51250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127f51810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127f51dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127f52390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127f52950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127f52f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127f534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127f53a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127f54050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127f54610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127f54bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127f55190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127f55750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127f55d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127f562d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127f56890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127f56e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127f57410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127f57910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127f57e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127f58310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127f58810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127f58d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127f59210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127f59710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127f59c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127f5a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127f5a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127f5ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127f5b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127f5b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127f5ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127f5bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127f5c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127f5d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127f5d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127f5de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127f5e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127f5e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127f5ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127f5f200 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127f5c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127f4d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127f4bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127f48b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127f462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127f55a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127f531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127f50f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127f4ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127f46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127f44610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127f49690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127f4a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127f4fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127f4ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127f548d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127f48550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127f51ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127f4b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127f4d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127f47f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127f55fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127f45190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127f43a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127f45d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127f56590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127f4b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127f53d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127f49c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127f4c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127f503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127f479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127f50990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127f52090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127f46890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127f54e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127f52650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127f4e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127f57110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127f45750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127f56b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127f44bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127f55450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127f4f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127f51510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127f54310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127f52c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127f4ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127f426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127f05020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127f5e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127f0c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127f5f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127f5fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127f5fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127f60120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127f603e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127f606a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127f60960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127f60c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127f60ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127f611a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127f61460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127f61720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127f619e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127f61ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127f61f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127f62220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127f624e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127f627a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127f62a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127f62d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127f62fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127f632a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127f63560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127f63820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127f63ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127f63da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127f64060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127f64320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127f645e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127f648a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127f64b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127f64e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127f650e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127f653a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127f65660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127f65920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127f65be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127f65ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127f66160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127f66420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127f666e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127f669a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127f66c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127f66f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127f671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127f674a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127f67760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127f67a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127f67ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127f67fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127f68260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127f68520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127f687e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127f68aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127f68d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127f69020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127f692e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127f695a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127f69860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127f69b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127f69de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127f6a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127f6a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127f6a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127f6a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127f6aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127f6ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127f6b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127f6b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127f6b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127f6b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127f6bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127f6bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127f6c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127f6c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127f6c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127f6c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127f6cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127f6cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127f6d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127f6d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127f6d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127f6da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127f6dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127f6dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127f6e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127f6e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127f6e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127f6eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127f6eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127f6f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127f6f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127f6f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127f6f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127f6fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127f6fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127f700e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127f703a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127f70660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127f70920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127f70be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127f70ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127f71160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127f71420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127f716e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127f719a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127f71c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127f71f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127f721e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127f724a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127f72760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127f72a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127f72ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127f72fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127f73260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127f73520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127f737e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127f73aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127f73d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127f74020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127f742e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127f745a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127f74860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127f74b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127f74de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127f750a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127f75360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127f75620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127f758e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127f75ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127f75e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127f76120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127f763e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127f766a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127f76960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127f76c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127f76ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127f771a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127f77460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127f77720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127f779e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127f77ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127f77f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127f78220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127f784e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127f787a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127f78a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127f78d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127f78fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127f792a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127f79560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127f79820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127f79ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127f79da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127f7a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127f7a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127f7a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127f7a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127f7ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127f7b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127f7b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127f7b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127f7b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127f7bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127f7c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127f7c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127f7ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127f7d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127f7d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127f7dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127f7e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127f7e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127f7ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127f7f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127f7f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127f7fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127f803d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127f80920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127f80e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127f813c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127f81910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127f81e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127f823b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127f82900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127f82e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127f833a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127f838f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127f83e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127f84390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127f848e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127f84e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127f85380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127f858d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127f85e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127f86370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127f868c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127f86e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127f87360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127f878b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127f87e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127f88350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127f888a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127f88df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127f89340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127f89890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127f89de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127f8a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127f8a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127f8add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127f8b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127f8b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127f8bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127f8c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127f8c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127f8cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127f8d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127f8d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127f8d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127f8da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127f8ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127f8e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127f8e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127f8ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127f8f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127f8f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127f8f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127f8fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127f90250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127f906c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127f90b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127f90fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127f91410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127f92100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127f92820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127f92f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127f93200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127f93670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127f93c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127f94280 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.981s
user	0m0.249s
sys	0m0.193s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
