### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.42 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.79 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.43 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.44 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.99 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.02 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.21 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.28 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.13 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.19 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.23 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.18 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  179.02 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.92 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   25.78 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.34 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.20 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 221.30 sec*proc (27 tests)

Total Test time (real) = 221.32 sec

real	3m41.405s
user	7m43.960s
sys	0m5.869s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.16 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.18 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.25 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    1.01 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.17 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.18 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.17 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   28.85 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.27 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.06 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.20 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.19 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.11 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  50.47 sec*proc (27 tests)

Total Test time (real) =  50.48 sec

real	0m50.487s
user	1m10.473s
sys	0m5.279s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.128 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.194 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.202 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.026.210 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.213 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.026.214 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.215 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.026.216 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.026.217 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.026.219 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.026.219 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.026.220 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.026.221 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.026.221 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.026.226 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.026.226 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.026.227 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.026.228 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.026.228 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.026.229 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.026.230 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.031.923 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.033.323 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.326 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.033.326 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.033.327 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.033.327 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.033.328 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.033.328 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.033.329 I llama_model_loader: - type  f32:  124 tensors
0.00.033.330 I llama_model_loader: - type  f16:   73 tensors
0.00.038.382 I llm_load_vocab: special tokens cache size = 5
0.00.040.849 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.040.853 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.040.854 I llm_load_print_meta: arch             = bert
0.00.040.854 I llm_load_print_meta: vocab type       = WPM
0.00.040.855 I llm_load_print_meta: n_vocab          = 30522
0.00.040.855 I llm_load_print_meta: n_merges         = 0
0.00.040.855 I llm_load_print_meta: vocab_only       = 0
0.00.040.855 I llm_load_print_meta: n_ctx_train      = 512
0.00.040.856 I llm_load_print_meta: n_embd           = 384
0.00.040.856 I llm_load_print_meta: n_layer          = 12
0.00.040.859 I llm_load_print_meta: n_head           = 12
0.00.040.860 I llm_load_print_meta: n_head_kv        = 12
0.00.040.861 I llm_load_print_meta: n_rot            = 32
0.00.040.861 I llm_load_print_meta: n_swa            = 0
0.00.040.861 I llm_load_print_meta: n_embd_head_k    = 32
0.00.040.861 I llm_load_print_meta: n_embd_head_v    = 32
0.00.040.862 I llm_load_print_meta: n_gqa            = 1
0.00.040.863 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.040.864 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.040.865 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.040.866 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.040.866 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.040.866 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.040.867 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.040.868 I llm_load_print_meta: n_ff             = 1536
0.00.040.868 I llm_load_print_meta: n_expert         = 0
0.00.040.868 I llm_load_print_meta: n_expert_used    = 0
0.00.040.869 I llm_load_print_meta: causal attn      = 0
0.00.040.869 I llm_load_print_meta: pooling type     = 2
0.00.040.869 I llm_load_print_meta: rope type        = 2
0.00.040.869 I llm_load_print_meta: rope scaling     = linear
0.00.040.870 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.040.871 I llm_load_print_meta: freq_scale_train = 1
0.00.040.871 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.040.871 I llm_load_print_meta: rope_finetuned   = unknown
0.00.040.871 I llm_load_print_meta: ssm_d_conv       = 0
0.00.040.872 I llm_load_print_meta: ssm_d_inner      = 0
0.00.040.872 I llm_load_print_meta: ssm_d_state      = 0
0.00.040.872 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.040.873 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.040.887 I llm_load_print_meta: model type       = 33M
0.00.040.887 I llm_load_print_meta: model ftype      = F16
0.00.040.888 I llm_load_print_meta: model params     = 33.21 M
0.00.040.889 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.040.889 I llm_load_print_meta: general.name     = Bge Small
0.00.040.890 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.040.890 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.040.890 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.040.891 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.040.891 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.040.891 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.040.892 I llm_load_print_meta: max token length = 21
0.00.043.150 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.043.151 I llm_load_tensors: offloading output layer to GPU
0.00.043.152 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.043.177 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.043.179 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.043.772 I llama_new_context_with_model: n_seq_max     = 1
0.00.043.773 I llama_new_context_with_model: n_ctx         = 512
0.00.043.774 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.043.774 I llama_new_context_with_model: n_batch       = 2048
0.00.043.774 I llama_new_context_with_model: n_ubatch      = 2048
0.00.043.774 I llama_new_context_with_model: flash_attn    = 0
0.00.043.775 I llama_new_context_with_model: freq_base     = 10000.0
0.00.043.775 I llama_new_context_with_model: freq_scale    = 1
0.00.043.776 I ggml_metal_init: allocating
0.00.043.780 I ggml_metal_init: found device: Apple M4
0.00.043.785 I ggml_metal_init: picking default device: Apple M4
0.00.044.655 I ggml_metal_init: using embedded metal library
0.00.048.582 I ggml_metal_init: GPU name:   Apple M4
0.00.048.585 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.048.586 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.048.587 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.048.587 I ggml_metal_init: simdgroup reduction   = true
0.00.048.587 I ggml_metal_init: simdgroup matrix mul. = true
0.00.048.588 I ggml_metal_init: has bfloat            = true
0.00.048.588 I ggml_metal_init: use bfloat            = true
0.00.048.588 I ggml_metal_init: hasUnifiedMemory      = true
0.00.048.589 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.060.769 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.060.772 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.060.773 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.061.603 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.061.604 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.061.605 I llama_new_context_with_model: graph nodes  = 429
0.00.061.605 I llama_new_context_with_model: graph splits = 2
0.00.061.627 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.068.151 I 
0.00.068.183 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.068.932 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.074.027 I llama_perf_context_print:        load time =      46.95 ms
0.00.074.028 I llama_perf_context_print: prompt eval time =       4.95 ms /     9 tokens (    0.55 ms per token,  1819.65 tokens per second)
0.00.074.029 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.074.029 I llama_perf_context_print:       total time =       5.88 ms /    10 tokens
0.00.074.163 I ggml_metal_free: deallocating

real	0m0.292s
user	0m0.054s
sys	0m0.033s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.039 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.447 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.574 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.577 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.579 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.579 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.580 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.580 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.580 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.581 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.581 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.582 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.582 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.583 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.585 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.585 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.585 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.585 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.586 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.586 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.586 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.280 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.947 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.948 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.948 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.949 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.949 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.949 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.949 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.950 I llama_model_loader: - type  f32:  124 tensors
0.00.014.950 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.453 I llm_load_vocab: special tokens cache size = 5
0.00.018.794 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.796 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.797 I llm_load_print_meta: arch             = bert
0.00.018.797 I llm_load_print_meta: vocab type       = WPM
0.00.018.797 I llm_load_print_meta: n_vocab          = 30522
0.00.018.797 I llm_load_print_meta: n_merges         = 0
0.00.018.798 I llm_load_print_meta: vocab_only       = 0
0.00.018.798 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.798 I llm_load_print_meta: n_embd           = 384
0.00.018.798 I llm_load_print_meta: n_layer          = 12
0.00.018.800 I llm_load_print_meta: n_head           = 12
0.00.018.801 I llm_load_print_meta: n_head_kv        = 12
0.00.018.803 I llm_load_print_meta: n_rot            = 32
0.00.018.803 I llm_load_print_meta: n_swa            = 0
0.00.018.803 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.803 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.804 I llm_load_print_meta: n_gqa            = 1
0.00.018.804 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.805 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.809 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.809 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.809 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.810 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.810 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.810 I llm_load_print_meta: n_ff             = 1536
0.00.018.810 I llm_load_print_meta: n_expert         = 0
0.00.018.810 I llm_load_print_meta: n_expert_used    = 0
0.00.018.811 I llm_load_print_meta: causal attn      = 0
0.00.018.812 I llm_load_print_meta: pooling type     = 2
0.00.018.812 I llm_load_print_meta: rope type        = 2
0.00.018.812 I llm_load_print_meta: rope scaling     = linear
0.00.018.813 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.813 I llm_load_print_meta: freq_scale_train = 1
0.00.018.813 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.813 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.813 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.814 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.814 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.814 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.814 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.820 I llm_load_print_meta: model type       = 33M
0.00.018.820 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.821 I llm_load_print_meta: model params     = 33.21 M
0.00.018.821 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.821 I llm_load_print_meta: general.name     = Bge Small
0.00.018.822 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.822 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.823 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.823 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.823 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.824 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.824 I llm_load_print_meta: max token length = 21
0.00.020.096 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.096 I llm_load_tensors: offloading output layer to GPU
0.00.020.096 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.103 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.104 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.470 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.471 I llama_new_context_with_model: n_ctx         = 512
0.00.020.471 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.471 I llama_new_context_with_model: n_batch       = 2048
0.00.020.471 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.471 I llama_new_context_with_model: flash_attn    = 0
0.00.020.472 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.472 I llama_new_context_with_model: freq_scale    = 1
0.00.020.472 I ggml_metal_init: allocating
0.00.020.475 I ggml_metal_init: found device: Apple M4
0.00.020.477 I ggml_metal_init: picking default device: Apple M4
0.00.020.989 I ggml_metal_init: using embedded metal library
0.00.023.102 I ggml_metal_init: GPU name:   Apple M4
0.00.023.104 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.105 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.105 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.105 I ggml_metal_init: simdgroup reduction   = true
0.00.023.105 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.105 I ggml_metal_init: has bfloat            = true
0.00.023.106 I ggml_metal_init: use bfloat            = true
0.00.023.106 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.106 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.146 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.032.148 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.032.149 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.032.758 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.032.759 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.032.760 I llama_new_context_with_model: graph nodes  = 429
0.00.032.760 I llama_new_context_with_model: graph splits = 2
0.00.032.773 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.037.107 I 
0.00.037.132 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.037.671 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.041.076 I llama_perf_context_print:        load time =      27.66 ms
0.00.041.077 I llama_perf_context_print: prompt eval time =       3.27 ms /     9 tokens (    0.36 ms per token,  2751.45 tokens per second)
0.00.041.078 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.041.079 I llama_perf_context_print:       total time =       3.97 ms /    10 tokens
0.00.041.236 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.030s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.176 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.880 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.670 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.674 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.677 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.030.677 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.678 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.030.679 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.030.679 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.030.681 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.030.682 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.030.682 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.030.683 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.030.683 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.030.687 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.030.687 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.030.688 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.030.688 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.689 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.038.338 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.040.481 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.005 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.045.007 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.007 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.045.008 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.045.008 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.045.008 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.045.008 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.045.009 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.045.009 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.045.009 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.045.010 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.045.010 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.045.010 I llama_model_loader: - type  f32:   41 tensors
0.00.045.011 I llama_model_loader: - type  f16:   29 tensors
0.00.062.412 W llm_load_vocab: empty token at index 5
0.00.066.947 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.068.262 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.068.286 I llm_load_vocab: special tokens cache size = 5
0.00.330.190 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.330.198 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.330.198 I llm_load_print_meta: arch             = jina-bert-v2
0.00.330.199 I llm_load_print_meta: vocab type       = BPE
0.00.330.199 I llm_load_print_meta: n_vocab          = 61056
0.00.330.199 I llm_load_print_meta: n_merges         = 39382
0.00.330.203 I llm_load_print_meta: vocab_only       = 0
0.00.330.203 I llm_load_print_meta: n_ctx_train      = 8192
0.00.330.203 I llm_load_print_meta: n_embd           = 384
0.00.330.203 I llm_load_print_meta: n_layer          = 4
0.00.330.211 I llm_load_print_meta: n_head           = 12
0.00.330.211 I llm_load_print_meta: n_head_kv        = 12
0.00.330.211 I llm_load_print_meta: n_rot            = 32
0.00.330.211 I llm_load_print_meta: n_swa            = 0
0.00.330.212 I llm_load_print_meta: n_embd_head_k    = 32
0.00.330.212 I llm_load_print_meta: n_embd_head_v    = 32
0.00.330.212 I llm_load_print_meta: n_gqa            = 1
0.00.330.213 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.330.213 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.330.215 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.330.217 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.330.217 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.330.217 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.330.217 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.330.218 I llm_load_print_meta: n_ff             = 1536
0.00.330.218 I llm_load_print_meta: n_expert         = 0
0.00.330.218 I llm_load_print_meta: n_expert_used    = 0
0.00.330.218 I llm_load_print_meta: causal attn      = 0
0.00.330.218 I llm_load_print_meta: pooling type     = -1
0.00.330.218 I llm_load_print_meta: rope type        = -1
0.00.330.219 I llm_load_print_meta: rope scaling     = linear
0.00.330.219 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.330.220 I llm_load_print_meta: freq_scale_train = 1
0.00.330.220 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.330.220 I llm_load_print_meta: rope_finetuned   = unknown
0.00.330.220 I llm_load_print_meta: ssm_d_conv       = 0
0.00.330.220 I llm_load_print_meta: ssm_d_inner      = 0
0.00.330.221 I llm_load_print_meta: ssm_d_state      = 0
0.00.330.221 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.330.221 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.330.251 I llm_load_print_meta: model type       = 33M
0.00.330.252 I llm_load_print_meta: model ftype      = F16
0.00.330.253 I llm_load_print_meta: model params     = 32.90 M
0.00.330.253 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.330.253 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.330.253 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.330.254 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.330.254 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.330.256 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.330.256 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.330.256 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.330.257 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.330.257 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.330.257 I llm_load_print_meta: max token length = 45
0.00.331.651 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.331.651 I llm_load_tensors: offloading output layer to GPU
0.00.331.652 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.331.680 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.331.681 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.332.827 I llama_new_context_with_model: n_seq_max     = 1
0.00.332.828 I llama_new_context_with_model: n_ctx         = 8192
0.00.332.828 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.332.828 I llama_new_context_with_model: n_batch       = 2048
0.00.332.829 I llama_new_context_with_model: n_ubatch      = 2048
0.00.332.829 I llama_new_context_with_model: flash_attn    = 0
0.00.332.829 I llama_new_context_with_model: freq_base     = 10000.0
0.00.332.829 I llama_new_context_with_model: freq_scale    = 1
0.00.332.830 I ggml_metal_init: allocating
0.00.332.833 I ggml_metal_init: found device: Apple M4
0.00.332.835 I ggml_metal_init: picking default device: Apple M4
0.00.333.908 I ggml_metal_init: using embedded metal library
0.00.336.374 I ggml_metal_init: GPU name:   Apple M4
0.00.336.375 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.336.375 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.336.376 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.336.376 I ggml_metal_init: simdgroup reduction   = true
0.00.336.376 I ggml_metal_init: simdgroup matrix mul. = true
0.00.336.376 I ggml_metal_init: has bfloat            = true
0.00.336.376 I ggml_metal_init: use bfloat            = true
0.00.336.377 I ggml_metal_init: hasUnifiedMemory      = true
0.00.336.377 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.346.711 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.346.713 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.346.714 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.347.351 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.347.352 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.347.352 I llama_new_context_with_model: graph nodes  = 154
0.00.347.353 I llama_new_context_with_model: graph splits = 2
0.00.347.371 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.359.766 I 
0.00.359.803 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.359.953 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.359.954 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.359.957 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.359.957 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.359.961 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.359.963 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.360.516 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.363.332 I llama_perf_context_print:        load time =     338.88 ms
0.00.363.333 I llama_perf_context_print: prompt eval time =       2.81 ms /    62 tokens (    0.05 ms per token, 22095.51 tokens per second)
0.00.363.334 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.363.334 I llama_perf_context_print:       total time =       3.57 ms /    63 tokens
0.00.363.540 I ggml_metal_free: deallocating

real	0m1.043s
user	0m0.340s
sys	0m0.046s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.142 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.248 I main: llama backend init
0.00.000.254 I main: load the model and apply lora adapter, if any
0.00.052.898 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.064.388 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.064.402 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.064.406 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.064.407 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.064.407 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.064.408 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.064.409 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.064.424 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.064.424 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.064.425 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.064.426 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.064.426 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.064.427 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.064.428 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.064.433 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.064.434 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.064.434 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.072.030 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.074.263 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.081.241 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.081.250 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.081.251 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.081.251 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.081.252 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.081.254 I llama_model_loader: - type  f32:  194 tensors
0.00.081.255 I llama_model_loader: - type  f16:   98 tensors
0.00.121.201 I llm_load_vocab: special tokens cache size = 25
0.00.128.868 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.128.872 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.128.872 I llm_load_print_meta: arch             = gptneox
0.00.128.872 I llm_load_print_meta: vocab type       = BPE
0.00.128.873 I llm_load_print_meta: n_vocab          = 50304
0.00.128.873 I llm_load_print_meta: n_merges         = 50009
0.00.128.873 I llm_load_print_meta: vocab_only       = 0
0.00.128.873 I llm_load_print_meta: n_ctx_train      = 2048
0.00.128.873 I llm_load_print_meta: n_embd           = 2048
0.00.128.873 I llm_load_print_meta: n_layer          = 24
0.00.128.877 I llm_load_print_meta: n_head           = 16
0.00.128.878 I llm_load_print_meta: n_head_kv        = 16
0.00.128.878 I llm_load_print_meta: n_rot            = 32
0.00.128.878 I llm_load_print_meta: n_swa            = 0
0.00.128.878 I llm_load_print_meta: n_embd_head_k    = 128
0.00.128.879 I llm_load_print_meta: n_embd_head_v    = 128
0.00.128.879 I llm_load_print_meta: n_gqa            = 1
0.00.128.880 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.128.881 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.128.881 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.128.882 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.128.882 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.128.882 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.128.884 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.128.885 I llm_load_print_meta: n_ff             = 8192
0.00.128.885 I llm_load_print_meta: n_expert         = 0
0.00.128.886 I llm_load_print_meta: n_expert_used    = 0
0.00.128.886 I llm_load_print_meta: causal attn      = 1
0.00.128.887 I llm_load_print_meta: pooling type     = 0
0.00.128.887 I llm_load_print_meta: rope type        = 2
0.00.128.887 I llm_load_print_meta: rope scaling     = linear
0.00.128.887 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.128.888 I llm_load_print_meta: freq_scale_train = 1
0.00.128.888 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.128.888 I llm_load_print_meta: rope_finetuned   = unknown
0.00.128.888 I llm_load_print_meta: ssm_d_conv       = 0
0.00.128.888 I llm_load_print_meta: ssm_d_inner      = 0
0.00.128.888 I llm_load_print_meta: ssm_d_state      = 0
0.00.128.889 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.128.889 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.128.900 I llm_load_print_meta: model type       = 1.4B
0.00.128.901 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.128.901 I llm_load_print_meta: model params     = 1.41 B
0.00.128.902 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.128.902 I llm_load_print_meta: general.name     = 1.4B
0.00.128.902 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.128.902 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.128.902 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.128.903 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.128.903 I llm_load_print_meta: LF token         = 128 ''
0.00.128.903 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.128.903 I llm_load_print_meta: max token length = 1024
0.00.130.783 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.130.783 I llm_load_tensors: offloading output layer to GPU
0.00.130.783 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.130.801 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.130.802 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.131.761 I llama_new_context_with_model: n_seq_max     = 1
0.00.131.762 I llama_new_context_with_model: n_ctx         = 2048
0.00.131.762 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.131.763 I llama_new_context_with_model: n_batch       = 2048
0.00.131.763 I llama_new_context_with_model: n_ubatch      = 512
0.00.131.763 I llama_new_context_with_model: flash_attn    = 0
0.00.131.763 I llama_new_context_with_model: freq_base     = 10000.0
0.00.131.764 I llama_new_context_with_model: freq_scale    = 1
0.00.131.764 I ggml_metal_init: allocating
0.00.131.767 I ggml_metal_init: found device: Apple M4
0.00.131.769 I ggml_metal_init: picking default device: Apple M4
0.00.132.427 I ggml_metal_init: using embedded metal library
0.00.142.382 I ggml_metal_init: GPU name:   Apple M4
0.00.142.384 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.142.384 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.142.385 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.142.385 I ggml_metal_init: simdgroup reduction   = true
0.00.142.385 I ggml_metal_init: simdgroup matrix mul. = true
0.00.142.385 I ggml_metal_init: has bfloat            = true
0.00.142.385 I ggml_metal_init: use bfloat            = true
0.00.142.386 I ggml_metal_init: hasUnifiedMemory      = true
0.00.142.386 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.186.153 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.186.158 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.186.177 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.187.107 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.187.108 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.187.108 I llama_new_context_with_model: graph nodes  = 967
0.00.187.109 I llama_new_context_with_model: graph splits = 2
0.00.187.124 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.260.487 I main: llama threadpool init, n_threads = 4
0.00.260.520 I 
0.00.260.556 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.260.557 I 
0.00.260.635 I sampler seed: 1234
0.00.260.639 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.260.673 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.260.675 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.260.675 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.106.004 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54157.13 tokens per second)
0.02.106.005 I llama_perf_context_print:        load time =     207.58 ms
0.02.106.005 I llama_perf_context_print: prompt eval time =      37.78 ms /     7 tokens (    5.40 ms per token,   185.29 tokens per second)
0.02.106.006 I llama_perf_context_print:        eval time =    1804.52 ms /    63 runs   (   28.64 ms per token,    34.91 tokens per second)
0.02.106.006 I llama_perf_context_print:       total time =    1845.52 ms /    70 tokens
0.02.106.181 I ggml_metal_free: deallocating

real	0m2.449s
user	0m0.154s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.672 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.195 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.692 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.025.702 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.705 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.706 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.706 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.707 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.708 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.709 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.710 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.711 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.712 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.712 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.713 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.714 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.717 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.717 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.718 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.960 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.738 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.950 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.952 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.953 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.953 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.954 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.954 I llama_model_loader: - type  f32:  194 tensors
0.00.039.955 I llama_model_loader: - type  f16:   98 tensors
0.00.070.115 I llm_load_vocab: special tokens cache size = 25
0.00.076.746 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.076.749 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.076.749 I llm_load_print_meta: arch             = gptneox
0.00.076.749 I llm_load_print_meta: vocab type       = BPE
0.00.076.749 I llm_load_print_meta: n_vocab          = 50304
0.00.076.750 I llm_load_print_meta: n_merges         = 50009
0.00.076.750 I llm_load_print_meta: vocab_only       = 0
0.00.076.750 I llm_load_print_meta: n_ctx_train      = 2048
0.00.076.750 I llm_load_print_meta: n_embd           = 2048
0.00.076.750 I llm_load_print_meta: n_layer          = 24
0.00.076.752 I llm_load_print_meta: n_head           = 16
0.00.076.753 I llm_load_print_meta: n_head_kv        = 16
0.00.076.753 I llm_load_print_meta: n_rot            = 32
0.00.076.754 I llm_load_print_meta: n_swa            = 0
0.00.076.754 I llm_load_print_meta: n_embd_head_k    = 128
0.00.076.754 I llm_load_print_meta: n_embd_head_v    = 128
0.00.076.755 I llm_load_print_meta: n_gqa            = 1
0.00.076.755 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.076.756 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.076.756 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.076.757 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.076.757 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.076.757 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.076.757 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.076.758 I llm_load_print_meta: n_ff             = 8192
0.00.076.758 I llm_load_print_meta: n_expert         = 0
0.00.076.758 I llm_load_print_meta: n_expert_used    = 0
0.00.076.758 I llm_load_print_meta: causal attn      = 1
0.00.076.758 I llm_load_print_meta: pooling type     = 0
0.00.076.758 I llm_load_print_meta: rope type        = 2
0.00.076.759 I llm_load_print_meta: rope scaling     = linear
0.00.076.759 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.076.759 I llm_load_print_meta: freq_scale_train = 1
0.00.076.759 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.076.760 I llm_load_print_meta: rope_finetuned   = unknown
0.00.076.760 I llm_load_print_meta: ssm_d_conv       = 0
0.00.076.760 I llm_load_print_meta: ssm_d_inner      = 0
0.00.076.760 I llm_load_print_meta: ssm_d_state      = 0
0.00.076.760 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.076.760 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.076.772 I llm_load_print_meta: model type       = 1.4B
0.00.076.772 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.076.773 I llm_load_print_meta: model params     = 1.41 B
0.00.076.773 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.076.774 I llm_load_print_meta: general.name     = 1.4B
0.00.076.774 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.076.774 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.076.774 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.076.774 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.076.775 I llm_load_print_meta: LF token         = 128 ''
0.00.076.775 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.076.775 I llm_load_print_meta: max token length = 1024
0.00.079.329 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.079.329 I llm_load_tensors: offloading output layer to GPU
0.00.079.330 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.079.339 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.079.340 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.080.267 I llama_new_context_with_model: n_seq_max     = 1
0.00.080.268 I llama_new_context_with_model: n_ctx         = 128
0.00.080.268 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.080.268 I llama_new_context_with_model: n_batch       = 128
0.00.080.268 I llama_new_context_with_model: n_ubatch      = 128
0.00.080.268 I llama_new_context_with_model: flash_attn    = 0
0.00.080.269 I llama_new_context_with_model: freq_base     = 10000.0
0.00.080.269 I llama_new_context_with_model: freq_scale    = 1
0.00.080.269 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.080.270 I ggml_metal_init: allocating
0.00.080.273 I ggml_metal_init: found device: Apple M4
0.00.080.274 I ggml_metal_init: picking default device: Apple M4
0.00.080.849 I ggml_metal_init: using embedded metal library
0.00.082.961 I ggml_metal_init: GPU name:   Apple M4
0.00.082.963 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.082.963 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.082.964 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.082.964 I ggml_metal_init: simdgroup reduction   = true
0.00.082.964 I ggml_metal_init: simdgroup matrix mul. = true
0.00.082.964 I ggml_metal_init: has bfloat            = true
0.00.082.964 I ggml_metal_init: use bfloat            = true
0.00.082.965 I ggml_metal_init: hasUnifiedMemory      = true
0.00.082.965 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.258 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.092.260 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.092.274 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.066 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.093.067 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.093.067 I llama_new_context_with_model: graph nodes  = 967
0.00.093.067 I llama_new_context_with_model: graph splits = 2
0.00.093.075 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.858.756 I 
0.00.858.843 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.858.855 I perplexity: tokenizing the input ..
0.00.872.684 I perplexity: tokenization took 13.823 ms
0.00.872.691 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.993.141 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.00.994.775 I Final estimate: PPL = 10.1498 +/- 3.22650

0.00.994.820 I llama_perf_context_print:        load time =     841.54 ms
0.00.994.822 I llama_perf_context_print: prompt eval time =     119.51 ms /   128 tokens (    0.93 ms per token,  1071.01 tokens per second)
0.00.994.823 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.994.825 I llama_perf_context_print:       total time =     136.08 ms /   129 tokens
0.00.995.523 I ggml_metal_free: deallocating

real	0m1.189s
user	0m0.116s
sys	0m0.210s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.507 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.182 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.188 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.191 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.192 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.192 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.193 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.193 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.194 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.194 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.195 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.195 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.195 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.196 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.196 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.198 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.199 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.199 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.371 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.481 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.911 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.913 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.913 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.913 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.914 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.914 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.915 I llama_model_loader: - type  f32:  194 tensors
0.00.036.915 I llama_model_loader: - type q8_0:   98 tensors
0.00.062.233 I llm_load_vocab: special tokens cache size = 25
0.00.069.349 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.353 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.353 I llm_load_print_meta: arch             = gptneox
0.00.069.353 I llm_load_print_meta: vocab type       = BPE
0.00.069.354 I llm_load_print_meta: n_vocab          = 50304
0.00.069.354 I llm_load_print_meta: n_merges         = 50009
0.00.069.354 I llm_load_print_meta: vocab_only       = 0
0.00.069.354 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.355 I llm_load_print_meta: n_embd           = 2048
0.00.069.355 I llm_load_print_meta: n_layer          = 24
0.00.069.360 I llm_load_print_meta: n_head           = 16
0.00.069.360 I llm_load_print_meta: n_head_kv        = 16
0.00.069.361 I llm_load_print_meta: n_rot            = 32
0.00.069.361 I llm_load_print_meta: n_swa            = 0
0.00.069.361 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.361 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.362 I llm_load_print_meta: n_gqa            = 1
0.00.069.363 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.363 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.364 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.364 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.368 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.368 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.369 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.369 I llm_load_print_meta: n_ff             = 8192
0.00.069.369 I llm_load_print_meta: n_expert         = 0
0.00.069.369 I llm_load_print_meta: n_expert_used    = 0
0.00.069.370 I llm_load_print_meta: causal attn      = 1
0.00.069.370 I llm_load_print_meta: pooling type     = 0
0.00.069.370 I llm_load_print_meta: rope type        = 2
0.00.069.370 I llm_load_print_meta: rope scaling     = linear
0.00.069.371 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.371 I llm_load_print_meta: freq_scale_train = 1
0.00.069.371 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.371 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.371 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.371 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.372 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.372 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.372 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.385 I llm_load_print_meta: model type       = 1.4B
0.00.069.385 I llm_load_print_meta: model ftype      = Q8_0
0.00.069.386 I llm_load_print_meta: model params     = 1.41 B
0.00.069.386 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.069.386 I llm_load_print_meta: general.name     = 1.4B
0.00.069.387 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.387 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.387 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.387 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.387 I llm_load_print_meta: LF token         = 128 ''
0.00.069.388 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.389 I llm_load_print_meta: max token length = 1024
0.00.071.263 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.263 I llm_load_tensors: offloading output layer to GPU
0.00.071.263 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.273 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.274 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.072.257 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.258 I llama_new_context_with_model: n_ctx         = 2048
0.00.072.258 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.072.258 I llama_new_context_with_model: n_batch       = 2048
0.00.072.258 I llama_new_context_with_model: n_ubatch      = 512
0.00.072.259 I llama_new_context_with_model: flash_attn    = 0
0.00.072.259 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.259 I llama_new_context_with_model: freq_scale    = 1
0.00.072.259 I ggml_metal_init: allocating
0.00.072.263 I ggml_metal_init: found device: Apple M4
0.00.072.265 I ggml_metal_init: picking default device: Apple M4
0.00.072.991 I ggml_metal_init: using embedded metal library
0.00.075.427 I ggml_metal_init: GPU name:   Apple M4
0.00.075.428 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.429 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.429 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.429 I ggml_metal_init: simdgroup reduction   = true
0.00.075.430 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.430 I ggml_metal_init: has bfloat            = true
0.00.075.430 I ggml_metal_init: use bfloat            = true
0.00.075.430 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.431 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.021 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.109.039 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.109.066 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.172 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.110.175 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.110.176 I llama_new_context_with_model: graph nodes  = 967
0.00.110.176 I llama_new_context_with_model: graph splits = 2
0.00.110.192 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.393.274 I main: llama threadpool init, n_threads = 4
0.01.393.312 I 
0.01.393.336 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.393.337 I 
0.01.393.572 I sampler seed: 1234
0.01.393.576 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.393.624 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.393.628 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.393.629 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.485.293 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58872.31 tokens per second)
0.02.485.293 I llama_perf_context_print:        load time =    1383.76 ms
0.02.485.294 I llama_perf_context_print: prompt eval time =      37.37 ms /     7 tokens (    5.34 ms per token,   187.34 tokens per second)
0.02.485.295 I llama_perf_context_print:        eval time =    1051.30 ms /    63 runs   (   16.69 ms per token,    59.93 tokens per second)
0.02.485.295 I llama_perf_context_print:       total time =    1092.02 ms /    70 tokens
0.02.485.465 I ggml_metal_free: deallocating

real	0m2.503s
user	0m0.119s
sys	0m0.241s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.116 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.311 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.048 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.053 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.054 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.055 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.055 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.056 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.056 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.057 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.057 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.058 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.058 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.058 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.059 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.060 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.062 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.062 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.062 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.432 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.797 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.073 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.075 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.075 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.076 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.076 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.076 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.030.077 I llama_model_loader: - type  f32:  194 tensors
0.00.030.077 I llama_model_loader: - type q8_0:   98 tensors
0.00.053.008 I llm_load_vocab: special tokens cache size = 25
0.00.058.930 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.932 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.932 I llm_load_print_meta: arch             = gptneox
0.00.058.933 I llm_load_print_meta: vocab type       = BPE
0.00.058.933 I llm_load_print_meta: n_vocab          = 50304
0.00.058.933 I llm_load_print_meta: n_merges         = 50009
0.00.058.933 I llm_load_print_meta: vocab_only       = 0
0.00.058.933 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.933 I llm_load_print_meta: n_embd           = 2048
0.00.058.934 I llm_load_print_meta: n_layer          = 24
0.00.058.937 I llm_load_print_meta: n_head           = 16
0.00.058.938 I llm_load_print_meta: n_head_kv        = 16
0.00.058.938 I llm_load_print_meta: n_rot            = 32
0.00.058.938 I llm_load_print_meta: n_swa            = 0
0.00.058.940 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.940 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.941 I llm_load_print_meta: n_gqa            = 1
0.00.058.941 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.942 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.942 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.943 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.943 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.943 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.943 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.944 I llm_load_print_meta: n_ff             = 8192
0.00.058.944 I llm_load_print_meta: n_expert         = 0
0.00.058.944 I llm_load_print_meta: n_expert_used    = 0
0.00.058.945 I llm_load_print_meta: causal attn      = 1
0.00.058.945 I llm_load_print_meta: pooling type     = 0
0.00.058.945 I llm_load_print_meta: rope type        = 2
0.00.058.945 I llm_load_print_meta: rope scaling     = linear
0.00.058.946 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.946 I llm_load_print_meta: freq_scale_train = 1
0.00.058.946 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.946 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.947 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.947 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.948 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.948 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.948 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.960 I llm_load_print_meta: model type       = 1.4B
0.00.058.960 I llm_load_print_meta: model ftype      = Q8_0
0.00.058.961 I llm_load_print_meta: model params     = 1.41 B
0.00.058.961 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.058.962 I llm_load_print_meta: general.name     = 1.4B
0.00.058.962 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.962 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.962 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.963 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.963 I llm_load_print_meta: LF token         = 128 ''
0.00.058.963 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.963 I llm_load_print_meta: max token length = 1024
0.00.061.125 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.125 I llm_load_tensors: offloading output layer to GPU
0.00.061.125 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.135 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.061.136 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.062.074 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.075 I llama_new_context_with_model: n_ctx         = 128
0.00.062.075 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.062.075 I llama_new_context_with_model: n_batch       = 128
0.00.062.075 I llama_new_context_with_model: n_ubatch      = 128
0.00.062.075 I llama_new_context_with_model: flash_attn    = 0
0.00.062.076 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.076 I llama_new_context_with_model: freq_scale    = 1
0.00.062.077 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.062.077 I ggml_metal_init: allocating
0.00.062.080 I ggml_metal_init: found device: Apple M4
0.00.062.082 I ggml_metal_init: picking default device: Apple M4
0.00.062.633 I ggml_metal_init: using embedded metal library
0.00.064.522 I ggml_metal_init: GPU name:   Apple M4
0.00.064.523 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.524 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.524 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.524 I ggml_metal_init: simdgroup reduction   = true
0.00.064.525 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.525 I ggml_metal_init: has bfloat            = true
0.00.064.525 I ggml_metal_init: use bfloat            = true
0.00.064.525 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.526 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.473 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.073.475 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.073.489 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.074.362 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.074.363 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.074.363 I llama_new_context_with_model: graph nodes  = 967
0.00.074.364 I llama_new_context_with_model: graph splits = 2
0.00.074.376 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.865.588 I 
0.00.865.619 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.865.627 I perplexity: tokenizing the input ..
0.00.873.427 I perplexity: tokenization took 7.798 ms
0.00.873.430 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.994.715 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.995.942 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.995.964 I llama_perf_context_print:        load time =     855.27 ms
0.00.995.965 I llama_perf_context_print: prompt eval time =     121.06 ms /   128 tokens (    0.95 ms per token,  1057.34 tokens per second)
0.00.995.966 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.995.966 I llama_perf_context_print:       total time =     130.38 ms /   129 tokens
0.00.996.346 I ggml_metal_free: deallocating

real	0m1.013s
user	0m0.086s
sys	0m0.167s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.010.941 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.933 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.937 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.941 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.942 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.942 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.942 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.943 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.944 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.944 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.945 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.945 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.945 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.946 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.946 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.947 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.948 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.948 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.971 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.035 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.071 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.072 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.072 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.073 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.073 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.073 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.074 I llama_model_loader: - type  f32:  194 tensors
0.00.028.074 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.075 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.458 I llm_load_vocab: special tokens cache size = 25
0.00.054.378 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.381 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.382 I llm_load_print_meta: arch             = gptneox
0.00.054.382 I llm_load_print_meta: vocab type       = BPE
0.00.054.382 I llm_load_print_meta: n_vocab          = 50304
0.00.054.383 I llm_load_print_meta: n_merges         = 50009
0.00.054.383 I llm_load_print_meta: vocab_only       = 0
0.00.054.383 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.383 I llm_load_print_meta: n_embd           = 2048
0.00.054.383 I llm_load_print_meta: n_layer          = 24
0.00.054.386 I llm_load_print_meta: n_head           = 16
0.00.054.387 I llm_load_print_meta: n_head_kv        = 16
0.00.054.388 I llm_load_print_meta: n_rot            = 32
0.00.054.388 I llm_load_print_meta: n_swa            = 0
0.00.054.388 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.388 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.392 I llm_load_print_meta: n_gqa            = 1
0.00.054.393 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.393 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.394 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.394 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.394 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.395 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.395 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.396 I llm_load_print_meta: n_ff             = 8192
0.00.054.396 I llm_load_print_meta: n_expert         = 0
0.00.054.396 I llm_load_print_meta: n_expert_used    = 0
0.00.054.397 I llm_load_print_meta: causal attn      = 1
0.00.054.397 I llm_load_print_meta: pooling type     = 0
0.00.054.397 I llm_load_print_meta: rope type        = 2
0.00.054.398 I llm_load_print_meta: rope scaling     = linear
0.00.054.399 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.399 I llm_load_print_meta: freq_scale_train = 1
0.00.054.400 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.400 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.400 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.400 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.400 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.400 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.400 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.413 I llm_load_print_meta: model type       = 1.4B
0.00.054.413 I llm_load_print_meta: model ftype      = Q4_0
0.00.054.417 I llm_load_print_meta: model params     = 1.41 B
0.00.054.418 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.054.418 I llm_load_print_meta: general.name     = 1.4B
0.00.054.418 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.419 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.420 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.420 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.420 I llm_load_print_meta: LF token         = 128 ''
0.00.054.420 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.420 I llm_load_print_meta: max token length = 1024
0.00.056.674 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.674 I llm_load_tensors: offloading output layer to GPU
0.00.056.675 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.686 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.056.687 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.057.689 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.689 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.690 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.690 I llama_new_context_with_model: n_batch       = 2048
0.00.057.690 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.690 I llama_new_context_with_model: flash_attn    = 0
0.00.057.690 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.691 I llama_new_context_with_model: freq_scale    = 1
0.00.057.691 I ggml_metal_init: allocating
0.00.057.694 I ggml_metal_init: found device: Apple M4
0.00.057.696 I ggml_metal_init: picking default device: Apple M4
0.00.058.402 I ggml_metal_init: using embedded metal library
0.00.060.474 I ggml_metal_init: GPU name:   Apple M4
0.00.060.475 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.475 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.476 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.476 I ggml_metal_init: simdgroup reduction   = true
0.00.060.476 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.476 I ggml_metal_init: has bfloat            = true
0.00.060.476 I ggml_metal_init: use bfloat            = true
0.00.060.477 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.477 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.935 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.946 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.970 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.055 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.058 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.058 I llama_new_context_with_model: graph nodes  = 967
0.00.095.059 I llama_new_context_with_model: graph splits = 2
0.00.095.070 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.664.265 I main: llama threadpool init, n_threads = 4
0.00.664.302 I 
0.00.664.332 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.664.336 I 
0.00.664.567 I sampler seed: 1234
0.00.664.571 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.664.612 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.664.617 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.664.617 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.342.445 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56891.03 tokens per second)
0.01.342.446 I llama_perf_context_print:        load time =     653.32 ms
0.01.342.447 I llama_perf_context_print: prompt eval time =      37.55 ms /     7 tokens (    5.36 ms per token,   186.43 tokens per second)
0.01.342.447 I llama_perf_context_print:        eval time =     637.21 ms /    63 runs   (   10.11 ms per token,    98.87 tokens per second)
0.01.342.448 I llama_perf_context_print:       total time =     678.18 ms /    70 tokens
0.01.342.618 I ggml_metal_free: deallocating

real	0m1.362s
user	0m0.112s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.816 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.597 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.602 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.603 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.604 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.604 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.604 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.605 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.606 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.606 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.606 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.607 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.607 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.607 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.608 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.609 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.610 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.610 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.634 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.680 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.792 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.793 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.793 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.794 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.794 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.794 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.795 I llama_model_loader: - type  f32:  194 tensors
0.00.024.795 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.795 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.481 I llm_load_vocab: special tokens cache size = 25
0.00.051.402 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.405 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.405 I llm_load_print_meta: arch             = gptneox
0.00.051.405 I llm_load_print_meta: vocab type       = BPE
0.00.051.406 I llm_load_print_meta: n_vocab          = 50304
0.00.051.406 I llm_load_print_meta: n_merges         = 50009
0.00.051.406 I llm_load_print_meta: vocab_only       = 0
0.00.051.406 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.406 I llm_load_print_meta: n_embd           = 2048
0.00.051.407 I llm_load_print_meta: n_layer          = 24
0.00.051.410 I llm_load_print_meta: n_head           = 16
0.00.051.411 I llm_load_print_meta: n_head_kv        = 16
0.00.051.411 I llm_load_print_meta: n_rot            = 32
0.00.051.411 I llm_load_print_meta: n_swa            = 0
0.00.051.412 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.412 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.412 I llm_load_print_meta: n_gqa            = 1
0.00.051.413 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.414 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.415 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.415 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.415 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.415 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.415 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.416 I llm_load_print_meta: n_ff             = 8192
0.00.051.416 I llm_load_print_meta: n_expert         = 0
0.00.051.416 I llm_load_print_meta: n_expert_used    = 0
0.00.051.417 I llm_load_print_meta: causal attn      = 1
0.00.051.417 I llm_load_print_meta: pooling type     = 0
0.00.051.417 I llm_load_print_meta: rope type        = 2
0.00.051.417 I llm_load_print_meta: rope scaling     = linear
0.00.051.418 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.418 I llm_load_print_meta: freq_scale_train = 1
0.00.051.418 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.418 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.420 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.420 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.420 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.422 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.422 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.434 I llm_load_print_meta: model type       = 1.4B
0.00.051.434 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.435 I llm_load_print_meta: model params     = 1.41 B
0.00.051.435 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.435 I llm_load_print_meta: general.name     = 1.4B
0.00.051.435 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.436 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.436 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.436 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.436 I llm_load_print_meta: LF token         = 128 ''
0.00.051.436 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.437 I llm_load_print_meta: max token length = 1024
0.00.053.371 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.371 I llm_load_tensors: offloading output layer to GPU
0.00.053.371 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.382 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.383 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.334 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.335 I llama_new_context_with_model: n_ctx         = 128
0.00.054.335 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.335 I llama_new_context_with_model: n_batch       = 128
0.00.054.335 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.335 I llama_new_context_with_model: flash_attn    = 0
0.00.054.336 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.336 I llama_new_context_with_model: freq_scale    = 1
0.00.054.336 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.337 I ggml_metal_init: allocating
0.00.054.342 I ggml_metal_init: found device: Apple M4
0.00.054.344 I ggml_metal_init: picking default device: Apple M4
0.00.054.857 I ggml_metal_init: using embedded metal library
0.00.056.796 I ggml_metal_init: GPU name:   Apple M4
0.00.056.798 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.798 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.798 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.799 I ggml_metal_init: simdgroup reduction   = true
0.00.056.799 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.799 I ggml_metal_init: has bfloat            = true
0.00.056.799 I ggml_metal_init: use bfloat            = true
0.00.056.800 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.800 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.772 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.774 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.787 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.645 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.646 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.647 I llama_new_context_with_model: graph nodes  = 967
0.00.066.647 I llama_new_context_with_model: graph splits = 2
0.00.066.659 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.606.378 I 
0.00.606.438 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.606.442 I perplexity: tokenizing the input ..
0.00.614.157 I perplexity: tokenization took 7.711 ms
0.00.614.161 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.736.346 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.737.454 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.737.471 I llama_perf_context_print:        load time =     596.56 ms
0.00.737.472 I llama_perf_context_print: prompt eval time =     121.96 ms /   128 tokens (    0.95 ms per token,  1049.52 tokens per second)
0.00.737.473 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.737.474 I llama_perf_context_print:       total time =     131.10 ms /   129 tokens
0.00.737.784 I ggml_metal_free: deallocating

real	0m0.754s
user	0m0.077s
sys	0m0.110s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.804 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.398 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.402 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.403 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.409 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.409 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.410 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.410 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.411 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.411 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.412 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.412 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.412 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.413 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.413 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.415 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.415 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.456 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.482 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.393 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.394 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.395 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.395 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.395 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.395 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.396 I llama_model_loader: - type  f32:  194 tensors
0.00.025.396 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.397 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.451 I llm_load_vocab: special tokens cache size = 25
0.00.052.259 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.262 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.262 I llm_load_print_meta: arch             = gptneox
0.00.052.263 I llm_load_print_meta: vocab type       = BPE
0.00.052.263 I llm_load_print_meta: n_vocab          = 50304
0.00.052.263 I llm_load_print_meta: n_merges         = 50009
0.00.052.263 I llm_load_print_meta: vocab_only       = 0
0.00.052.264 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.264 I llm_load_print_meta: n_embd           = 2048
0.00.052.264 I llm_load_print_meta: n_layer          = 24
0.00.052.267 I llm_load_print_meta: n_head           = 16
0.00.052.268 I llm_load_print_meta: n_head_kv        = 16
0.00.052.268 I llm_load_print_meta: n_rot            = 32
0.00.052.268 I llm_load_print_meta: n_swa            = 0
0.00.052.268 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.269 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.269 I llm_load_print_meta: n_gqa            = 1
0.00.052.270 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.271 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.271 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.272 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.272 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.272 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.272 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.273 I llm_load_print_meta: n_ff             = 8192
0.00.052.273 I llm_load_print_meta: n_expert         = 0
0.00.052.273 I llm_load_print_meta: n_expert_used    = 0
0.00.052.273 I llm_load_print_meta: causal attn      = 1
0.00.052.274 I llm_load_print_meta: pooling type     = 0
0.00.052.274 I llm_load_print_meta: rope type        = 2
0.00.052.274 I llm_load_print_meta: rope scaling     = linear
0.00.052.274 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.275 I llm_load_print_meta: freq_scale_train = 1
0.00.052.275 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.275 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.275 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.275 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.277 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.277 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.277 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.284 I llm_load_print_meta: model type       = 1.4B
0.00.052.284 I llm_load_print_meta: model ftype      = Q4_1
0.00.052.285 I llm_load_print_meta: model params     = 1.41 B
0.00.052.285 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.052.285 I llm_load_print_meta: general.name     = 1.4B
0.00.052.286 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.286 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.286 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.286 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.286 I llm_load_print_meta: LF token         = 128 ''
0.00.052.287 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.287 I llm_load_print_meta: max token length = 1024
0.00.054.034 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.034 I llm_load_tensors: offloading output layer to GPU
0.00.054.034 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.039 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.039 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.959 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.960 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.960 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.960 I llama_new_context_with_model: n_batch       = 2048
0.00.054.960 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.960 I llama_new_context_with_model: flash_attn    = 0
0.00.054.961 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.961 I llama_new_context_with_model: freq_scale    = 1
0.00.054.961 I ggml_metal_init: allocating
0.00.054.964 I ggml_metal_init: found device: Apple M4
0.00.054.966 I ggml_metal_init: picking default device: Apple M4
0.00.055.505 I ggml_metal_init: using embedded metal library
0.00.057.392 I ggml_metal_init: GPU name:   Apple M4
0.00.057.393 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.393 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.393 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.394 I ggml_metal_init: simdgroup reduction   = true
0.00.057.394 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.394 I ggml_metal_init: has bfloat            = true
0.00.057.394 I ggml_metal_init: use bfloat            = true
0.00.057.395 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.396 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.856 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.867 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.886 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.974 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.975 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.976 I llama_new_context_with_model: graph nodes  = 967
0.00.086.976 I llama_new_context_with_model: graph splits = 2
0.00.086.985 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.722.559 I main: llama threadpool init, n_threads = 4
0.00.722.593 I 
0.00.722.619 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.722.619 I 
0.00.722.826 I sampler seed: 1234
0.00.722.830 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.722.871 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.722.875 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.722.876 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.447.119 I llama_perf_sampler_print:    sampling time =       1.06 ms /    71 runs   (    0.01 ms per token, 66981.13 tokens per second)
0.01.447.120 I llama_perf_context_print:        load time =     712.75 ms
0.01.447.121 I llama_perf_context_print: prompt eval time =      39.96 ms /     7 tokens (    5.71 ms per token,   175.18 tokens per second)
0.01.447.121 I llama_perf_context_print:        eval time =     681.50 ms /    63 runs   (   10.82 ms per token,    92.44 tokens per second)
0.01.447.122 I llama_perf_context_print:       total time =     724.56 ms /    70 tokens
0.01.447.299 I ggml_metal_free: deallocating

real	0m1.466s
user	0m0.109s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.583 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.676 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.681 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.686 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.687 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.687 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.688 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.688 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.689 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.689 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.690 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.690 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.690 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.690 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.691 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.692 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.693 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.693 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.619 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.676 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.592 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.593 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.593 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.593 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.594 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.594 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.594 I llama_model_loader: - type  f32:  194 tensors
0.00.023.595 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.595 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.558 I llm_load_vocab: special tokens cache size = 25
0.00.049.360 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.363 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.363 I llm_load_print_meta: arch             = gptneox
0.00.049.363 I llm_load_print_meta: vocab type       = BPE
0.00.049.364 I llm_load_print_meta: n_vocab          = 50304
0.00.049.364 I llm_load_print_meta: n_merges         = 50009
0.00.049.364 I llm_load_print_meta: vocab_only       = 0
0.00.049.364 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.364 I llm_load_print_meta: n_embd           = 2048
0.00.049.364 I llm_load_print_meta: n_layer          = 24
0.00.049.367 I llm_load_print_meta: n_head           = 16
0.00.049.368 I llm_load_print_meta: n_head_kv        = 16
0.00.049.368 I llm_load_print_meta: n_rot            = 32
0.00.049.368 I llm_load_print_meta: n_swa            = 0
0.00.049.368 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.369 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.369 I llm_load_print_meta: n_gqa            = 1
0.00.049.370 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.371 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.371 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.372 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.372 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.372 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.372 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.373 I llm_load_print_meta: n_ff             = 8192
0.00.049.373 I llm_load_print_meta: n_expert         = 0
0.00.049.373 I llm_load_print_meta: n_expert_used    = 0
0.00.049.374 I llm_load_print_meta: causal attn      = 1
0.00.049.374 I llm_load_print_meta: pooling type     = 0
0.00.049.374 I llm_load_print_meta: rope type        = 2
0.00.049.374 I llm_load_print_meta: rope scaling     = linear
0.00.049.374 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.375 I llm_load_print_meta: freq_scale_train = 1
0.00.049.375 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.375 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.375 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.375 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.375 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.376 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.377 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.389 I llm_load_print_meta: model type       = 1.4B
0.00.049.390 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.390 I llm_load_print_meta: model params     = 1.41 B
0.00.049.390 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.391 I llm_load_print_meta: general.name     = 1.4B
0.00.049.391 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.391 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.391 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.391 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.392 I llm_load_print_meta: LF token         = 128 ''
0.00.049.392 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.392 I llm_load_print_meta: max token length = 1024
0.00.050.894 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.894 I llm_load_tensors: offloading output layer to GPU
0.00.050.895 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.904 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.905 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.730 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.731 I llama_new_context_with_model: n_ctx         = 128
0.00.051.731 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.731 I llama_new_context_with_model: n_batch       = 128
0.00.051.731 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.731 I llama_new_context_with_model: flash_attn    = 0
0.00.051.732 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.732 I llama_new_context_with_model: freq_scale    = 1
0.00.051.732 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.733 I ggml_metal_init: allocating
0.00.051.739 I ggml_metal_init: found device: Apple M4
0.00.051.741 I ggml_metal_init: picking default device: Apple M4
0.00.052.308 I ggml_metal_init: using embedded metal library
0.00.054.262 I ggml_metal_init: GPU name:   Apple M4
0.00.054.264 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.264 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.265 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.265 I ggml_metal_init: simdgroup reduction   = true
0.00.054.265 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.265 I ggml_metal_init: has bfloat            = true
0.00.054.265 I ggml_metal_init: use bfloat            = true
0.00.054.266 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.267 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.350 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.352 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.369 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.241 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.242 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.243 I llama_new_context_with_model: graph nodes  = 967
0.00.064.243 I llama_new_context_with_model: graph splits = 2
0.00.064.255 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.606 I 
0.00.659.642 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.659.648 I perplexity: tokenizing the input ..
0.00.667.435 I perplexity: tokenization took 7.785 ms
0.00.667.438 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.790.140 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.791.311 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.791.333 I llama_perf_context_print:        load time =     651.01 ms
0.00.791.334 I llama_perf_context_print: prompt eval time =     122.48 ms /   128 tokens (    0.96 ms per token,  1045.09 tokens per second)
0.00.791.335 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.335 I llama_perf_context_print:       total time =     131.73 ms /   129 tokens
0.00.791.763 I ggml_metal_free: deallocating

real	0m0.804s
user	0m0.076s
sys	0m0.109s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.008.479 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.299 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.303 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.309 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.309 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.310 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.310 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.310 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.311 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.312 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.312 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.312 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.313 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.313 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.313 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.315 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.315 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.315 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.301 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.337 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.232 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.233 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.233 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.234 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.234 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.234 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.235 I llama_model_loader: - type  f32:  194 tensors
0.00.024.235 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.236 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.260 I llm_load_vocab: special tokens cache size = 25
0.00.049.980 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.982 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.983 I llm_load_print_meta: arch             = gptneox
0.00.049.983 I llm_load_print_meta: vocab type       = BPE
0.00.049.983 I llm_load_print_meta: n_vocab          = 50304
0.00.049.983 I llm_load_print_meta: n_merges         = 50009
0.00.049.984 I llm_load_print_meta: vocab_only       = 0
0.00.049.984 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.984 I llm_load_print_meta: n_embd           = 2048
0.00.049.984 I llm_load_print_meta: n_layer          = 24
0.00.049.987 I llm_load_print_meta: n_head           = 16
0.00.049.988 I llm_load_print_meta: n_head_kv        = 16
0.00.049.988 I llm_load_print_meta: n_rot            = 32
0.00.049.988 I llm_load_print_meta: n_swa            = 0
0.00.049.988 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.988 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.989 I llm_load_print_meta: n_gqa            = 1
0.00.049.990 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.991 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.991 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.992 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.992 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.992 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.992 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.993 I llm_load_print_meta: n_ff             = 8192
0.00.049.993 I llm_load_print_meta: n_expert         = 0
0.00.049.993 I llm_load_print_meta: n_expert_used    = 0
0.00.049.993 I llm_load_print_meta: causal attn      = 1
0.00.049.994 I llm_load_print_meta: pooling type     = 0
0.00.049.995 I llm_load_print_meta: rope type        = 2
0.00.049.995 I llm_load_print_meta: rope scaling     = linear
0.00.049.997 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.997 I llm_load_print_meta: freq_scale_train = 1
0.00.049.997 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.997 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.998 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.998 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.998 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.998 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.998 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.010 I llm_load_print_meta: model type       = 1.4B
0.00.050.010 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.011 I llm_load_print_meta: model params     = 1.41 B
0.00.050.011 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.011 I llm_load_print_meta: general.name     = 1.4B
0.00.050.012 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.012 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.012 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.012 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.012 I llm_load_print_meta: LF token         = 128 ''
0.00.050.013 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.013 I llm_load_print_meta: max token length = 1024
0.00.052.022 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.022 I llm_load_tensors: offloading output layer to GPU
0.00.052.022 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.032 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.034 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.920 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.920 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.921 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.921 I llama_new_context_with_model: n_batch       = 2048
0.00.052.921 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.921 I llama_new_context_with_model: flash_attn    = 0
0.00.052.922 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.922 I llama_new_context_with_model: freq_scale    = 1
0.00.052.922 I ggml_metal_init: allocating
0.00.052.928 I ggml_metal_init: found device: Apple M4
0.00.052.930 I ggml_metal_init: picking default device: Apple M4
0.00.053.468 I ggml_metal_init: using embedded metal library
0.00.055.407 I ggml_metal_init: GPU name:   Apple M4
0.00.055.408 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.409 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.409 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.409 I ggml_metal_init: simdgroup reduction   = true
0.00.055.409 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.410 I ggml_metal_init: has bfloat            = true
0.00.055.410 I ggml_metal_init: use bfloat            = true
0.00.055.410 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.411 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.878 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.081.888 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.081.909 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.082.845 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.082.846 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.082.846 I llama_new_context_with_model: graph nodes  = 967
0.00.082.846 I llama_new_context_with_model: graph splits = 2
0.00.082.860 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.227 I main: llama threadpool init, n_threads = 4
0.00.717.266 I 
0.00.717.315 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.717.316 I 
0.00.717.540 I sampler seed: 1234
0.00.717.544 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.717.578 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.717.580 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.717.580 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.504.043 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58532.56 tokens per second)
0.01.504.044 I llama_perf_context_print:        load time =     708.74 ms
0.01.504.044 I llama_perf_context_print: prompt eval time =      36.55 ms /     7 tokens (    5.22 ms per token,   191.50 tokens per second)
0.01.504.045 I llama_perf_context_print:        eval time =     746.94 ms /    63 runs   (   11.86 ms per token,    84.34 tokens per second)
0.01.504.046 I llama_perf_context_print:       total time =     786.82 ms /    70 tokens
0.01.504.225 I ggml_metal_free: deallocating

real	0m1.521s
user	0m0.107s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.778 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.531 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.535 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.536 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.537 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.537 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.541 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.542 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.543 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.543 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.543 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.544 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.544 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.546 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.546 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.548 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.548 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.548 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.348 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.335 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.177 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.178 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.178 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.179 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.179 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.179 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.180 I llama_model_loader: - type  f32:  194 tensors
0.00.025.180 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.180 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.032 I llm_load_vocab: special tokens cache size = 25
0.00.050.845 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.847 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.848 I llm_load_print_meta: arch             = gptneox
0.00.050.848 I llm_load_print_meta: vocab type       = BPE
0.00.050.848 I llm_load_print_meta: n_vocab          = 50304
0.00.050.848 I llm_load_print_meta: n_merges         = 50009
0.00.050.849 I llm_load_print_meta: vocab_only       = 0
0.00.050.849 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.849 I llm_load_print_meta: n_embd           = 2048
0.00.050.849 I llm_load_print_meta: n_layer          = 24
0.00.050.852 I llm_load_print_meta: n_head           = 16
0.00.050.853 I llm_load_print_meta: n_head_kv        = 16
0.00.050.853 I llm_load_print_meta: n_rot            = 32
0.00.050.856 I llm_load_print_meta: n_swa            = 0
0.00.050.856 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.856 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.857 I llm_load_print_meta: n_gqa            = 1
0.00.050.858 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.858 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.859 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.859 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.859 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.860 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.860 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.862 I llm_load_print_meta: n_ff             = 8192
0.00.050.862 I llm_load_print_meta: n_expert         = 0
0.00.050.862 I llm_load_print_meta: n_expert_used    = 0
0.00.050.862 I llm_load_print_meta: causal attn      = 1
0.00.050.862 I llm_load_print_meta: pooling type     = 0
0.00.050.863 I llm_load_print_meta: rope type        = 2
0.00.050.863 I llm_load_print_meta: rope scaling     = linear
0.00.050.863 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.863 I llm_load_print_meta: freq_scale_train = 1
0.00.050.864 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.864 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.864 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.864 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.864 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.864 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.864 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.876 I llm_load_print_meta: model type       = 1.4B
0.00.050.876 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.877 I llm_load_print_meta: model params     = 1.41 B
0.00.050.877 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.877 I llm_load_print_meta: general.name     = 1.4B
0.00.050.878 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.878 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.878 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.878 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.878 I llm_load_print_meta: LF token         = 128 ''
0.00.050.879 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.879 I llm_load_print_meta: max token length = 1024
0.00.052.470 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.470 I llm_load_tensors: offloading output layer to GPU
0.00.052.471 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.480 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.481 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.319 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.319 I llama_new_context_with_model: n_ctx         = 128
0.00.053.320 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.320 I llama_new_context_with_model: n_batch       = 128
0.00.053.320 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.320 I llama_new_context_with_model: flash_attn    = 0
0.00.053.320 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.321 I llama_new_context_with_model: freq_scale    = 1
0.00.053.321 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.321 I ggml_metal_init: allocating
0.00.053.328 I ggml_metal_init: found device: Apple M4
0.00.053.330 I ggml_metal_init: picking default device: Apple M4
0.00.053.869 I ggml_metal_init: using embedded metal library
0.00.055.771 I ggml_metal_init: GPU name:   Apple M4
0.00.055.773 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.773 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.773 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.773 I ggml_metal_init: simdgroup reduction   = true
0.00.055.774 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.774 I ggml_metal_init: has bfloat            = true
0.00.055.774 I ggml_metal_init: use bfloat            = true
0.00.055.774 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.775 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.891 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.893 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.907 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.771 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.772 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.772 I llama_new_context_with_model: graph nodes  = 967
0.00.065.772 I llama_new_context_with_model: graph splits = 2
0.00.065.784 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.673.735 I 
0.00.673.764 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.673.769 I perplexity: tokenizing the input ..
0.00.681.079 I perplexity: tokenization took 7.309 ms
0.00.681.084 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.815.972 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.817.121 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.817.146 I llama_perf_context_print:        load time =     662.95 ms
0.00.817.147 I llama_perf_context_print: prompt eval time =     134.63 ms /   128 tokens (    1.05 ms per token,   950.73 tokens per second)
0.00.817.148 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.817.148 I llama_perf_context_print:       total time =     143.41 ms /   129 tokens
0.00.817.517 I ggml_metal_free: deallocating

real	0m0.834s
user	0m0.077s
sys	0m0.115s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.900 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.975 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.979 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.981 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.981 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.981 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.982 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.982 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.983 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.984 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.985 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.985 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.985 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.986 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.986 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.988 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.989 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.989 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.096 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.156 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.162 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.163 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.164 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.164 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.164 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.165 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.165 I llama_model_loader: - type  f32:  194 tensors
0.00.026.166 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.166 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.320 I llm_load_vocab: special tokens cache size = 25
0.00.052.181 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.183 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.184 I llm_load_print_meta: arch             = gptneox
0.00.052.184 I llm_load_print_meta: vocab type       = BPE
0.00.052.184 I llm_load_print_meta: n_vocab          = 50304
0.00.052.184 I llm_load_print_meta: n_merges         = 50009
0.00.052.185 I llm_load_print_meta: vocab_only       = 0
0.00.052.185 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.185 I llm_load_print_meta: n_embd           = 2048
0.00.052.185 I llm_load_print_meta: n_layer          = 24
0.00.052.188 I llm_load_print_meta: n_head           = 16
0.00.052.189 I llm_load_print_meta: n_head_kv        = 16
0.00.052.189 I llm_load_print_meta: n_rot            = 32
0.00.052.189 I llm_load_print_meta: n_swa            = 0
0.00.052.189 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.192 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.193 I llm_load_print_meta: n_gqa            = 1
0.00.052.194 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.194 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.195 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.196 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.196 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.196 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.196 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.197 I llm_load_print_meta: n_ff             = 8192
0.00.052.197 I llm_load_print_meta: n_expert         = 0
0.00.052.197 I llm_load_print_meta: n_expert_used    = 0
0.00.052.199 I llm_load_print_meta: causal attn      = 1
0.00.052.200 I llm_load_print_meta: pooling type     = 0
0.00.052.200 I llm_load_print_meta: rope type        = 2
0.00.052.200 I llm_load_print_meta: rope scaling     = linear
0.00.052.201 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.201 I llm_load_print_meta: freq_scale_train = 1
0.00.052.201 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.201 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.201 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.202 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.202 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.202 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.202 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.213 I llm_load_print_meta: model type       = 1.4B
0.00.052.214 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.214 I llm_load_print_meta: model params     = 1.41 B
0.00.052.215 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.215 I llm_load_print_meta: general.name     = 1.4B
0.00.052.215 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.215 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.216 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.216 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.218 I llm_load_print_meta: LF token         = 128 ''
0.00.052.218 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.218 I llm_load_print_meta: max token length = 1024
0.00.054.229 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.229 I llm_load_tensors: offloading output layer to GPU
0.00.054.229 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.239 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.240 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.184 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.184 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.185 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.185 I llama_new_context_with_model: n_batch       = 2048
0.00.055.185 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.185 I llama_new_context_with_model: flash_attn    = 0
0.00.055.185 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.186 I llama_new_context_with_model: freq_scale    = 1
0.00.055.186 I ggml_metal_init: allocating
0.00.055.189 I ggml_metal_init: found device: Apple M4
0.00.055.191 I ggml_metal_init: picking default device: Apple M4
0.00.055.757 I ggml_metal_init: using embedded metal library
0.00.057.684 I ggml_metal_init: GPU name:   Apple M4
0.00.057.685 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.686 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.686 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.686 I ggml_metal_init: simdgroup reduction   = true
0.00.057.688 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.688 I ggml_metal_init: has bfloat            = true
0.00.057.688 I ggml_metal_init: use bfloat            = true
0.00.057.688 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.693 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.547 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.556 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.574 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.710 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.711 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.711 I llama_new_context_with_model: graph nodes  = 967
0.00.085.712 I llama_new_context_with_model: graph splits = 2
0.00.085.725 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.793.727 I main: llama threadpool init, n_threads = 4
0.00.793.760 I 
0.00.793.787 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.793.789 I 
0.00.794.022 I sampler seed: 1234
0.00.794.026 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.794.050 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.794.052 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.794.052 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.626.165 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58921.16 tokens per second)
0.01.626.165 I llama_perf_context_print:        load time =     783.82 ms
0.01.626.166 I llama_perf_context_print: prompt eval time =      36.56 ms /     7 tokens (    5.22 ms per token,   191.44 tokens per second)
0.01.626.170 I llama_perf_context_print:        eval time =     792.55 ms /    63 runs   (   12.58 ms per token,    79.49 tokens per second)
0.01.626.170 I llama_perf_context_print:       total time =     832.44 ms /    70 tokens
0.01.626.336 I ggml_metal_free: deallocating

real	0m1.644s
user	0m0.109s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.099 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.163 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.167 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.169 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.169 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.169 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.170 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.170 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.171 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.171 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.172 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.172 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.172 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.173 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.173 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.175 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.176 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.176 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.433 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.469 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.636 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.637 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.637 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.637 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.638 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.638 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.639 I llama_model_loader: - type  f32:  194 tensors
0.00.024.639 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.639 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.272 I llm_load_vocab: special tokens cache size = 25
0.00.051.107 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.110 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.110 I llm_load_print_meta: arch             = gptneox
0.00.051.111 I llm_load_print_meta: vocab type       = BPE
0.00.051.111 I llm_load_print_meta: n_vocab          = 50304
0.00.051.111 I llm_load_print_meta: n_merges         = 50009
0.00.051.111 I llm_load_print_meta: vocab_only       = 0
0.00.051.111 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.111 I llm_load_print_meta: n_embd           = 2048
0.00.051.112 I llm_load_print_meta: n_layer          = 24
0.00.051.115 I llm_load_print_meta: n_head           = 16
0.00.051.116 I llm_load_print_meta: n_head_kv        = 16
0.00.051.116 I llm_load_print_meta: n_rot            = 32
0.00.051.116 I llm_load_print_meta: n_swa            = 0
0.00.051.118 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.118 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.119 I llm_load_print_meta: n_gqa            = 1
0.00.051.120 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.120 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.121 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.122 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.124 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.124 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.124 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.125 I llm_load_print_meta: n_ff             = 8192
0.00.051.125 I llm_load_print_meta: n_expert         = 0
0.00.051.125 I llm_load_print_meta: n_expert_used    = 0
0.00.051.125 I llm_load_print_meta: causal attn      = 1
0.00.051.125 I llm_load_print_meta: pooling type     = 0
0.00.051.125 I llm_load_print_meta: rope type        = 2
0.00.051.125 I llm_load_print_meta: rope scaling     = linear
0.00.051.126 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.126 I llm_load_print_meta: freq_scale_train = 1
0.00.051.126 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.126 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.127 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.127 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.127 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.128 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.128 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.140 I llm_load_print_meta: model type       = 1.4B
0.00.051.140 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.140 I llm_load_print_meta: model params     = 1.41 B
0.00.051.141 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.141 I llm_load_print_meta: general.name     = 1.4B
0.00.051.141 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.143 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.143 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.143 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.143 I llm_load_print_meta: LF token         = 128 ''
0.00.051.143 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.143 I llm_load_print_meta: max token length = 1024
0.00.052.758 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.759 I llm_load_tensors: offloading output layer to GPU
0.00.052.759 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.769 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.770 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.637 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.638 I llama_new_context_with_model: n_ctx         = 128
0.00.053.638 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.638 I llama_new_context_with_model: n_batch       = 128
0.00.053.639 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.639 I llama_new_context_with_model: flash_attn    = 0
0.00.053.639 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.639 I llama_new_context_with_model: freq_scale    = 1
0.00.053.640 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.640 I ggml_metal_init: allocating
0.00.053.643 I ggml_metal_init: found device: Apple M4
0.00.053.645 I ggml_metal_init: picking default device: Apple M4
0.00.054.200 I ggml_metal_init: using embedded metal library
0.00.056.142 I ggml_metal_init: GPU name:   Apple M4
0.00.056.144 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.144 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.145 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.145 I ggml_metal_init: simdgroup reduction   = true
0.00.056.145 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.145 I ggml_metal_init: has bfloat            = true
0.00.056.145 I ggml_metal_init: use bfloat            = true
0.00.056.146 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.146 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.320 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.327 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.350 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.269 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.270 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.270 I llama_new_context_with_model: graph nodes  = 967
0.00.066.271 I llama_new_context_with_model: graph splits = 2
0.00.066.282 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.604 I 
0.00.745.637 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.745.641 I perplexity: tokenizing the input ..
0.00.753.764 I perplexity: tokenization took 8.121 ms
0.00.753.770 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.888.441 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.889.620 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.889.648 I llama_perf_context_print:        load time =     736.50 ms
0.00.889.649 I llama_perf_context_print: prompt eval time =     134.43 ms /   128 tokens (    1.05 ms per token,   952.19 tokens per second)
0.00.889.650 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.889.650 I llama_perf_context_print:       total time =     144.05 ms /   129 tokens
0.00.890.043 I ggml_metal_free: deallocating

real	0m0.904s
user	0m0.078s
sys	0m0.123s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.324 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.863 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.868 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.869 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.870 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.870 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.870 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.871 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.871 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.872 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.872 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.872 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.873 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.873 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.873 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.875 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.875 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.875 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.788 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.809 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.670 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.671 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.672 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.672 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.672 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.672 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.673 I llama_model_loader: - type  f32:  194 tensors
0.00.023.673 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.674 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.674 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.684 I llm_load_vocab: special tokens cache size = 25
0.00.049.544 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.547 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.547 I llm_load_print_meta: arch             = gptneox
0.00.049.547 I llm_load_print_meta: vocab type       = BPE
0.00.049.548 I llm_load_print_meta: n_vocab          = 50304
0.00.049.548 I llm_load_print_meta: n_merges         = 50009
0.00.049.548 I llm_load_print_meta: vocab_only       = 0
0.00.049.548 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.548 I llm_load_print_meta: n_embd           = 2048
0.00.049.548 I llm_load_print_meta: n_layer          = 24
0.00.049.551 I llm_load_print_meta: n_head           = 16
0.00.049.552 I llm_load_print_meta: n_head_kv        = 16
0.00.049.552 I llm_load_print_meta: n_rot            = 32
0.00.049.552 I llm_load_print_meta: n_swa            = 0
0.00.049.552 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.554 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.554 I llm_load_print_meta: n_gqa            = 1
0.00.049.555 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.556 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.556 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.557 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.559 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.559 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.559 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.560 I llm_load_print_meta: n_ff             = 8192
0.00.049.560 I llm_load_print_meta: n_expert         = 0
0.00.049.560 I llm_load_print_meta: n_expert_used    = 0
0.00.049.560 I llm_load_print_meta: causal attn      = 1
0.00.049.560 I llm_load_print_meta: pooling type     = 0
0.00.049.560 I llm_load_print_meta: rope type        = 2
0.00.049.561 I llm_load_print_meta: rope scaling     = linear
0.00.049.561 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.561 I llm_load_print_meta: freq_scale_train = 1
0.00.049.563 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.563 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.563 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.564 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.564 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.564 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.564 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.571 I llm_load_print_meta: model type       = 1.4B
0.00.049.571 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.571 I llm_load_print_meta: model params     = 1.41 B
0.00.049.572 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.572 I llm_load_print_meta: general.name     = 1.4B
0.00.049.572 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.572 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.572 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.573 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.573 I llm_load_print_meta: LF token         = 128 ''
0.00.049.574 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.574 I llm_load_print_meta: max token length = 1024
0.00.051.307 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.307 I llm_load_tensors: offloading output layer to GPU
0.00.051.307 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.312 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.312 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.228 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.228 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.228 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.229 I llama_new_context_with_model: n_batch       = 2048
0.00.052.229 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.229 I llama_new_context_with_model: flash_attn    = 0
0.00.052.229 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.230 I llama_new_context_with_model: freq_scale    = 1
0.00.052.230 I ggml_metal_init: allocating
0.00.052.233 I ggml_metal_init: found device: Apple M4
0.00.052.235 I ggml_metal_init: picking default device: Apple M4
0.00.052.777 I ggml_metal_init: using embedded metal library
0.00.054.664 I ggml_metal_init: GPU name:   Apple M4
0.00.054.666 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.666 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.666 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.667 I ggml_metal_init: simdgroup reduction   = true
0.00.054.667 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.667 I ggml_metal_init: has bfloat            = true
0.00.054.667 I ggml_metal_init: use bfloat            = true
0.00.054.668 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.668 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.333 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.081.340 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.081.357 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.082.266 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.082.267 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.082.268 I llama_new_context_with_model: graph nodes  = 967
0.00.082.268 I llama_new_context_with_model: graph splits = 2
0.00.082.276 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.532.086 I main: llama threadpool init, n_threads = 4
0.00.532.126 I 
0.00.532.154 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.532.154 I 
0.00.532.386 I sampler seed: 1234
0.00.532.391 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.532.424 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.532.427 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.532.427 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.215.932 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58149.06 tokens per second)
0.01.215.933 I llama_perf_context_print:        load time =     522.75 ms
0.01.215.935 I llama_perf_context_print: prompt eval time =      39.70 ms /     7 tokens (    5.67 ms per token,   176.34 tokens per second)
0.01.215.936 I llama_perf_context_print:        eval time =     640.76 ms /    63 runs   (   10.17 ms per token,    98.32 tokens per second)
0.01.215.936 I llama_perf_context_print:       total time =     683.85 ms /    70 tokens
0.01.216.114 I ggml_metal_free: deallocating

real	0m1.235s
user	0m0.107s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.922 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.714 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.719 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.721 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.721 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.722 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.722 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.722 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.723 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.723 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.724 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.724 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.725 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.725 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.725 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.727 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.727 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.727 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.473 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.464 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.233 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.234 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.234 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.234 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.235 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.235 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.235 I llama_model_loader: - type  f32:  194 tensors
0.00.024.236 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.236 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.236 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.116 I llm_load_vocab: special tokens cache size = 25
0.00.050.057 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.060 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.060 I llm_load_print_meta: arch             = gptneox
0.00.050.060 I llm_load_print_meta: vocab type       = BPE
0.00.050.060 I llm_load_print_meta: n_vocab          = 50304
0.00.050.061 I llm_load_print_meta: n_merges         = 50009
0.00.050.061 I llm_load_print_meta: vocab_only       = 0
0.00.050.061 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.061 I llm_load_print_meta: n_embd           = 2048
0.00.050.061 I llm_load_print_meta: n_layer          = 24
0.00.050.064 I llm_load_print_meta: n_head           = 16
0.00.050.065 I llm_load_print_meta: n_head_kv        = 16
0.00.050.065 I llm_load_print_meta: n_rot            = 32
0.00.050.065 I llm_load_print_meta: n_swa            = 0
0.00.050.065 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.066 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.066 I llm_load_print_meta: n_gqa            = 1
0.00.050.067 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.069 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.069 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.070 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.070 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.070 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.070 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.071 I llm_load_print_meta: n_ff             = 8192
0.00.050.071 I llm_load_print_meta: n_expert         = 0
0.00.050.071 I llm_load_print_meta: n_expert_used    = 0
0.00.050.072 I llm_load_print_meta: causal attn      = 1
0.00.050.072 I llm_load_print_meta: pooling type     = 0
0.00.050.072 I llm_load_print_meta: rope type        = 2
0.00.050.072 I llm_load_print_meta: rope scaling     = linear
0.00.050.073 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.073 I llm_load_print_meta: freq_scale_train = 1
0.00.050.073 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.073 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.073 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.074 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.074 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.074 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.074 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.086 I llm_load_print_meta: model type       = 1.4B
0.00.050.086 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.086 I llm_load_print_meta: model params     = 1.41 B
0.00.050.087 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.087 I llm_load_print_meta: general.name     = 1.4B
0.00.050.088 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.088 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.088 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.088 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.089 I llm_load_print_meta: LF token         = 128 ''
0.00.050.089 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.089 I llm_load_print_meta: max token length = 1024
0.00.051.933 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.933 I llm_load_tensors: offloading output layer to GPU
0.00.051.933 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.943 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.944 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.836 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.837 I llama_new_context_with_model: n_ctx         = 128
0.00.052.837 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.838 I llama_new_context_with_model: n_batch       = 128
0.00.052.838 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.838 I llama_new_context_with_model: flash_attn    = 0
0.00.052.838 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.839 I llama_new_context_with_model: freq_scale    = 1
0.00.052.839 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.839 I ggml_metal_init: allocating
0.00.052.846 I ggml_metal_init: found device: Apple M4
0.00.052.848 I ggml_metal_init: picking default device: Apple M4
0.00.053.388 I ggml_metal_init: using embedded metal library
0.00.055.359 I ggml_metal_init: GPU name:   Apple M4
0.00.055.361 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.361 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.361 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.362 I ggml_metal_init: simdgroup reduction   = true
0.00.055.362 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.362 I ggml_metal_init: has bfloat            = true
0.00.055.362 I ggml_metal_init: use bfloat            = true
0.00.055.362 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.363 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.429 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.433 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.447 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.411 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.413 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.413 I llama_new_context_with_model: graph nodes  = 967
0.00.065.413 I llama_new_context_with_model: graph splits = 2
0.00.065.425 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.470.880 I 
0.00.470.904 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.470.908 I perplexity: tokenizing the input ..
0.00.478.797 I perplexity: tokenization took 7.887 ms
0.00.478.800 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.611.062 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.612.222 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.612.245 I llama_perf_context_print:        load time =     460.95 ms
0.00.612.247 I llama_perf_context_print: prompt eval time =     132.03 ms /   128 tokens (    1.03 ms per token,   969.48 tokens per second)
0.00.612.247 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.612.248 I llama_perf_context_print:       total time =     141.37 ms /   129 tokens
0.00.612.771 I ggml_metal_free: deallocating

real	0m0.631s
user	0m0.076s
sys	0m0.090s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.546 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.961 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.966 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.967 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.968 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.968 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.968 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.969 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.970 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.970 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.970 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.971 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.971 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.971 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.972 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.974 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.974 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.974 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.052 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.195 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.267 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.268 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.268 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.269 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.269 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.269 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.270 I llama_model_loader: - type  f32:  194 tensors
0.00.024.270 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.270 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.271 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.271 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.262 I llm_load_vocab: special tokens cache size = 25
0.00.051.199 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.202 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.202 I llm_load_print_meta: arch             = gptneox
0.00.051.202 I llm_load_print_meta: vocab type       = BPE
0.00.051.203 I llm_load_print_meta: n_vocab          = 50304
0.00.051.203 I llm_load_print_meta: n_merges         = 50009
0.00.051.203 I llm_load_print_meta: vocab_only       = 0
0.00.051.203 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.203 I llm_load_print_meta: n_embd           = 2048
0.00.051.203 I llm_load_print_meta: n_layer          = 24
0.00.051.206 I llm_load_print_meta: n_head           = 16
0.00.051.207 I llm_load_print_meta: n_head_kv        = 16
0.00.051.207 I llm_load_print_meta: n_rot            = 32
0.00.051.207 I llm_load_print_meta: n_swa            = 0
0.00.051.208 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.208 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.209 I llm_load_print_meta: n_gqa            = 1
0.00.051.210 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.210 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.211 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.211 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.212 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.213 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.213 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.213 I llm_load_print_meta: n_ff             = 8192
0.00.051.214 I llm_load_print_meta: n_expert         = 0
0.00.051.214 I llm_load_print_meta: n_expert_used    = 0
0.00.051.215 I llm_load_print_meta: causal attn      = 1
0.00.051.215 I llm_load_print_meta: pooling type     = 0
0.00.051.217 I llm_load_print_meta: rope type        = 2
0.00.051.217 I llm_load_print_meta: rope scaling     = linear
0.00.051.218 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.218 I llm_load_print_meta: freq_scale_train = 1
0.00.051.218 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.218 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.218 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.219 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.219 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.219 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.219 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.231 I llm_load_print_meta: model type       = 1.4B
0.00.051.232 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.232 I llm_load_print_meta: model params     = 1.41 B
0.00.051.233 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.233 I llm_load_print_meta: general.name     = 1.4B
0.00.051.233 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.234 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.234 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.234 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.234 I llm_load_print_meta: LF token         = 128 ''
0.00.051.235 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.235 I llm_load_print_meta: max token length = 1024
0.00.053.137 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.138 I llm_load_tensors: offloading output layer to GPU
0.00.053.138 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.148 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.149 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.065 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.065 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.066 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.066 I llama_new_context_with_model: n_batch       = 2048
0.00.054.066 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.066 I llama_new_context_with_model: flash_attn    = 0
0.00.054.067 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.067 I llama_new_context_with_model: freq_scale    = 1
0.00.054.067 I ggml_metal_init: allocating
0.00.054.070 I ggml_metal_init: found device: Apple M4
0.00.054.072 I ggml_metal_init: picking default device: Apple M4
0.00.054.592 I ggml_metal_init: using embedded metal library
0.00.056.522 I ggml_metal_init: GPU name:   Apple M4
0.00.056.523 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.523 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.524 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.524 I ggml_metal_init: simdgroup reduction   = true
0.00.056.524 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.524 I ggml_metal_init: has bfloat            = true
0.00.056.524 I ggml_metal_init: use bfloat            = true
0.00.056.525 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.525 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.564 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.572 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.589 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.570 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.572 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.572 I llama_new_context_with_model: graph nodes  = 967
0.00.085.572 I llama_new_context_with_model: graph splits = 2
0.00.085.586 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.620.744 I main: llama threadpool init, n_threads = 4
0.00.620.784 I 
0.00.620.811 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.620.812 I 
0.00.620.949 I sampler seed: 1234
0.00.620.955 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.620.986 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.620.989 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.620.990 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.365.455 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60891.94 tokens per second)
0.01.365.456 I llama_perf_context_print:        load time =     611.19 ms
0.01.365.457 I llama_perf_context_print: prompt eval time =      35.51 ms /     7 tokens (    5.07 ms per token,   197.14 tokens per second)
0.01.365.458 I llama_perf_context_print:        eval time =     706.02 ms /    63 runs   (   11.21 ms per token,    89.23 tokens per second)
0.01.365.458 I llama_perf_context_print:       total time =     744.72 ms /    70 tokens
0.01.365.642 I ggml_metal_free: deallocating

real	0m1.382s
user	0m0.109s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.780 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.419 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.424 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.426 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.426 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.426 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.427 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.427 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.428 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.428 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.428 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.429 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.429 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.429 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.430 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.432 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.432 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.432 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.469 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.507 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.570 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.571 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.571 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.572 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.572 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.572 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.573 I llama_model_loader: - type  f32:  194 tensors
0.00.023.573 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.573 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.574 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.574 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.574 I llm_load_vocab: special tokens cache size = 25
0.00.049.492 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.495 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.495 I llm_load_print_meta: arch             = gptneox
0.00.049.496 I llm_load_print_meta: vocab type       = BPE
0.00.049.496 I llm_load_print_meta: n_vocab          = 50304
0.00.049.496 I llm_load_print_meta: n_merges         = 50009
0.00.049.496 I llm_load_print_meta: vocab_only       = 0
0.00.049.496 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.497 I llm_load_print_meta: n_embd           = 2048
0.00.049.497 I llm_load_print_meta: n_layer          = 24
0.00.049.499 I llm_load_print_meta: n_head           = 16
0.00.049.500 I llm_load_print_meta: n_head_kv        = 16
0.00.049.500 I llm_load_print_meta: n_rot            = 32
0.00.049.501 I llm_load_print_meta: n_swa            = 0
0.00.049.501 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.501 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.502 I llm_load_print_meta: n_gqa            = 1
0.00.049.503 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.503 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.505 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.505 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.505 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.505 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.506 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.506 I llm_load_print_meta: n_ff             = 8192
0.00.049.506 I llm_load_print_meta: n_expert         = 0
0.00.049.507 I llm_load_print_meta: n_expert_used    = 0
0.00.049.507 I llm_load_print_meta: causal attn      = 1
0.00.049.507 I llm_load_print_meta: pooling type     = 0
0.00.049.507 I llm_load_print_meta: rope type        = 2
0.00.049.507 I llm_load_print_meta: rope scaling     = linear
0.00.049.508 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.508 I llm_load_print_meta: freq_scale_train = 1
0.00.049.508 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.508 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.508 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.509 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.509 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.509 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.509 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.522 I llm_load_print_meta: model type       = 1.4B
0.00.049.523 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.523 I llm_load_print_meta: model params     = 1.41 B
0.00.049.523 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.524 I llm_load_print_meta: general.name     = 1.4B
0.00.049.524 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.524 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.524 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.524 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.524 I llm_load_print_meta: LF token         = 128 ''
0.00.049.525 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.525 I llm_load_print_meta: max token length = 1024
0.00.051.432 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.432 I llm_load_tensors: offloading output layer to GPU
0.00.051.432 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.442 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.443 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.343 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.344 I llama_new_context_with_model: n_ctx         = 128
0.00.052.344 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.344 I llama_new_context_with_model: n_batch       = 128
0.00.052.344 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.345 I llama_new_context_with_model: flash_attn    = 0
0.00.052.345 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.345 I llama_new_context_with_model: freq_scale    = 1
0.00.052.346 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.346 I ggml_metal_init: allocating
0.00.052.349 I ggml_metal_init: found device: Apple M4
0.00.052.351 I ggml_metal_init: picking default device: Apple M4
0.00.052.895 I ggml_metal_init: using embedded metal library
0.00.054.774 I ggml_metal_init: GPU name:   Apple M4
0.00.054.775 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.775 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.776 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.776 I ggml_metal_init: simdgroup reduction   = true
0.00.054.776 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.776 I ggml_metal_init: has bfloat            = true
0.00.054.776 I ggml_metal_init: use bfloat            = true
0.00.054.777 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.777 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.724 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.729 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.741 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.657 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.658 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.659 I llama_new_context_with_model: graph nodes  = 967
0.00.064.659 I llama_new_context_with_model: graph splits = 2
0.00.064.671 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.586.649 I 
0.00.586.716 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.586.725 I perplexity: tokenizing the input ..
0.00.594.786 I perplexity: tokenization took 8.06 ms
0.00.594.789 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.726.347 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.727.509 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.727.532 I llama_perf_context_print:        load time =     577.85 ms
0.00.727.539 I llama_perf_context_print: prompt eval time =     131.33 ms /   128 tokens (    1.03 ms per token,   974.66 tokens per second)
0.00.727.541 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.727.542 I llama_perf_context_print:       total time =     140.90 ms /   129 tokens
0.00.727.949 I ggml_metal_free: deallocating

real	0m0.742s
user	0m0.077s
sys	0m0.099s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.601 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.939 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.945 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.950 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.951 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.951 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.952 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.952 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.953 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.953 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.954 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.954 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.956 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.956 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.956 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.958 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.958 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.958 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.115 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.256 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.285 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.286 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.286 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.286 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.286 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.287 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.287 I llama_model_loader: - type  f32:  194 tensors
0.00.025.287 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.288 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.288 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.104 I llm_load_vocab: special tokens cache size = 25
0.00.051.945 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.948 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.948 I llm_load_print_meta: arch             = gptneox
0.00.051.948 I llm_load_print_meta: vocab type       = BPE
0.00.051.949 I llm_load_print_meta: n_vocab          = 50304
0.00.051.949 I llm_load_print_meta: n_merges         = 50009
0.00.051.949 I llm_load_print_meta: vocab_only       = 0
0.00.051.949 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.949 I llm_load_print_meta: n_embd           = 2048
0.00.051.950 I llm_load_print_meta: n_layer          = 24
0.00.051.958 I llm_load_print_meta: n_head           = 16
0.00.051.965 I llm_load_print_meta: n_head_kv        = 16
0.00.051.965 I llm_load_print_meta: n_rot            = 32
0.00.051.966 I llm_load_print_meta: n_swa            = 0
0.00.051.966 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.966 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.967 I llm_load_print_meta: n_gqa            = 1
0.00.051.968 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.969 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.969 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.970 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.970 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.970 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.970 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.971 I llm_load_print_meta: n_ff             = 8192
0.00.051.971 I llm_load_print_meta: n_expert         = 0
0.00.051.971 I llm_load_print_meta: n_expert_used    = 0
0.00.051.971 I llm_load_print_meta: causal attn      = 1
0.00.051.971 I llm_load_print_meta: pooling type     = 0
0.00.051.971 I llm_load_print_meta: rope type        = 2
0.00.051.972 I llm_load_print_meta: rope scaling     = linear
0.00.051.972 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.972 I llm_load_print_meta: freq_scale_train = 1
0.00.051.972 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.972 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.973 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.973 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.973 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.973 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.973 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.985 I llm_load_print_meta: model type       = 1.4B
0.00.051.986 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.986 I llm_load_print_meta: model params     = 1.41 B
0.00.051.986 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.987 I llm_load_print_meta: general.name     = 1.4B
0.00.051.987 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.987 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.987 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.988 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.988 I llm_load_print_meta: LF token         = 128 ''
0.00.051.989 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.989 I llm_load_print_meta: max token length = 1024
0.00.053.962 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.962 I llm_load_tensors: offloading output layer to GPU
0.00.053.963 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.973 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.974 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.892 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.893 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.893 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.893 I llama_new_context_with_model: n_batch       = 2048
0.00.054.893 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.893 I llama_new_context_with_model: flash_attn    = 0
0.00.054.894 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.894 I llama_new_context_with_model: freq_scale    = 1
0.00.054.894 I ggml_metal_init: allocating
0.00.054.897 I ggml_metal_init: found device: Apple M4
0.00.054.899 I ggml_metal_init: picking default device: Apple M4
0.00.055.422 I ggml_metal_init: using embedded metal library
0.00.057.365 I ggml_metal_init: GPU name:   Apple M4
0.00.057.367 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.367 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.367 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.368 I ggml_metal_init: simdgroup reduction   = true
0.00.057.368 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.368 I ggml_metal_init: has bfloat            = true
0.00.057.368 I ggml_metal_init: use bfloat            = true
0.00.057.369 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.370 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.580 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.588 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.605 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.590 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.591 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.591 I llama_new_context_with_model: graph nodes  = 967
0.00.086.591 I llama_new_context_with_model: graph splits = 2
0.00.086.597 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.635.147 I main: llama threadpool init, n_threads = 4
0.00.635.188 I 
0.00.635.211 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.635.211 I 
0.00.635.434 I sampler seed: 1234
0.00.635.438 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.635.485 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.635.489 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.635.489 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.390.759 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56528.66 tokens per second)
0.01.390.759 I llama_perf_context_print:        load time =     625.54 ms
0.01.390.760 I llama_perf_context_print: prompt eval time =      39.68 ms /     7 tokens (    5.67 ms per token,   176.42 tokens per second)
0.01.390.761 I llama_perf_context_print:        eval time =     712.53 ms /    63 runs   (   11.31 ms per token,    88.42 tokens per second)
0.01.390.761 I llama_perf_context_print:       total time =     755.61 ms /    70 tokens
0.01.390.940 I ggml_metal_free: deallocating

real	0m1.411s
user	0m0.109s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.069 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.825 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.829 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.831 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.832 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.832 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.832 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.833 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.833 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.834 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.834 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.835 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.837 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.838 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.838 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.839 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.840 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.840 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.030 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.125 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.117 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.118 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.118 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.118 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.119 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.119 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.119 I llama_model_loader: - type  f32:  194 tensors
0.00.024.120 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.120 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.120 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.022 I llm_load_vocab: special tokens cache size = 25
0.00.049.778 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.781 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.782 I llm_load_print_meta: arch             = gptneox
0.00.049.782 I llm_load_print_meta: vocab type       = BPE
0.00.049.782 I llm_load_print_meta: n_vocab          = 50304
0.00.049.783 I llm_load_print_meta: n_merges         = 50009
0.00.049.783 I llm_load_print_meta: vocab_only       = 0
0.00.049.783 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.783 I llm_load_print_meta: n_embd           = 2048
0.00.049.783 I llm_load_print_meta: n_layer          = 24
0.00.049.786 I llm_load_print_meta: n_head           = 16
0.00.049.787 I llm_load_print_meta: n_head_kv        = 16
0.00.049.787 I llm_load_print_meta: n_rot            = 32
0.00.049.788 I llm_load_print_meta: n_swa            = 0
0.00.049.789 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.789 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.790 I llm_load_print_meta: n_gqa            = 1
0.00.049.791 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.791 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.792 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.792 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.792 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.792 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.793 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.794 I llm_load_print_meta: n_ff             = 8192
0.00.049.794 I llm_load_print_meta: n_expert         = 0
0.00.049.794 I llm_load_print_meta: n_expert_used    = 0
0.00.049.794 I llm_load_print_meta: causal attn      = 1
0.00.049.795 I llm_load_print_meta: pooling type     = 0
0.00.049.795 I llm_load_print_meta: rope type        = 2
0.00.049.795 I llm_load_print_meta: rope scaling     = linear
0.00.049.795 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.796 I llm_load_print_meta: freq_scale_train = 1
0.00.049.796 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.796 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.796 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.796 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.798 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.798 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.798 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.810 I llm_load_print_meta: model type       = 1.4B
0.00.049.810 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.810 I llm_load_print_meta: model params     = 1.41 B
0.00.049.811 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.811 I llm_load_print_meta: general.name     = 1.4B
0.00.049.811 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.811 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.812 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.812 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.812 I llm_load_print_meta: LF token         = 128 ''
0.00.049.812 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.812 I llm_load_print_meta: max token length = 1024
0.00.051.376 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.376 I llm_load_tensors: offloading output layer to GPU
0.00.051.377 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.386 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.387 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.226 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.226 I llama_new_context_with_model: n_ctx         = 128
0.00.052.227 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.227 I llama_new_context_with_model: n_batch       = 128
0.00.052.227 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.227 I llama_new_context_with_model: flash_attn    = 0
0.00.052.228 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.228 I llama_new_context_with_model: freq_scale    = 1
0.00.052.228 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.228 I ggml_metal_init: allocating
0.00.052.232 I ggml_metal_init: found device: Apple M4
0.00.052.233 I ggml_metal_init: picking default device: Apple M4
0.00.052.744 I ggml_metal_init: using embedded metal library
0.00.054.624 I ggml_metal_init: GPU name:   Apple M4
0.00.054.625 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.626 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.626 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.626 I ggml_metal_init: simdgroup reduction   = true
0.00.054.626 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.626 I ggml_metal_init: has bfloat            = true
0.00.054.627 I ggml_metal_init: use bfloat            = true
0.00.054.627 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.627 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.755 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.759 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.774 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.674 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.675 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.675 I llama_new_context_with_model: graph nodes  = 967
0.00.064.676 I llama_new_context_with_model: graph splits = 2
0.00.064.687 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.607.819 I 
0.00.607.852 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.607.855 I perplexity: tokenizing the input ..
0.00.615.957 I perplexity: tokenization took 8.101 ms
0.00.615.965 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.750.235 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.751.400 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.751.421 I llama_perf_context_print:        load time =     598.75 ms
0.00.751.422 I llama_perf_context_print: prompt eval time =     134.05 ms /   128 tokens (    1.05 ms per token,   954.89 tokens per second)
0.00.751.423 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.751.424 I llama_perf_context_print:       total time =     143.60 ms /   129 tokens
0.00.751.716 I ggml_metal_free: deallocating

real	0m0.768s
user	0m0.077s
sys	0m0.129s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.008.429 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.040 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.045 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.051 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.051 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.052 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.052 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.052 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.053 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.053 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.054 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.054 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.055 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.055 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.055 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.057 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.057 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.057 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.116 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.156 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.175 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.176 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.176 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.177 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.177 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.177 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.178 I llama_model_loader: - type  f32:  194 tensors
0.00.024.178 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.178 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.954 I llm_load_vocab: special tokens cache size = 25
0.00.050.809 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.812 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.812 I llm_load_print_meta: arch             = gptneox
0.00.050.812 I llm_load_print_meta: vocab type       = BPE
0.00.050.813 I llm_load_print_meta: n_vocab          = 50304
0.00.050.813 I llm_load_print_meta: n_merges         = 50009
0.00.050.813 I llm_load_print_meta: vocab_only       = 0
0.00.050.813 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.813 I llm_load_print_meta: n_embd           = 2048
0.00.050.814 I llm_load_print_meta: n_layer          = 24
0.00.050.817 I llm_load_print_meta: n_head           = 16
0.00.050.817 I llm_load_print_meta: n_head_kv        = 16
0.00.050.817 I llm_load_print_meta: n_rot            = 32
0.00.050.818 I llm_load_print_meta: n_swa            = 0
0.00.050.818 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.819 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.820 I llm_load_print_meta: n_gqa            = 1
0.00.050.821 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.821 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.822 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.822 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.822 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.822 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.823 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.824 I llm_load_print_meta: n_ff             = 8192
0.00.050.824 I llm_load_print_meta: n_expert         = 0
0.00.050.824 I llm_load_print_meta: n_expert_used    = 0
0.00.050.824 I llm_load_print_meta: causal attn      = 1
0.00.050.824 I llm_load_print_meta: pooling type     = 0
0.00.050.824 I llm_load_print_meta: rope type        = 2
0.00.050.825 I llm_load_print_meta: rope scaling     = linear
0.00.050.826 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.827 I llm_load_print_meta: freq_scale_train = 1
0.00.050.827 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.827 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.827 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.827 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.827 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.827 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.828 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.839 I llm_load_print_meta: model type       = 1.4B
0.00.050.839 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.840 I llm_load_print_meta: model params     = 1.41 B
0.00.050.840 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.840 I llm_load_print_meta: general.name     = 1.4B
0.00.050.841 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.841 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.841 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.841 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.841 I llm_load_print_meta: LF token         = 128 ''
0.00.050.842 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.842 I llm_load_print_meta: max token length = 1024
0.00.052.378 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.378 I llm_load_tensors: offloading output layer to GPU
0.00.052.378 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.388 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.389 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.196 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.197 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.197 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.197 I llama_new_context_with_model: n_batch       = 2048
0.00.053.197 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.197 I llama_new_context_with_model: flash_attn    = 0
0.00.053.198 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.198 I llama_new_context_with_model: freq_scale    = 1
0.00.053.198 I ggml_metal_init: allocating
0.00.053.201 I ggml_metal_init: found device: Apple M4
0.00.053.204 I ggml_metal_init: picking default device: Apple M4
0.00.053.723 I ggml_metal_init: using embedded metal library
0.00.055.625 I ggml_metal_init: GPU name:   Apple M4
0.00.055.627 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.627 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.628 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.628 I ggml_metal_init: simdgroup reduction   = true
0.00.055.628 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.628 I ggml_metal_init: has bfloat            = true
0.00.055.628 I ggml_metal_init: use bfloat            = true
0.00.055.629 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.630 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.872 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.878 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.899 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.890 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.892 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.892 I llama_new_context_with_model: graph nodes  = 967
0.00.083.892 I llama_new_context_with_model: graph splits = 2
0.00.083.906 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.825 I main: llama threadpool init, n_threads = 4
0.00.698.871 I 
0.00.698.899 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.698.901 I 
0.00.699.125 I sampler seed: 1234
0.00.699.132 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.699.167 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.699.169 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.699.169 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.541.261 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59764.31 tokens per second)
0.01.541.261 I llama_perf_context_print:        load time =     690.39 ms
0.01.541.262 I llama_perf_context_print: prompt eval time =      42.47 ms /     7 tokens (    6.07 ms per token,   164.83 tokens per second)
0.01.541.263 I llama_perf_context_print:        eval time =     796.68 ms /    63 runs   (   12.65 ms per token,    79.08 tokens per second)
0.01.541.263 I llama_perf_context_print:       total time =     842.44 ms /    70 tokens
0.01.541.422 I ggml_metal_free: deallocating

real	0m1.558s
user	0m0.108s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.993 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.642 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.646 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.648 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.648 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.649 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.649 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.649 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.650 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.650 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.651 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.651 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.651 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.652 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.652 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.654 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.654 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.654 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.625 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.677 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.603 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.604 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.605 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.605 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.605 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.606 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.606 I llama_model_loader: - type  f32:  194 tensors
0.00.023.606 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.606 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.439 I llm_load_vocab: special tokens cache size = 25
0.00.049.273 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.276 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.276 I llm_load_print_meta: arch             = gptneox
0.00.049.277 I llm_load_print_meta: vocab type       = BPE
0.00.049.277 I llm_load_print_meta: n_vocab          = 50304
0.00.049.277 I llm_load_print_meta: n_merges         = 50009
0.00.049.277 I llm_load_print_meta: vocab_only       = 0
0.00.049.277 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.277 I llm_load_print_meta: n_embd           = 2048
0.00.049.278 I llm_load_print_meta: n_layer          = 24
0.00.049.281 I llm_load_print_meta: n_head           = 16
0.00.049.281 I llm_load_print_meta: n_head_kv        = 16
0.00.049.282 I llm_load_print_meta: n_rot            = 32
0.00.049.282 I llm_load_print_meta: n_swa            = 0
0.00.049.282 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.282 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.286 I llm_load_print_meta: n_gqa            = 1
0.00.049.286 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.288 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.289 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.289 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.290 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.290 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.290 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.291 I llm_load_print_meta: n_ff             = 8192
0.00.049.291 I llm_load_print_meta: n_expert         = 0
0.00.049.291 I llm_load_print_meta: n_expert_used    = 0
0.00.049.291 I llm_load_print_meta: causal attn      = 1
0.00.049.291 I llm_load_print_meta: pooling type     = 0
0.00.049.291 I llm_load_print_meta: rope type        = 2
0.00.049.292 I llm_load_print_meta: rope scaling     = linear
0.00.049.292 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.292 I llm_load_print_meta: freq_scale_train = 1
0.00.049.293 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.293 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.293 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.293 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.293 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.293 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.293 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.305 I llm_load_print_meta: model type       = 1.4B
0.00.049.305 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.306 I llm_load_print_meta: model params     = 1.41 B
0.00.049.306 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.306 I llm_load_print_meta: general.name     = 1.4B
0.00.049.306 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.307 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.307 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.307 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.307 I llm_load_print_meta: LF token         = 128 ''
0.00.049.307 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.308 I llm_load_print_meta: max token length = 1024
0.00.050.839 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.839 I llm_load_tensors: offloading output layer to GPU
0.00.050.839 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.848 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.050.849 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.058 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.059 I llama_new_context_with_model: n_ctx         = 128
0.00.052.059 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.060 I llama_new_context_with_model: n_batch       = 128
0.00.052.060 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.060 I llama_new_context_with_model: flash_attn    = 0
0.00.052.060 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.061 I llama_new_context_with_model: freq_scale    = 1
0.00.052.061 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.061 I ggml_metal_init: allocating
0.00.052.067 I ggml_metal_init: found device: Apple M4
0.00.052.069 I ggml_metal_init: picking default device: Apple M4
0.00.052.612 I ggml_metal_init: using embedded metal library
0.00.054.532 I ggml_metal_init: GPU name:   Apple M4
0.00.054.533 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.534 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.534 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.534 I ggml_metal_init: simdgroup reduction   = true
0.00.054.534 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.535 I ggml_metal_init: has bfloat            = true
0.00.054.535 I ggml_metal_init: use bfloat            = true
0.00.054.535 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.536 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.789 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.791 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.806 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.680 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.681 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.682 I llama_new_context_with_model: graph nodes  = 967
0.00.064.682 I llama_new_context_with_model: graph splits = 2
0.00.064.694 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.647.579 I 
0.00.647.606 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.647.609 I perplexity: tokenizing the input ..
0.00.655.574 I perplexity: tokenization took 7.963 ms
0.00.655.577 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.795.808 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.796.948 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.796.975 I llama_perf_context_print:        load time =     638.58 ms
0.00.796.976 I llama_perf_context_print: prompt eval time =     140.00 ms /   128 tokens (    1.09 ms per token,   914.25 tokens per second)
0.00.796.978 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.979 I llama_perf_context_print:       total time =     149.40 ms /   129 tokens
0.00.797.436 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.077s
sys	0m0.121s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.823 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.236 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.240 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.242 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.242 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.243 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.243 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.243 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.244 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.245 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.245 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.245 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.246 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.246 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.246 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.248 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.248 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.250 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.278 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.362 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.492 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.493 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.493 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.494 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.494 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.494 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.495 I llama_model_loader: - type  f32:  194 tensors
0.00.025.495 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.647 I llm_load_vocab: special tokens cache size = 25
0.00.051.521 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.523 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.524 I llm_load_print_meta: arch             = gptneox
0.00.051.524 I llm_load_print_meta: vocab type       = BPE
0.00.051.524 I llm_load_print_meta: n_vocab          = 50304
0.00.051.524 I llm_load_print_meta: n_merges         = 50009
0.00.051.525 I llm_load_print_meta: vocab_only       = 0
0.00.051.525 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.525 I llm_load_print_meta: n_embd           = 2048
0.00.051.525 I llm_load_print_meta: n_layer          = 24
0.00.051.528 I llm_load_print_meta: n_head           = 16
0.00.051.529 I llm_load_print_meta: n_head_kv        = 16
0.00.051.529 I llm_load_print_meta: n_rot            = 32
0.00.051.529 I llm_load_print_meta: n_swa            = 0
0.00.051.529 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.530 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.530 I llm_load_print_meta: n_gqa            = 1
0.00.051.531 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.532 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.532 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.533 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.533 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.533 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.533 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.534 I llm_load_print_meta: n_ff             = 8192
0.00.051.534 I llm_load_print_meta: n_expert         = 0
0.00.051.534 I llm_load_print_meta: n_expert_used    = 0
0.00.051.534 I llm_load_print_meta: causal attn      = 1
0.00.051.536 I llm_load_print_meta: pooling type     = 0
0.00.051.538 I llm_load_print_meta: rope type        = 2
0.00.051.538 I llm_load_print_meta: rope scaling     = linear
0.00.051.538 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.539 I llm_load_print_meta: freq_scale_train = 1
0.00.051.539 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.539 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.539 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.540 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.540 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.540 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.540 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.552 I llm_load_print_meta: model type       = 1.4B
0.00.051.552 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.552 I llm_load_print_meta: model params     = 1.41 B
0.00.051.553 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.553 I llm_load_print_meta: general.name     = 1.4B
0.00.051.553 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.553 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.554 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.554 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.555 I llm_load_print_meta: LF token         = 128 ''
0.00.051.555 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.555 I llm_load_print_meta: max token length = 1024
0.00.053.554 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.554 I llm_load_tensors: offloading output layer to GPU
0.00.053.555 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.564 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.565 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.531 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.531 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.532 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.532 I llama_new_context_with_model: n_batch       = 2048
0.00.054.532 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.532 I llama_new_context_with_model: flash_attn    = 0
0.00.054.533 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.533 I llama_new_context_with_model: freq_scale    = 1
0.00.054.533 I ggml_metal_init: allocating
0.00.054.536 I ggml_metal_init: found device: Apple M4
0.00.054.538 I ggml_metal_init: picking default device: Apple M4
0.00.055.102 I ggml_metal_init: using embedded metal library
0.00.057.196 I ggml_metal_init: GPU name:   Apple M4
0.00.057.198 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.198 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.199 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.199 I ggml_metal_init: simdgroup reduction   = true
0.00.057.200 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.200 I ggml_metal_init: has bfloat            = true
0.00.057.201 I ggml_metal_init: use bfloat            = true
0.00.057.201 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.203 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.845 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.853 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.880 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.991 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.993 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.993 I llama_new_context_with_model: graph nodes  = 967
0.00.084.993 I llama_new_context_with_model: graph splits = 2
0.00.085.006 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.776.581 I main: llama threadpool init, n_threads = 4
0.00.776.613 I 
0.00.776.638 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.776.639 I 
0.00.776.887 I sampler seed: 1234
0.00.776.891 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.776.906 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.776.908 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.776.908 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.641.008 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57073.95 tokens per second)
0.01.641.009 I llama_perf_context_print:        load time =     766.75 ms
0.01.641.010 I llama_perf_context_print: prompt eval time =      38.46 ms /     7 tokens (    5.49 ms per token,   182.00 tokens per second)
0.01.641.010 I llama_perf_context_print:        eval time =     822.52 ms /    63 runs   (   13.06 ms per token,    76.59 tokens per second)
0.01.641.011 I llama_perf_context_print:       total time =     864.43 ms /    70 tokens
0.01.641.203 I ggml_metal_free: deallocating

real	0m1.661s
user	0m0.109s
sys	0m0.185s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4227 (2af44aac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.797 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.419 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.423 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.424 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.425 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.425 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.425 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.425 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.426 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.426 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.427 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.427 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.427 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.428 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.428 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.429 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.430 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.430 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.488 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.508 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.565 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.566 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.566 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.566 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.567 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.567 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.567 I llama_model_loader: - type  f32:  194 tensors
0.00.024.568 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.285 I llm_load_vocab: special tokens cache size = 25
0.00.050.207 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.210 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.210 I llm_load_print_meta: arch             = gptneox
0.00.050.210 I llm_load_print_meta: vocab type       = BPE
0.00.050.211 I llm_load_print_meta: n_vocab          = 50304
0.00.050.211 I llm_load_print_meta: n_merges         = 50009
0.00.050.211 I llm_load_print_meta: vocab_only       = 0
0.00.050.211 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.211 I llm_load_print_meta: n_embd           = 2048
0.00.050.211 I llm_load_print_meta: n_layer          = 24
0.00.050.215 I llm_load_print_meta: n_head           = 16
0.00.050.216 I llm_load_print_meta: n_head_kv        = 16
0.00.050.218 I llm_load_print_meta: n_rot            = 32
0.00.050.218 I llm_load_print_meta: n_swa            = 0
0.00.050.218 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.218 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.219 I llm_load_print_meta: n_gqa            = 1
0.00.050.220 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.221 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.222 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.222 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.222 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.222 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.222 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.223 I llm_load_print_meta: n_ff             = 8192
0.00.050.223 I llm_load_print_meta: n_expert         = 0
0.00.050.223 I llm_load_print_meta: n_expert_used    = 0
0.00.050.224 I llm_load_print_meta: causal attn      = 1
0.00.050.224 I llm_load_print_meta: pooling type     = 0
0.00.050.224 I llm_load_print_meta: rope type        = 2
0.00.050.224 I llm_load_print_meta: rope scaling     = linear
0.00.050.225 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.225 I llm_load_print_meta: freq_scale_train = 1
0.00.050.226 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.226 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.226 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.226 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.226 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.227 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.228 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.240 I llm_load_print_meta: model type       = 1.4B
0.00.050.240 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.240 I llm_load_print_meta: model params     = 1.41 B
0.00.050.241 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.241 I llm_load_print_meta: general.name     = 1.4B
0.00.050.241 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.241 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.241 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.241 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.242 I llm_load_print_meta: LF token         = 128 ''
0.00.050.242 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.242 I llm_load_print_meta: max token length = 1024
0.00.052.213 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.213 I llm_load_tensors: offloading output layer to GPU
0.00.052.213 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.223 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.224 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.094 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.095 I llama_new_context_with_model: n_ctx         = 128
0.00.053.095 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.095 I llama_new_context_with_model: n_batch       = 128
0.00.053.095 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.096 I llama_new_context_with_model: flash_attn    = 0
0.00.053.096 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.096 I llama_new_context_with_model: freq_scale    = 1
0.00.053.097 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.097 I ggml_metal_init: allocating
0.00.053.102 I ggml_metal_init: found device: Apple M4
0.00.053.105 I ggml_metal_init: picking default device: Apple M4
0.00.053.640 I ggml_metal_init: using embedded metal library
0.00.055.601 I ggml_metal_init: GPU name:   Apple M4
0.00.055.602 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.603 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.603 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.603 I ggml_metal_init: simdgroup reduction   = true
0.00.055.603 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.603 I ggml_metal_init: has bfloat            = true
0.00.055.603 I ggml_metal_init: use bfloat            = true
0.00.055.604 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.604 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.563 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.565 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.579 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.420 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.421 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.421 I llama_new_context_with_model: graph nodes  = 967
0.00.065.421 I llama_new_context_with_model: graph splits = 2
0.00.065.433 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.232.494 I 
0.00.232.546 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.232.554 I perplexity: tokenizing the input ..
0.00.240.507 I perplexity: tokenization took 7.952 ms
0.00.240.514 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.380.891 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.382.155 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.382.183 I llama_perf_context_print:        load time =     222.69 ms
0.00.382.184 I llama_perf_context_print: prompt eval time =     140.13 ms /   128 tokens (    1.09 ms per token,   913.41 tokens per second)
0.00.382.184 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.382.185 I llama_perf_context_print:       total time =     149.69 ms /   129 tokens
0.00.382.616 I ggml_metal_free: deallocating

real	0m0.400s
user	0m0.076s
sys	0m0.056s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4227 (2af44aac)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12570a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12570a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12570ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12570b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12570b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12570bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12570c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12570ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12570d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12570d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12570da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12570df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12570ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12570f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12570f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125710100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125710820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x125710f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125711660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125711e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125712550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125712c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125713390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125713c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125714350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125714610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125714c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125715890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125715dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125716090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125716530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1257167f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125717080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1257175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125717880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125717d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1257181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125718660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125718b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125718fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125719440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1257198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125719d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12571a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12571a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12571aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12571b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12571ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12571c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12571c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12571cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12571d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12571d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12571de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12571e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12571eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12571efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12571f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12571f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125720070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125720330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1257207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125720c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125721110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1257215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125721a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125721ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125722390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125722830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125722cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125723170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125723610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125723ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125723f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1257243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125724890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125724d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1257251d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125725670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125725b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125725fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125726450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1257268f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125726d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125727230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1257276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125727b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125728010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1257284b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125728950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125728df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125729290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125729730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125729bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12572a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12572a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12572a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12571b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12572b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12572b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12572b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12572bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12572c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12572c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12572cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12572d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12572d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12572d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12572de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12572e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12572e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12572ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12572f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12572f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12572fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12572fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125730340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1257307e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125730c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125731120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1257315c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125731a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125731f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1257323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125732840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125732ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125733180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125733620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125733ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125733f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125734400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1257348a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125734d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1257351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125735680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125735b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125735fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125736460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125736900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125736da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125737240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1257376e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125737b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125738020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1257384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125738960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125738e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1257392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125739740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125739be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12573a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12573a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12573a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12573af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12573b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12573b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12573bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12573c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12573c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12573cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12573d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12573da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12573e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12573e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12573eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12573f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12573f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12573fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1257402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125740830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125740d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1257412d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125741820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125741d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1257422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125742810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125742d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1257432b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125743800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125743d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1257442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1257447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125744d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125745290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1257457e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125745d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125746280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1257467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125746d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125747270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1257477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125747d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125748260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1257487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125748d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125749250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1257497a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125749cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12574a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12574a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12574ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12574b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12574b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12574bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12574c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12574c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12574ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12574d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12574d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12574dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12574e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12574e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12574eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12574f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12574f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12574fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1257501e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125750730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125750c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1257511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125751720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125751c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1257521c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125752710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125752bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125753050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1257534f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125753990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125753e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1257542d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125754770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125754c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1257550b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125755550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1257559f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125755e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125756330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125756880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125756fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1257576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125757de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125758500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1257587c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125758dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1257593e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.140.549 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125604ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125604f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1256053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125605830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125605ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125606110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125606580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1256069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125606e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125607380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1256077f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125607e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125608990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125609140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125609950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12560a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12560a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12560aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12560b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12560bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12560c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12560cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12560d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12560da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12560e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12560e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12560e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12560eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12560efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12560f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12560f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12560fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125610220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1256104e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125610950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125610dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125611230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1256116a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125611b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125611f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1256123f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125612860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125612cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125613140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1256135b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125613a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125613e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125614300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125614770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125614be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125615050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1256154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125615930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125615da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125616210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125616680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125616bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1256170f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125617560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1256179d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125617e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1256182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125618720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125618b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125619000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125619470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1256198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125619d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12561a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12561a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12561aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12561af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12561b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12561b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12561bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12561c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12561c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12561c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12561ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12561d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12561d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12561db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12561dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12561e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12561e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12561ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12561f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12561f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12561fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12561fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125620360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1256207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125620c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1256210b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125621520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x125621990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125621e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125622270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1256226e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125622b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125622fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125623430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1256238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125623d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125624180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1256245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125624a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125624ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125625340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1256257b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125625c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125626090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125626500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125626970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125626de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125627250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1256276c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125627b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125627fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125628410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125628880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125628cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125629160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1256295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125629a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125629eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12562a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12562a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12562ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12562b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12562b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12562b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12562bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12562c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12562c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12562cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12562cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12562d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12562d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12562dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12562e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12562e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12562ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12562ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12562f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12562f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12562fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125630050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1256304c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125630930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125630da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125631210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125631680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125631af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125631f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1256323d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125632840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125632cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125633120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x125633590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125633a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125633e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1256342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125634750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125634bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125635030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1256354a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125636030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1256362f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1256365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125636a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125636e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125637300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125637770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125637be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125638050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1256384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125638930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125638da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125639210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125639680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125639af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125639f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12563a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12563a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12563acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12563b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12563b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12563ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12563be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12563c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12563c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12563cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12563d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12563d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12563d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12563dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12563e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12563e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12563ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12563ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12563f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12563f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12563fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125640100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125640570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1256409e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125640e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1256412c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125641730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125641ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125642010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125642480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1256428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125642d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1256431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125643640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125643ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125643f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125644390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125644800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125644c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1256450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125645550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1256459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125645e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1256462a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125646710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125646b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125646ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125647460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1256478d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125647d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1256481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125648620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125648a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125648f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125649370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125649eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12564a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12564acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12564b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12564b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12564b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12564be00 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125604ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125604f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1256053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125605830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125605ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125606110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125606580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1256069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125606e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1256072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125607740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125607d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125608610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125608d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125609570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125609c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12560a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12560aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12560b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12560bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12560c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12560c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12560cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12560d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12560dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12560e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12560e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12560eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12560ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12560f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12560f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12560fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1256100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1256103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125610810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125610c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1256110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125611560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1256119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125611e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1256122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125612720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125612b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125613000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125613470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1256138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125613d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1256141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125614630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125614aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125614f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125615380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1256157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125615c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1256160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125616540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1256169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125616e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125617290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125617700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125617b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125617fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125618450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1256188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125618d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1256191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125619610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125619a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125619ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12561a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12561a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12561ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12561b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12561b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12561b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12561be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12561c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12561c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12561cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12561cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12561d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12561d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12561dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12561e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12561e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12561ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12561eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12561f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12561f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12561fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125620090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125620500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125620970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125620de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125621250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1256216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125621b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125621fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125622410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125622880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125622cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125623160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1256235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125623a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125623eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125624320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125624790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125624c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125625070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1256254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125625950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125625dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125626230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1256266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125626b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125626f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1256273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125627860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125627cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125628140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1256285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125628a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125628e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125629300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125629770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125629be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12562a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12562a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12562a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12562ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12562b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12562b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12562baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12562bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12562c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12562c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12562ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12562d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12562d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12562da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12562de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12562e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12562e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12562ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12562f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12562f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12562f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12562fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1256301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125630660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125630ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125630f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1256313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125631820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125631c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125632100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125632570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1256329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125632e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1256332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125633730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125633ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125634010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125634480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1256348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125634d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1256351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125635950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125635dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125636230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1256366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125636b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125636f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1256373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125637860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125637cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125638140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1256385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125638a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125638e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125639300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125639770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125639be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12563a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12563a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12563a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12563ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12563b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12563b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12563baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12563bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12563c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12563c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12563ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12563d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12563d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12563da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12563de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12563e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12563e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12563ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12563f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12563f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12563f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12563fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1256401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125640660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125640ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125640f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1256413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125641820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125641c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125642100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125642570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1256429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125642e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1256432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125643730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125643ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125644010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125644480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1256448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125644d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1256451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125645640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125645ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125645f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125646390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125646800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125646c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1256470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125647550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1256479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125647e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1256482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125648710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125648b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125648ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1256496e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125649dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12564a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12564abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12564b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12564b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12564b900 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.777s
user	0m0.297s
sys	0m0.288s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4227 (2af44aac)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1407104d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x140710990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x140710f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1407114f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x140711aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x140712050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x140712600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x140712bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x140713160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x140713660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x140713b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x140714060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x140714b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x140715330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x140715b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x140716260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x140716980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1407170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1407177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x140717f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1407186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x140718dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1407194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x140719d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14071a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14071a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14071ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14071b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14071bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14071c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14071c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14071c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14071d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14071d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14071d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14071de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14071e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14071e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14071ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14071f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14071f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14071fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14071fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x140720380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x140720640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x140720c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x140721260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x140721b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x140722190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1407227a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x140722db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1407233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1407239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x140723fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1407247d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x140724c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x140725110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1407253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1407259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1407261d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x140726490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x140726930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x140726dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x140727270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x140727710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x140727bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x140728050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1407284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x140728990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x140728e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1407292d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x140729770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x140729c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14072a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14072a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14072a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14072ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14072b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14072b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14072bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14072c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14072c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14072ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14072cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14072d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14072d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14072dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14072e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14072e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14072eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14072ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14072f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14072f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14072fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1407301d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x140730670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x140730b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x140721870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x140731160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x140731600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x140731aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x140731f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1407323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x140732880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x140732d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1407331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x140733660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x140733b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x140733fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x140734440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1407348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x140734d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x140735220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1407356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x140735b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x140736000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1407364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x140736940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x140736de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x140737280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x140737720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x140737bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x140738060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x140738500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1407389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x140738e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1407392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x140739780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x140739c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14073a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14073a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14073aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14073aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14073b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14073b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14073bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14073c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14073c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14073ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14073cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14073d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14073d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14073dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14073e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14073e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14073eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14073ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14073f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14073f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14073fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1407401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x140740680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x140740b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x140741070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1407415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x140741b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x140742060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x140742320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x140742930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x140742f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x140743550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x140743b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x140744170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x140744960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x140744e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1407452a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x140745740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x140745ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x140746440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x140746990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x140746ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x140747430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x140747980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x140747ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x140748420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x140748970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x140748ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x140749410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x140749960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x140749eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14074a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14074a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14074aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14074b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14074b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14074be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14074c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14074c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14074ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14074d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14074d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14074de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14074e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14074e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14074ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14074f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14074f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14074fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1407503a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1407508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x140750e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x140751390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1407518e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x140751e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x140752380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1407528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x140752e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x140753370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1407538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x140753e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x140754360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1407548b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x140754e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x140755350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1407558a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x140755df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x140756340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x140756890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x140756de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x140757330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x140757880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x140757dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x140758320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x140758870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x140758d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1407591b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x140759650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x140759af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x140759f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14075a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14075a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14075ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14075b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14075b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14075bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14075bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14075c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14075c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14075d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14075d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14075df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14075e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14075e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14075ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14075f540 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.084.575 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141804ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141804f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1418053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141805830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141805ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141806110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141806580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1418069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141806e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1418072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141807740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141807e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141808940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1418090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141809900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14180a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14180a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14180ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14180b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14180bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14180c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14180cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14180d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14180d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14180e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14180e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14180e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14180eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14180ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14180f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14180f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14180fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1418101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141810490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141810900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141810d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1418111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141811650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141811ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141811f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1418123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141812810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141812c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1418130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141813560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1418139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141813e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1418142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141814720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141814b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141815000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141815470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1418158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141815d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1418161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141816630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141816ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1418170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141817510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141817980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141817df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141818260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1418186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141818b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141818fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141819420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141819890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141819d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14181a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14181a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14181aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14181aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14181b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14181b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14181bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14181c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14181c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14181c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14181cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14181d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14181d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14181db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14181df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14181e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14181e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14181ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14181f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14181f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14181fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14181fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141820310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141820780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141820bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141821060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1418214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141821940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141821db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141822220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141822690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141822b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141822f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1418233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141823850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141823cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141824130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1418245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141824a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141824e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1418252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141825760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141825bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141826040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1418264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141826920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141826d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141827200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141827670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141827ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141827f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1418283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141828830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141828ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141829110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141829580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1418299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141829e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14182a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14182a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14182abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14182b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14182b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14182b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14182bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14182c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14182c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14182cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14182cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14182d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14182d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14182dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14182e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14182e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14182e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14182ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14182f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14182f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14182fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141830000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141830470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1418308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141830d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1418311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141831630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141831aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141831f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141832380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1418327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141832c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1418330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141833540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1418339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141833e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141834290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141834700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141834b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141834fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141835450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141835fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1418362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141836560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1418369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141836e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1418372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141837720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141837b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141838000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141838470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1418388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141838d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1418391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141839630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141839aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141839f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14183a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14183a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14183ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14183b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14183b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14183b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14183be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14183c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14183c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14183cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14183cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14183d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14183d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14183dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14183e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14183e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14183ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14183eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14183f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14183f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14183fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1418400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141840520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141840990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141840e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141841270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1418416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141841b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141841fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141842430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1418428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141842d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141843180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1418435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141843a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141843ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141844340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1418447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141844c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141845090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141845500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141845970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141845de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141846250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1418466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141846b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141846fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141847410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141847880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141847cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141848160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1418485d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141848a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141848eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141849320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141849e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14184a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14184aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14184b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14184b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14184b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14184bdb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1406053b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x140605820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x140605c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x140606100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x140606570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1406069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x140606e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1406072c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x140607730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x140607cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x140608120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1406087a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1406092c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x140609a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14060a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14060a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14060b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14060b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14060bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14060c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14060cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14060d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14060dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14060e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14060ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14060ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14060eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14060f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14060f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14060fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1406101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1406106e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x140610b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x140610e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x140611280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1406116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x140611b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x140611fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140612440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1406128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x140612d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x140613190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x140613600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x140613a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x140613ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x140614350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1406147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x140614c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1406150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x140615510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x140615980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x140615df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x140616260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1406166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x140616b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x140616fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x140617520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x140617a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x140617e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x140618300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x140618770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x140618be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x140619050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1406194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x140619930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x140619da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14061a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14061a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14061aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14061af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14061b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14061b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14061bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14061c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14061c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14061ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14061ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14061d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14061d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14061dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14061e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14061e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14061e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14061ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14061f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14061f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14061fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14061ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1406203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x140620820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x140620c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x140621100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x140621570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1406219e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x140621e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1406222c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x140622730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x140622ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x140623010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x140623480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1406238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x140623d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1406241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x140624640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x140624ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x140624f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x140625390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x140625800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x140625c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1406260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x140626550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1406269c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x140626e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1406272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x140627710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x140627b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x140627ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x140628460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1406288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x140628d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1406291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x140629620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x140629a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x140629f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14062a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14062a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14062ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14062b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14062b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14062b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14062be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14062c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14062c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14062cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14062cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14062d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14062d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14062dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14062e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14062e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14062ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14062eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14062f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14062f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14062fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1406300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x140630510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x140630980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x140630df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x140631260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1406316d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x140631b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x140631fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x140632420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x140632890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x140632d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x140633170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1406335e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x140633a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x140633ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x140634330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1406347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x140634c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x140635080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1406354f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x140635960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x140635dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x140636960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x140636c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x140636ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x140637350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1406377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x140637c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1406380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x140638510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x140638980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x140638df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x140639260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1406396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x140639b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x140639fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14063a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14063a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14063ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14063b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14063b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14063ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14063bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14063c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14063c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14063cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14063d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14063d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14063d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14063ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14063e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14063e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14063eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14063ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14063f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14063f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14063fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x140640150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1406405c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x140640a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x140640ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x140641310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x140641780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x140641bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x140642060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1406424d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x140642940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x140642db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x140643220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x140643690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x140643b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x140643f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1406443e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x140644850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x140644cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x140645130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1406455a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x140645a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x140645e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1406462f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x140646760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x140646bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x140647040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1406474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x140647920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x140647d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x140648200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x140648670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x140648ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x140648f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1406493c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x140649830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x140649ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14064a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14064af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14064b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14064bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14064c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14064c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14064c730 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.923s
user	0m0.238s
sys	0m0.139s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.55 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.57 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.12 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.16 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.25 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.14 user         0.04 sys
```
