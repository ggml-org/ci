Found 2 model parts in ../models-mnt/mpt/7B

* Loading part: pytorch_model-00001-of-00002.bin
Processing variable: transformer.wte.weight with shape:  (50432, 4096) -> float16
Processing variable: transformer.blocks.0.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.0.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.0.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.0.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.0.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.0.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.1.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.1.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.1.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.1.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.1.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.1.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.2.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.2.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.2.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.2.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.2.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.2.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.3.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.3.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.3.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.3.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.3.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.3.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.4.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.4.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.4.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.4.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.4.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.4.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.5.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.5.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.5.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.5.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.5.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.5.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.6.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.6.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.6.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.6.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.6.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.6.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.7.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.7.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.7.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.7.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.7.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.7.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.8.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.8.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.8.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.8.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.8.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.8.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.9.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.9.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.9.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.9.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.9.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.9.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.10.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.10.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.10.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.10.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.10.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.10.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.11.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.11.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.11.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.11.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.11.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.11.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.12.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.12.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.12.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.12.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.12.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.12.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.13.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.13.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.13.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.13.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.13.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.13.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.14.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.14.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.14.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.14.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.14.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.14.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.15.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.15.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.15.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.15.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.15.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.15.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.16.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.16.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.16.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.16.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.16.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.16.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.17.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.17.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.17.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.17.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.17.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.17.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.18.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.18.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.18.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.18.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.18.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.18.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.19.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.19.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.19.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.19.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.19.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.19.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.20.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.20.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.20.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.20.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.20.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.20.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.21.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.21.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.21.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.21.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.21.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.21.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.22.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.22.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.22.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.22.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.22.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.22.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.23.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.23.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.23.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.23.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.23.ffn.up_proj.weight with shape:  (16384, 4096) -> float16

* Loading part: pytorch_model-00002-of-00002.bin
Processing variable: transformer.blocks.23.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.24.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.24.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.24.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.24.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.24.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.24.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.25.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.25.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.25.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.25.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.25.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.25.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.26.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.26.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.26.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.26.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.26.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.26.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.27.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.27.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.27.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.27.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.27.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.27.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.28.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.28.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.28.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.28.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.28.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.28.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.29.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.29.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.29.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.29.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.29.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.29.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.30.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.30.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.30.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.30.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.30.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.30.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.blocks.31.norm_1.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.31.attn.Wqkv.weight with shape:  (12288, 4096) -> float16
Processing variable: transformer.blocks.31.attn.out_proj.weight with shape:  (4096, 4096) -> float16
Processing variable: transformer.blocks.31.norm_2.weight with shape:  (4096,) -> float32
Processing variable: transformer.blocks.31.ffn.up_proj.weight with shape:  (16384, 4096) -> float16
Processing variable: transformer.blocks.31.ffn.down_proj.weight with shape:  (4096, 16384) -> float16
Processing variable: transformer.norm_f.weight with shape:  (4096,) -> float32
Done. Output file: ../models-mnt/mpt/7B/ggml-model-f16.bin

mpt_model_quantize: loading model from '../models-mnt/mpt/7B/ggml-model-f16.bin'
mpt_model_quantize: d_model        = 4096
mpt_model_quantize: max_seq_len    = 2048
mpt_model_quantize: n_heads        = 32
mpt_model_quantize: n_layers       = 32
mpt_model_quantize: n_vocab        = 50432
mpt_model_quantize: alibi_bias_max = 8.000000
mpt_model_quantize: clip_qkv       = 0.000000
mpt_model_quantize: ftype (src) = 1
mpt_model_quantize: qntvr (src) = 0
mpt_model_quantize: ftype (dst) = 2002
mpt_model_quantize: qntvr (dst) = 2
mpt_model_quantize: quantizing tensors
                                          transformer.wte.weight - [ 4096, 50432,     1], type =    f16 size =   788.00 MB ->   110.81 MB | hist: 0.036 0.015 0.024 0.035 0.050 0.071 0.096 0.120 0.131 0.121 0.097 0.072 0.051 0.036 0.024 0.021 
                              transformer.blocks.0.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                           transformer.blocks.0.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.035 0.011 0.016 0.025 0.038 0.059 0.093 0.146 0.186 0.146 0.093 0.059 0.038 0.025 0.016 0.014 
                       transformer.blocks.0.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.098 0.117 0.125 0.117 0.098 0.075 0.054 0.036 0.023 0.019 
                              transformer.blocks.0.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                         transformer.blocks.0.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.040 0.058 0.078 0.096 0.109 0.113 0.109 0.096 0.078 0.058 0.040 0.026 0.022 
                       transformer.blocks.0.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.035 0.011 0.017 0.027 0.039 0.055 0.074 0.107 0.302 0.107 0.074 0.055 0.039 0.027 0.017 0.014 
                              transformer.blocks.1.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                           transformer.blocks.1.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 
                       transformer.blocks.1.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.014 0.022 0.035 0.052 0.075 0.099 0.119 0.128 0.119 0.099 0.075 0.052 0.035 0.022 0.018 
                              transformer.blocks.1.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                         transformer.blocks.1.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.040 0.057 0.077 0.096 0.109 0.114 0.110 0.096 0.078 0.057 0.040 0.026 0.021 
                       transformer.blocks.1.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 
                              transformer.blocks.2.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                           transformer.blocks.2.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.025 0.020 
                       transformer.blocks.2.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.014 0.022 0.035 0.052 0.074 0.099 0.119 0.128 0.120 0.099 0.075 0.052 0.035 0.022 0.018 
                              transformer.blocks.2.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                         transformer.blocks.2.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.025 0.021 
                       transformer.blocks.2.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.096 0.113 0.124 0.113 0.096 0.076 0.055 0.038 0.024 0.020 
                              transformer.blocks.3.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                           transformer.blocks.3.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 
                       transformer.blocks.3.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.076 0.098 0.116 0.123 0.116 0.098 0.076 0.054 0.037 0.024 0.019 
                              transformer.blocks.3.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                         transformer.blocks.3.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.039 0.026 0.021 
                       transformer.blocks.3.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 
                              transformer.blocks.4.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                           transformer.blocks.4.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.025 0.020 
                       transformer.blocks.4.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.122 0.115 0.098 0.076 0.054 0.037 0.024 0.020 
                              transformer.blocks.4.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                         transformer.blocks.4.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
                       transformer.blocks.4.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.121 0.115 0.098 0.076 0.055 0.037 0.024 0.020 
                              transformer.blocks.5.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                           transformer.blocks.5.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 
                       transformer.blocks.5.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 
                              transformer.blocks.5.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                         transformer.blocks.5.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
                       transformer.blocks.5.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.076 0.098 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.019 
                              transformer.blocks.6.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                           transformer.blocks.6.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 
                       transformer.blocks.6.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 
                              transformer.blocks.6.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                         transformer.blocks.6.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
                       transformer.blocks.6.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.076 0.098 0.116 0.123 0.116 0.098 0.076 0.054 0.036 0.023 0.019 
                              transformer.blocks.7.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                           transformer.blocks.7.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.021 
                       transformer.blocks.7.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
                              transformer.blocks.7.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                         transformer.blocks.7.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
                       transformer.blocks.7.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.098 0.116 0.124 0.116 0.098 0.076 0.054 0.036 0.023 0.019 
                              transformer.blocks.8.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                           transformer.blocks.8.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.021 
                       transformer.blocks.8.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 
                              transformer.blocks.8.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                         transformer.blocks.8.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
                       transformer.blocks.8.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.037 0.054 0.076 0.098 0.115 0.122 0.115 0.098 0.076 0.054 0.037 0.023 0.019 
                              transformer.blocks.9.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                           transformer.blocks.9.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 
                       transformer.blocks.9.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
                              transformer.blocks.9.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                         transformer.blocks.9.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
                       transformer.blocks.9.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.053 0.075 0.097 0.116 0.129 0.116 0.097 0.075 0.054 0.036 0.023 0.019 
                             transformer.blocks.10.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.10.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 
                      transformer.blocks.10.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
                             transformer.blocks.10.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.10.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.10.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.121 0.115 0.098 0.076 0.055 0.037 0.024 0.020 
                             transformer.blocks.11.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.11.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 
                      transformer.blocks.11.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
                             transformer.blocks.11.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.11.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.11.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.121 0.115 0.098 0.076 0.055 0.037 0.024 0.020 
                             transformer.blocks.12.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.12.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 
                      transformer.blocks.12.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
                             transformer.blocks.12.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.12.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.12.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.121 0.115 0.098 0.076 0.055 0.037 0.024 0.020 
                             transformer.blocks.13.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.13.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 
                      transformer.blocks.13.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 
                             transformer.blocks.13.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.13.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.13.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.121 0.115 0.098 0.076 0.055 0.037 0.024 0.020 
                             transformer.blocks.14.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.14.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.097 0.076 0.056 0.039 0.025 0.021 
                      transformer.blocks.14.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
                             transformer.blocks.14.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.14.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.14.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.121 0.115 0.098 0.076 0.055 0.037 0.024 0.020 
                             transformer.blocks.15.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.15.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 
                      transformer.blocks.15.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
                             transformer.blocks.15.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.15.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.15.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.121 0.115 0.098 0.076 0.055 0.037 0.024 0.020 
                             transformer.blocks.16.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.16.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 
                      transformer.blocks.16.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
                             transformer.blocks.16.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.16.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.16.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 
                             transformer.blocks.17.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.17.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 
                      transformer.blocks.17.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
                             transformer.blocks.17.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.17.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.17.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.121 0.115 0.098 0.076 0.055 0.037 0.024 0.020 
                             transformer.blocks.18.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.18.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 
                      transformer.blocks.18.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
                             transformer.blocks.18.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.18.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.18.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.019 
                             transformer.blocks.19.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.19.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.021 
                      transformer.blocks.19.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
                             transformer.blocks.19.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.19.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.19.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.121 0.115 0.098 0.076 0.055 0.037 0.024 0.020 
                             transformer.blocks.20.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.20.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 
                      transformer.blocks.20.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
                             transformer.blocks.20.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.20.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.20.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.121 0.115 0.098 0.076 0.055 0.037 0.024 0.019 
                             transformer.blocks.21.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.21.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 
                      transformer.blocks.21.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
                             transformer.blocks.21.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.21.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.21.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.121 0.115 0.098 0.076 0.055 0.037 0.024 0.020 
                             transformer.blocks.22.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.22.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.097 0.076 0.056 0.039 0.025 0.021 
                      transformer.blocks.22.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
                             transformer.blocks.22.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.22.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.22.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.114 0.121 0.114 0.098 0.076 0.055 0.037 0.024 0.020 
                             transformer.blocks.23.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.23.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 
                      transformer.blocks.23.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
                             transformer.blocks.23.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.23.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.23.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.114 0.120 0.114 0.098 0.076 0.055 0.037 0.024 0.020 
                             transformer.blocks.24.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.24.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 
                      transformer.blocks.24.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
                             transformer.blocks.24.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.24.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.24.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.024 0.020 
                             transformer.blocks.25.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.25.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 
                      transformer.blocks.25.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
                             transformer.blocks.25.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.25.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.25.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.077 0.056 0.038 0.024 0.020 
                             transformer.blocks.26.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.26.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 
                      transformer.blocks.26.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
                             transformer.blocks.26.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.26.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.26.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.024 0.020 
                             transformer.blocks.27.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.27.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 
                      transformer.blocks.27.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
                             transformer.blocks.27.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.27.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.27.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.098 0.114 0.120 0.114 0.098 0.076 0.055 0.038 0.024 0.020 
                             transformer.blocks.28.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.28.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 
                      transformer.blocks.28.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
                             transformer.blocks.28.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.28.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.28.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.121 0.115 0.098 0.076 0.055 0.037 0.024 0.019 
                             transformer.blocks.29.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.29.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 
                      transformer.blocks.29.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
                             transformer.blocks.29.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.29.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
                      transformer.blocks.29.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.014 0.022 0.035 0.053 0.075 0.099 0.118 0.125 0.118 0.099 0.075 0.053 0.035 0.022 0.018 
                             transformer.blocks.30.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.30.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 
                      transformer.blocks.30.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
                             transformer.blocks.30.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.30.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.113 0.097 0.077 0.056 0.038 0.025 0.020 
                      transformer.blocks.30.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.013 0.021 0.033 0.051 0.074 0.100 0.123 0.132 0.123 0.100 0.074 0.051 0.033 0.021 0.017 
                             transformer.blocks.31.norm_1.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                          transformer.blocks.31.attn.Wqkv.weight - [ 4096, 12288,     1], type =    f16 size =   192.00 MB ->    27.00 MB | hist: 0.036 0.014 0.023 0.036 0.053 0.075 0.098 0.118 0.126 0.118 0.098 0.075 0.053 0.036 0.023 0.019 
                      transformer.blocks.31.attn.out_proj.weight - [ 4096,  4096,     1], type =    f16 size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
                             transformer.blocks.31.norm_2.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
                        transformer.blocks.31.ffn.up_proj.weight - [ 4096, 16384,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.036 0.013 0.021 0.032 0.049 0.072 0.100 0.125 0.137 0.125 0.100 0.072 0.049 0.032 0.021 0.017 
                      transformer.blocks.31.ffn.down_proj.weight - [16384,  4096,     1], type =    f16 size =   256.00 MB ->    36.00 MB | hist: 0.035 0.012 0.019 0.030 0.045 0.068 0.097 0.132 0.162 0.131 0.096 0.067 0.045 0.029 0.018 0.016 
                                       transformer.norm_f.weight - [ 4096,     1,     1], type =    f32 size =    0.016 MB
ggml_common_quantize_0: model size  = 25365.02 MB
ggml_common_quantize_0: quant size  =  3567.83 MB | ftype = 2 (q4_0)
ggml_common_quantize_0: hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.123 0.114 0.097 0.076 0.055 0.038 0.024 0.020 

main: quantize time = 42456.10 ms
main:    total time = 42456.10 ms
+ ./bin/mpt --model ../models-mnt/mpt/7B/ggml-model-f16.bin -s 1234 -n 64 -p 'I believe the meaning of life is'
main: seed      = 1234
main: n_threads = 4
main: n_batch   = 8
main: n_ctx     = 512
main: n_predict = 64

mpt_model_load: loading model from '../models-mnt/mpt/7B/ggml-model-f16.bin' - please wait ...
mpt_model_load: d_model        = 4096
mpt_model_load: max_seq_len    = 2048
mpt_model_load: n_ctx          = 512
mpt_model_load: n_heads        = 32
mpt_model_load: n_layers       = 32
mpt_model_load: n_vocab        = 50432
mpt_model_load: alibi_bias_max = 8.000000
mpt_model_load: clip_qkv       = 0.000000
mpt_model_load: ftype          = 1
mpt_model_load: qntvr          = 0
mpt_model_load: ggml ctx size = 12939.11 MB
mpt_model_load: memory_size =   256.00 MB, n_mem = 16384
mpt_model_load: ........................extract_tests_from_file : No test file found.
test_gpt_tokenizer : 0 tests failed out of 0 tests.
 done
mpt_model_load: model size = 12683.02 MB / num tensors = 194

main: temp           = 0.800
main: top_k          = 50432
main: top_p          = 1.000
main: repeat_last_n  = 64
main: repeat_penalty = 1.020

main: number of tokens in prompt = 7
main: token[0] =     42
main: token[1] =   2868
main: token[2] =    253
main: token[3] =   4495
main: token[4] =    273
main: token[5] =   1495
main: token[6] =    310

I believe the meaning of life is to seek and follow Jesus Christ. It is my desire to share that message with as many people as I canto show them what life looks like when your heart is set on following Gods ultimate purpose. In 2002, I graduated from Trinity International University with a degree in Biblical Studies, then spent the next


main: sampled tokens =       64
main:  mem per token =   367052 bytes
main:      load time = 13360.59 ms
main:    sample time =   521.80 ms / 8.15 ms per token
main:      eval time = 43725.84 ms / 624.65 ms per token
main:     total time = 59979.82 ms

real	1m0.689s
user	3m4.787s
sys	0m9.061s
+ ./bin/mpt --model ../models-mnt/mpt/7B/ggml-model-q4_0.bin -s 1234 -n 64 -p 'I believe the meaning of life is'
main: seed      = 1234
main: n_threads = 4
main: n_batch   = 8
main: n_ctx     = 512
main: n_predict = 64

mpt_model_load: loading model from '../models-mnt/mpt/7B/ggml-model-q4_0.bin' - please wait ...
mpt_model_load: d_model        = 4096
mpt_model_load: max_seq_len    = 2048
mpt_model_load: n_ctx          = 512
mpt_model_load: n_heads        = 32
mpt_model_load: n_layers       = 32
mpt_model_load: n_vocab        = 50432
mpt_model_load: alibi_bias_max = 8.000000
mpt_model_load: clip_qkv       = 0.000000
mpt_model_load: ftype          = 2002
mpt_model_load: qntvr          = 2
mpt_model_load: ggml ctx size = 3823.92 MB
mpt_model_load: memory_size =   256.00 MB, n_mem = 16384
mpt_model_load: ........................extract_tests_from_file : No test file found.
test_gpt_tokenizer : 0 tests failed out of 0 tests.
 done
mpt_model_load: model size =  3567.83 MB / num tensors = 194

main: temp           = 0.800
main: top_k          = 50432
main: top_p          = 1.000
main: repeat_last_n  = 64
main: repeat_penalty = 1.020

main: number of tokens in prompt = 7
main: token[0] =     42
main: token[1] =   2868
main: token[2] =    253
main: token[3] =   4495
main: token[4] =    273
main: token[5] =   1495
main: token[6] =    310

I believe the meaning of life is to discover your gift and give it away.
I have always enjoyed helping people and making them feel comfortable with home decorating, especially as a first time parent.
I love my customers because they also want beautiful pieces. They are so fun and easy to work with.
I love that everything we do is from


main: sampled tokens =       64
main:  mem per token =   351692 bytes
main:      load time =  4366.08 ms
main:    sample time =   516.79 ms / 8.07 ms per token
main:      eval time = 17621.60 ms / 251.74 ms per token
main:     total time = 23467.47 ms

real	0m23.696s
user	1m14.685s
sys	0m2.890s
