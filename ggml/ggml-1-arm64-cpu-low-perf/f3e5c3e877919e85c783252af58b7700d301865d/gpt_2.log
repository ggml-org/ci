+ ./bin/gpt-2 --model -s 1234 -n 64 -t 4 -tt ../examples/prompts/gpt-2.txt
error: unknown argument: 1234
usage: ./bin/gpt-2 [options]

options:
  -h, --help            show this help message and exit
  -s SEED, --seed SEED  RNG seed (default: -1)
  -t N, --threads N     number of threads to use during computation (default: 4)
  -ngl N, --gpu-layers N  number of layers to offload to GPU on supported models (default: 0)
  -p PROMPT, --prompt PROMPT
                        prompt to start generation with (default: random)
  -f FNAME, --file FNAME
                        load prompt from a file
  -tt TOKEN_TEST, --token_test TOKEN_TEST
                        test tokenization
  -n N, --n_predict N   number of tokens to predict (default: 200)
  --top_k N             top-k sampling (default: 40)
  --top_p N             top-p sampling (default: 0.9)
  --temp N              temperature (default: 0.9)
  --repeat-last-n N     last n tokens to consider for penalize (default: 64, 0 = disabled)
  --repeat-penalty N    penalize repeat sequence of tokens (default: 1.00, 1.0 = disabled)
  -b N, --batch_size N  batch size for prompt processing (default: 8)
  -m FNAME, --model FNAME
                        model path (default: -s)


real	0m0.003s
user	0m0.003s
sys	0m0.001s
+ ./bin/gpt-2 --model -s 1234 -n 64 -t 4 -p 'I believe the meaning of life is'
error: unknown argument: 1234
usage: ./bin/gpt-2 [options]

options:
  -h, --help            show this help message and exit
  -s SEED, --seed SEED  RNG seed (default: -1)
  -t N, --threads N     number of threads to use during computation (default: 4)
  -ngl N, --gpu-layers N  number of layers to offload to GPU on supported models (default: 0)
  -p PROMPT, --prompt PROMPT
                        prompt to start generation with (default: random)
  -f FNAME, --file FNAME
                        load prompt from a file
  -tt TOKEN_TEST, --token_test TOKEN_TEST
                        test tokenization
  -n N, --n_predict N   number of tokens to predict (default: 200)
  --top_k N             top-k sampling (default: 40)
  --top_p N             top-p sampling (default: 0.9)
  --temp N              temperature (default: 0.9)
  --repeat-last-n N     last n tokens to consider for penalize (default: 64, 0 = disabled)
  --repeat-penalty N    penalize repeat sequence of tokens (default: 1.00, 1.0 = disabled)
  -b N, --batch_size N  batch size for prompt processing (default: 8)
  -m FNAME, --model FNAME
                        model path (default: -s)


real	0m0.003s
user	0m0.003s
sys	0m0.001s
