extract_tests_from_file : No test file found.
test_gpt_tokenizer : 0 tests failed out of 0 tests.
main: seed = 1689517287
gpt2_model_load: loading model from '../models/gpt-2/ggml-model-gpt-2-117M.bin'
gpt2_model_load: n_vocab = 50257
gpt2_model_load: n_ctx   = 1024
gpt2_model_load: n_embd  = 768
gpt2_model_load: n_head  = 12
gpt2_model_load: n_layer = 12
gpt2_model_load: ftype   = 1
gpt2_model_load: qntvr   = 0
gpt2_model_load: ggml tensor size = 240 bytes
gpt2_model_load: ggml ctx size = 384.77 MB
gpt2_model_load: memory size =    72.00 MB, n_mem = 12288
gpt2_model_load: model size  =   239.08 MB
main: prompt: 'When'
main: number of tokens in prompt = 1, first 8 tokens: 2215 

When the second party in question is the "non-government group" of the United States, we must ask the same question.

The question of whether the

main: mem per token =  2016924 bytes
main:     load time =   151.23 ms
main:   sample time =     9.14 ms
main:  predict time =   402.85 ms / 12.59 ms per token
main:    total time =   612.77 ms
