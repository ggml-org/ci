rm: cannot remove '/home/ggml/results/ggml/bb/f9afb379d5b519729b24ff1a1e2684ae395c3d/ggml-4-x86-cuda-v100/*.log': No such file or directory
rm: cannot remove '/home/ggml/results/ggml/bb/f9afb379d5b519729b24ff1a1e2684ae395c3d/ggml-4-x86-cuda-v100/*.exit': No such file or directory
rm: cannot remove '/home/ggml/results/ggml/bb/f9afb379d5b519729b24ff1a1e2684ae395c3d/ggml-4-x86-cuda-v100/*.md': No such file or directory
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: accelerate==0.19.0 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/ggml/requirements.txt (line 1)) (0.19.0)
Requirement already satisfied: numpy==1.24.3 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/ggml/requirements.txt (line 2)) (1.24.3)
Requirement already satisfied: sentencepiece==0.1.98 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/ggml/requirements.txt (line 3)) (0.1.98)
Requirement already satisfied: torch==2.0.1 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/ggml/requirements.txt (line 4)) (2.0.1)
Requirement already satisfied: torchaudio==2.0.2 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/ggml/requirements.txt (line 5)) (2.0.2)
Requirement already satisfied: torchvision==0.15.2 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/ggml/requirements.txt (line 6)) (0.15.2)
Requirement already satisfied: transformers==4.29.2 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/ggml/requirements.txt (line 7)) (4.29.2)
Requirement already satisfied: gguf==0.4.5 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/ggml/requirements.txt (line 8)) (0.4.5)
Requirement already satisfied: packaging>=20.0 in /home/ggml/.local/lib/python3.10/site-packages (from accelerate==0.19.0->-r /home/ggml/work/ggml/requirements.txt (line 1)) (23.1)
Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate==0.19.0->-r /home/ggml/work/ggml/requirements.txt (line 1)) (5.4.1)
Requirement already satisfied: psutil in /home/ggml/.local/lib/python3.10/site-packages (from accelerate==0.19.0->-r /home/ggml/work/ggml/requirements.txt (line 1)) (5.9.5)
Requirement already satisfied: sympy in /home/ggml/.local/lib/python3.10/site-packages (from torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (1.12)
Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/ggml/.local/lib/python3.10/site-packages (from torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (10.2.10.91)
Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/ggml/.local/lib/python3.10/site-packages (from torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (11.4.0.1)
Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/ggml/.local/lib/python3.10/site-packages (from torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (11.7.4.91)
Requirement already satisfied: triton==2.0.0 in /home/ggml/.local/lib/python3.10/site-packages (from torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (2.0.0)
Requirement already satisfied: filelock in /home/ggml/.local/lib/python3.10/site-packages (from torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (3.12.2)
Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/ggml/.local/lib/python3.10/site-packages (from torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (2.14.3)
Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/ggml/.local/lib/python3.10/site-packages (from torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (11.7.101)
Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/ggml/.local/lib/python3.10/site-packages (from torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (10.9.0.58)
Requirement already satisfied: typing-extensions in /home/ggml/.local/lib/python3.10/site-packages (from torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (4.7.1)
Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (3.0.3)
Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ggml/.local/lib/python3.10/site-packages (from torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (8.5.0.96)
Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ggml/.local/lib/python3.10/site-packages (from torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (11.7.99)
Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ggml/.local/lib/python3.10/site-packages (from torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (11.10.3.66)
Requirement already satisfied: networkx in /home/ggml/.local/lib/python3.10/site-packages (from torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (3.1)
Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ggml/.local/lib/python3.10/site-packages (from torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (11.7.99)
Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/ggml/.local/lib/python3.10/site-packages (from torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (11.7.91)
Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ggml/.local/lib/python3.10/site-packages (from torchvision==0.15.2->-r /home/ggml/work/ggml/requirements.txt (line 6)) (10.0.0)
Requirement already satisfied: requests in /home/ggml/.local/lib/python3.10/site-packages (from torchvision==0.15.2->-r /home/ggml/work/ggml/requirements.txt (line 6)) (2.31.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/ggml/.local/lib/python3.10/site-packages (from transformers==4.29.2->-r /home/ggml/work/ggml/requirements.txt (line 7)) (0.16.4)
Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ggml/.local/lib/python3.10/site-packages (from transformers==4.29.2->-r /home/ggml/work/ggml/requirements.txt (line 7)) (0.13.3)
Requirement already satisfied: regex!=2019.12.17 in /home/ggml/.local/lib/python3.10/site-packages (from transformers==4.29.2->-r /home/ggml/work/ggml/requirements.txt (line 7)) (2023.6.3)
Requirement already satisfied: tqdm>=4.27 in /home/ggml/.local/lib/python3.10/site-packages (from transformers==4.29.2->-r /home/ggml/work/ggml/requirements.txt (line 7)) (4.65.0)
Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (59.6.0)
Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (0.37.1)
Requirement already satisfied: lit in /home/ggml/.local/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (16.0.6)
Requirement already satisfied: cmake in /home/ggml/.local/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (3.27.0)
Requirement already satisfied: fsspec in /home/ggml/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2->-r /home/ggml/work/ggml/requirements.txt (line 7)) (2023.6.0)
Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ggml/.local/lib/python3.10/site-packages (from requests->torchvision==0.15.2->-r /home/ggml/work/ggml/requirements.txt (line 6)) (2.1.0)
Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision==0.15.2->-r /home/ggml/work/ggml/requirements.txt (line 6)) (3.3)
Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision==0.15.2->-r /home/ggml/work/ggml/requirements.txt (line 6)) (2020.6.20)
Requirement already satisfied: charset-normalizer<4,>=2 in /home/ggml/.local/lib/python3.10/site-packages (from requests->torchvision==0.15.2->-r /home/ggml/work/ggml/requirements.txt (line 6)) (3.3.2)
Requirement already satisfied: mpmath>=0.19 in /home/ggml/.local/lib/python3.10/site-packages (from sympy->torch==2.0.1->-r /home/ggml/work/ggml/requirements.txt (line 4)) (1.3.0)
+ gg_run_ctest_debug
+ tee /home/ggml/results/ggml/bb/f9afb379d5b519729b24ff1a1e2684ae395c3d/ggml-4-x86-cuda-v100/ctest_debug.log
+ cd /home/ggml/work/ggml
+ rm -rf build-ci-debug
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ tee -a /home/ggml/results/ggml/bb/f9afb379d5b519729b24ff1a1e2684ae395c3d/ggml-4-x86-cuda-v100/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DGGML_CUBLAS=ON ..
CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):
  Compatibility with CMake < 3.5 will be removed from a future version of
  CMake.

  Update the VERSION argument <min> value or use a ...<max> suffix to tell
  CMake that the project does not need compatibility with older versions.


-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- CMAKE_SYSTEM_PROCESSOR: x86_64
GNU ld (GNU Binutils for Ubuntu) 2.38
-- x86 detected
-- Linux detected
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.140") 
-- cuBLAS found
-- The CUDA compiler identification is NVIDIA 12.2.140
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- GGML CUDA sources found
-- GGML Configuring CUDA architectures 52;61;70
-- x86 detected
-- Linux detected
-- Configuring done (3.0s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/ggml/build-ci-debug

real	0m3.177s
user	0m2.477s
sys	0m0.705s
+ tee -a /home/ggml/results/ggml/bb/f9afb379d5b519729b24ff1a1e2684ae395c3d/ggml-4-x86-cuda-v100/ctest_debug-make.log
+ make -j
[  0%] Building CXX object examples/CMakeFiles/common.dir/common.cpp.o
[  1%] Building C object src/CMakeFiles/ggml.dir/ggml.c.o
[  2%] Building C object src/CMakeFiles/ggml.dir/ggml-alloc.c.o
[  3%] Building C object src/CMakeFiles/ggml.dir/ggml-backend.c.o
[  4%] Building C object src/CMakeFiles/ggml.dir/ggml-quants.c.o
[  4%] Building CUDA object src/CMakeFiles/ggml.dir/ggml-cuda.cu.o
[  5%] Linking CXX static library libcommon.a
[  5%] Built target common
/home/ggml/work/ggml/src/ggml-cuda.cu: In function â€˜bool ggml_cuda_compute_forward(ggml_compute_params*, ggml_tensor*)â€™:
/home/ggml/work/ggml/src/ggml-cuda.cu:8035:17: warning: format â€˜%dâ€™ expects argument of type â€˜intâ€™, but argument 5 has type â€˜int64_tâ€™ {aka â€˜long intâ€™} [-Wformat=]
 8035 |             fprintf(stderr, "%s: cannot compute %s: src0->ne[3] = %d, src1->ne[3] = %d - fallback to CPU\n", __func__, tensor->name, tensor->src[0]->ne[3], tensor->src[1]->ne[3]);
      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                          ~~~~~~~~~~~~~~~~~~~~~~~~~~~
      |                                                                                                                                                    |
      |                                                                                                                                                    int64_t {aka long int}
/home/ggml/work/ggml/src/ggml-cuda.cu:8035:17: warning: format â€˜%dâ€™ expects argument of type â€˜intâ€™, but argument 6 has type â€˜int64_tâ€™ {aka â€˜long intâ€™} [-Wformat=]
 8035 |             fprintf(stderr, "%s: cannot compute %s: src0->ne[3] = %d, src1->ne[3] = %d - fallback to CPU\n", __func__, tensor->name, tensor->src[0]->ne[3], tensor->src[1]->ne[3]);
      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~
      |                                                                                                                                                                                 |
      |                                                                                                                                                                                 int64_t {aka long int}
[  6%] Linking CUDA shared library libggml.so
[  6%] Built target ggml
[  7%] Building C object tests/CMakeFiles/test-vec0.dir/test-vec0.c.o
[  8%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[  9%] Building CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o
[ 11%] Building C object tests/CMakeFiles/test-vec1.dir/test-vec1.c.o
[ 11%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 11%] Building C object tests/CMakeFiles/test-mul-mat0.dir/test-mul-mat0.c.o
[ 12%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 13%] Building C object tests/CMakeFiles/test-mul-mat2.dir/test-mul-mat2.c.o
[ 14%] Building C object tests/CMakeFiles/test0.dir/test0.c.o
[ 15%] Building C object tests/CMakeFiles/test1.dir/test1.c.o
[ 15%] Building C object tests/CMakeFiles/test2.dir/test2.c.o
[ 16%] Building C object tests/CMakeFiles/test3.dir/test3.c.o
[ 17%] Building C object tests/CMakeFiles/test-pool.dir/test-pool.c.o
[ 17%] Building C object tests/CMakeFiles/test-conv-transpose.dir/test-conv-transpose.c.o
[ 17%] Building C object tests/CMakeFiles/test-rel-pos.dir/test-rel-pos.c.o
[ 18%] Linking C executable ../bin/test-vec0
[ 19%] Building CXX object tests/CMakeFiles/test-conv1d.dir/test-conv1d.cpp.o
[ 20%] Building C object tests/CMakeFiles/test-customop.dir/test-customop.c.o
[ 21%] Building C object tests/CMakeFiles/test-xpos.dir/test-xpos.c.o
[ 22%] Building CXX object tests/CMakeFiles/test-mul-mat.dir/test-mul-mat.cpp.o
[ 23%] Linking C executable ../bin/test0
[ 24%] Building CXX object tests/CMakeFiles/test-conv2d.dir/test-conv2d.cpp.o
[ 25%] Linking C executable ../bin/test-mul-mat0
[ 26%] Building CXX object tests/CMakeFiles/test-backend-buffer.dir/test-backend-buffer.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 28%] Building CXX object examples/CMakeFiles/common-ggml.dir/common-ggml.cpp.o
[ 29%] Linking C executable ../bin/test-rel-pos
[ 30%] Building CXX object examples/mnist/CMakeFiles/mnist.dir/main.cpp.o
[ 31%] Linking C executable ../bin/test3
[ 32%] Building CXX object examples/whisper/CMakeFiles/whisper-cpp.dir/whisper.cpp.o
[ 33%] Linking C executable ../bin/test2
[ 34%] Linking CXX executable ../bin/test-opt
[ 34%] Building CXX object examples/mnist/CMakeFiles/mnist-cnn.dir/main-cnn.cpp.o
[ 34%] Built target test-vec0
[ 35%] Building CXX object examples/mnist/CMakeFiles/mnist-cpu.dir/main-cpu.cpp.o
[ 35%] Building CXX object examples/sam/CMakeFiles/sam.dir/main.cpp.o
[ 36%] Building CXX object examples/yolo/CMakeFiles/yolov3-tiny.dir/yolov3-tiny.cpp.o
[ 37%] Linking C executable ../bin/test-customop
[ 38%] Building CXX object examples/yolo/CMakeFiles/yolov3-tiny.dir/yolo-image.cpp.o
[ 39%] Linking C executable ../bin/test-conv-transpose
[ 40%] Linking C executable ../bin/test1
[ 41%] Linking C executable ../bin/test-pool
[ 42%] Linking C executable ../bin/test-xpos
[ 43%] Linking CXX executable ../bin/test-backend-buffer
[ 43%] Built target test-mul-mat0
[ 44%] Linking CXX executable ../bin/test-grad0
[ 44%] Built target test-rel-pos
[ 44%] Built target test0
[ 44%] Built target test3
[ 44%] Built target test2
[ 44%] Built target test-backend-buffer
[ 44%] Built target test1
[ 44%] Built target test-opt
[ 44%] Built target test-customop
[ 44%] Built target test-conv-transpose
[ 44%] Built target test-pool
[ 44%] Built target test-grad0
[ 44%] Built target test-xpos
[ 45%] Linking C executable ../bin/test-vec1
[ 46%] Linking C executable ../bin/test-mul-mat2
[ 47%] Linking CXX executable ../bin/test-quantize-fns
[ 48%] Linking CXX executable ../bin/test-mul-mat
[ 48%] Built target test-mul-mat2
[ 48%] Built target test-vec1
[ 48%] Built target test-mul-mat
[ 49%] Linking CXX executable ../bin/test-conv2d
[ 49%] Built target test-quantize-fns
[ 50%] Linking CXX executable ../bin/test-conv1d
[ 51%] Linking CXX executable ../../bin/mnist-cpu
[ 51%] Built target test-conv2d
[ 51%] Built target test-conv1d
[ 51%] Built target mnist-cpu
[ 52%] Linking CXX executable ../../bin/mnist-cnn
[ 53%] Linking CXX executable ../../bin/mnist
[ 53%] Built target mnist-cnn
[ 53%] Built target mnist
[ 54%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Linking CXX executable ../bin/test-quantize-perf
[ 55%] Built target test-backend-ops
[ 55%] Built target test-quantize-perf
[ 56%] Linking CXX executable ../../bin/yolov3-tiny
[ 56%] Built target yolov3-tiny
[ 57%] Linking CXX executable ../../bin/sam
[ 57%] Built target sam
[ 58%] Linking CXX static library libcommon-ggml.a
[ 58%] Built target common-ggml
[ 59%] Building CXX object examples/gpt-2/CMakeFiles/gpt-2-ctx.dir/main-ctx.cpp.o
[ 60%] Building CXX object examples/gpt-2/CMakeFiles/gpt-2-alloc.dir/main-alloc.cpp.o
[ 61%] Building CXX object examples/gpt-2/CMakeFiles/gpt-2-backend.dir/main-backend.cpp.o
[ 62%] Building CXX object examples/gpt-2/CMakeFiles/gpt-2-backend2.dir/main.cpp.o
[ 62%] Building CXX object examples/gpt-2/CMakeFiles/gpt-2-quantize.dir/quantize.cpp.o
[ 63%] Building CXX object examples/gpt-2/CMakeFiles/gpt-2-batched.dir/main-batched.cpp.o
[ 64%] Building CXX object examples/gpt-j/CMakeFiles/gpt-j-quantize.dir/quantize.cpp.o
[ 65%] Building CXX object examples/gpt-j/CMakeFiles/gpt-j.dir/main.cpp.o
[ 66%] Building CXX object examples/whisper/CMakeFiles/whisper-quantize.dir/quantize.cpp.o
[ 67%] Building CXX object examples/gpt-neox/CMakeFiles/gpt-neox.dir/main.cpp.o
[ 68%] Building CXX object examples/dolly-v2/CMakeFiles/dollyv2.dir/main.cpp.o
[ 69%] Building CXX object examples/gpt-neox/CMakeFiles/gpt-neox-quantize.dir/quantize.cpp.o
[ 70%] Building CXX object examples/replit/CMakeFiles/replit.dir/main.cpp.o
[ 71%] Building CXX object examples/mpt/CMakeFiles/mpt.dir/main.cpp.o
[ 72%] Building CXX object examples/replit/CMakeFiles/replit-quantize.dir/quantize.cpp.o
[ 73%] Building CXX object examples/mpt/CMakeFiles/mpt-quantize.dir/quantize.cpp.o
[ 74%] Building CXX object examples/dolly-v2/CMakeFiles/dollyv2-quantize.dir/quantize.cpp.o
[ 75%] Building CXX object examples/starcoder/CMakeFiles/starcoder.dir/main.cpp.o
[ 76%] Building CXX object examples/starcoder/CMakeFiles/starcoder-mmap.dir/starcoder-mmap.cpp.o
[ 77%] Building CXX object examples/starcoder/CMakeFiles/starcoder-quantize.dir/quantize.cpp.o
[ 78%] Linking CXX executable ../../bin/mpt-quantize
[ 79%] Linking CXX executable ../../bin/replit-quantize
[ 80%] Linking CXX executable ../../bin/gpt-j-quantize
[ 81%] Linking CXX executable ../../bin/gpt-2-quantize
[ 82%] Linking CXX executable ../../bin/gpt-2-alloc
[ 83%] Linking CXX executable ../../bin/dollyv2-quantize
[ 84%] Linking CXX executable ../../bin/starcoder-quantize
[ 84%] Built target mpt-quantize
[ 84%] Built target replit-quantize
[ 85%] Linking CXX executable ../../bin/gpt-2-ctx
[ 86%] Linking CXX executable ../../bin/gpt-j
[ 87%] Linking CXX executable ../../bin/gpt-neox
[ 87%] Built target gpt-2-quantize
[ 88%] Linking CXX executable ../../bin/gpt-2-backend
[ 89%] Linking CXX executable ../../bin/starcoder-mmap
[ 89%] Built target gpt-j-quantize
[ 89%] Built target gpt-2-alloc
[ 90%] Linking CXX executable ../../bin/dollyv2
[ 91%] Linking CXX executable ../../bin/gpt-neox-quantize
[ 91%] Built target dollyv2-quantize
[ 92%] Linking CXX executable ../../bin/whisper-quantize
[ 92%] Built target starcoder-quantize
[ 92%] Built target gpt-j
[ 92%] Built target gpt-neox
[ 93%] Linking CXX executable ../../bin/mpt
[ 93%] Built target gpt-2-backend
[ 93%] Built target gpt-2-ctx
[ 93%] Built target dollyv2
[ 94%] Linking CXX executable ../../bin/starcoder
[ 94%] Built target whisper-quantize
[ 94%] Built target gpt-neox-quantize
[ 94%] Built target starcoder-mmap
[ 95%] Linking CXX executable ../../bin/gpt-2-backend2
[ 96%] Linking CXX executable ../../bin/gpt-2-batched
[ 97%] Linking CXX executable ../../bin/replit
[ 97%] Built target mpt
[ 97%] Built target starcoder
[ 97%] Built target gpt-2-backend2
[ 97%] Built target replit
[ 97%] Built target gpt-2-batched
[ 98%] Linking CXX static library libwhisper-cpp.a
[ 98%] Built target whisper-cpp
[ 99%] Building CXX object examples/whisper/CMakeFiles/whisper.dir/main.cpp.o
[100%] Linking CXX executable ../../bin/whisper
[100%] Built target whisper

real	0m37.119s
user	1m18.342s
sys	0m7.473s
+ '[' '!' -z ']'
+ tee -a /home/ggml/results/ggml/bb/f9afb379d5b519729b24ff1a1e2684ae395c3d/ggml-4-x86-cuda-v100/ctest_debug-ctest.log
+ ctest --output-on-failure -E test-opt
Test project /home/ggml/work/ggml/build-ci-debug
      Start  1: test-grad0
 1/19 Test  #1: test-grad0 .......................   Passed    6.50 sec
      Start  2: test-quantize-fns
 2/19 Test  #2: test-quantize-fns ................   Passed    0.57 sec
      Start  3: test-quantize-perf
 3/19 Test  #3: test-quantize-perf ...............   Passed    0.78 sec
      Start  4: test-mul-mat0
 4/19 Test  #4: test-mul-mat0 ....................   Passed    1.09 sec
      Start  5: test-mul-mat2
 5/19 Test  #5: test-mul-mat2 ....................   Passed    7.85 sec
      Start  6: test0
 6/19 Test  #6: test0 ............................   Passed    0.53 sec
      Start  7: test1
 7/19 Test  #7: test1 ............................   Passed    0.54 sec
      Start  8: test2
 8/19 Test  #8: test2 ............................   Passed    7.77 sec
      Start  9: test3
 9/19 Test  #9: test3 ............................   Passed    1.01 sec
      Start 10: test-pool
10/19 Test #10: test-pool ........................   Passed    0.52 sec
      Start 11: test-conv-transpose
11/19 Test #11: test-conv-transpose ..............   Passed    0.54 sec
      Start 12: test-rel-pos
12/19 Test #12: test-rel-pos .....................   Passed    0.53 sec
      Start 13: test-customop
13/19 Test #13: test-customop ....................   Passed    0.53 sec
      Start 14: test-xpos
14/19 Test #14: test-xpos ........................   Passed    0.55 sec
      Start 15: test-conv1d
15/19 Test #15: test-conv1d ......................   Passed    0.55 sec
      Start 16: test-conv2d
16/19 Test #16: test-conv2d ......................   Passed    0.54 sec
      Start 17: test-mul-mat
17/19 Test #17: test-mul-mat .....................   Passed    0.55 sec
      Start 18: test-backend-buffer
18/19 Test #18: test-backend-buffer ..............   Passed    0.56 sec
      Start 19: test-backend-ops
19/19 Test #19: test-backend-ops .................***Failed    2.93 sec
ggml_backend_register: registered backend CPU
ggml_backend_register: registered backend CUDA0
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0
Testing 2 backends

Backend 1/2 (CPU)
  Backend name: CPU
  ABS(type=f32,ne=[128,10,10,10]): [1;32mOK[0m
  SGN(type=f32,ne=[128,10,10,10]): [1;32mOK[0m
  NEG(type=f32,ne=[128,10,10,10]): [1;32mOK[0m
  STEP(type=f32,ne=[128,10,10,10]): [1;32mOK[0m
  TANH(type=f32,ne=[128,10,10,10]): [1;32mOK[0m
  ELU(type=f32,ne=[128,10,10,10]): [1;32mOK[0m
  RELU(type=f32,ne=[128,10,10,10]): [1;32mOK[0m
  GELU(type=f32,ne=[128,10,10,10]): [1;32mOK[0m
  GELU_QUICK(type=f32,ne=[128,10,10,10]): [1;32mOK[0m
  SILU(type=f32,ne=[128,10,10,10]): [1;32mOK[0m
  LEAKY(type=f32,ne=[128,10,10,10]): [1;32mOK[0m
  GET_ROWS(type=f32,n=10,m=5,r=3): [1;32mOK[0m
  GET_ROWS(type=f32,n=16,m=5,r=3): [1;32mOK[0m
  GET_ROWS(type=f16,n=10,m=5,r=3): [1;32mOK[0m
  GET_ROWS(type=f16,n=16,m=5,r=3): [1;32mOK[0m
  REPEAT(type=f32,ne=[10,10,10,10],nr=[2,2,2,2]): [1;32mOK[0m
  DUP(type=f32,ne=[10,10,10,1]): [1;32mOK[0m
  CPY(type_src=f32,type_dst=f32,ne=[10,10,10,1]): [1;32mOK[0m
  CONT(type=f32,ne=[10,10,10,1]): [1;32mOK[0m
  ADD(type=f32,ne=[16,10,1,1],nr=[1,1,1,1]): [1;32mOK[0m
  ADD(type=f32,ne=[16,10,10,1],nr=[1,1,1,1]): [1;32mOK[0m
  ADD(type=f32,ne=[16,10,10,10],nr=[1,1,1,1]): [1;32mOK[0m
  ADD(type=f32,ne=[16,10,10,10],nr=[1,2,1,1]): [1;32mOK[0m
  ADD(type=f32,ne=[16,10,10,10],nr=[1,1,2,1]): [1;32mOK[0m
  ADD(type=f32,ne=[16,10,10,10],nr=[1,1,1,2]): [1;32mOK[0m
  ADD(type=f32,ne=[16,10,10,10],nr=[1,1,2,2]): [1;32mOK[0m
  ADD(type=f32,ne=[16,10,10,10],nr=[1,2,2,2]): [1;32mOK[0m
  MUL(type=f32,ne=[16,10,1,1],nr=[1,1,1,1]): [1;32mOK[0m
  MUL(type=f32,ne=[16,10,10,1],nr=[1,1,1,1]): [1;32mOK[0m
  MUL(type=f32,ne=[16,10,10,10],nr=[1,1,1,1]): [1;32mOK[0m
  MUL(type=f32,ne=[16,10,10,10],nr=[1,2,1,1]): [1;32mOK[0m
  MUL(type=f32,ne=[16,10,10,10],nr=[1,1,2,1]): [1;32mOK[0m
  MUL(type=f32,ne=[16,10,10,10],nr=[1,1,1,2]): [1;32mOK[0m
  MUL(type=f32,ne=[16,10,10,10],nr=[1,1,2,2]): [1;32mOK[0m
  MUL(type=f32,ne=[16,10,10,10],nr=[1,2,2,2]): [1;32mOK[0m
  SCALE(type=f32,ne=[10,10,10,10]): [1;32mOK[0m
  NORM(type=f32,ne=[64,10,10,10],eps=0.000001): [1;32mOK[0m
  RMS_NORM(type=f32,ne=[64,10,10,10],eps=0.000001): [1;32mOK[0m
  NORM(type=f32,ne=[64,10,10,10],eps=0.000010): [1;32mOK[0m
  RMS_NORM(type=f32,ne=[64,10,10,10],eps=0.000010): [1;32mOK[0m
  NORM(type=f32,ne=[64,10,10,10],eps=0.001000): [1;32mOK[0m
  RMS_NORM(type=f32,ne=[64,10,10,10],eps=0.001000): [1;32mOK[0m
  NORM(type=f32,ne=[64,10,10,10],eps=0.100000): [1;32mOK[0m
  RMS_NORM(type=f32,ne=[64,10,10,10],eps=0.100000): [1;32mOK[0m
  MUL_MAT(type_a=f32,type_b=f32,m=32,n=32,k=32,bs=[1,1],nr=[1,1]): [1;32mOK[0m
  MUL_MAT(type_a=f32,type_b=f32,m=32,n=32,k=32,bs=[10,1],nr=[1,1]): [1;32mOK[0m
  MUL_MAT(type_a=f32,type_b=f32,m=32,n=32,k=32,bs=[10,1],nr=[2,1]): [1;32mOK[0m
  MUL_MAT(type_a=f32,type_b=f32,m=32,n=32,k=32,bs=[10,10],nr=[1,1]): [1;32mOK[0m
  MUL_MAT(type_a=f32,type_b=f32,m=32,n=32,k=32,bs=[10,10],nr=[2,1]): [1;32mOK[0m
  MUL_MAT(type_a=f32,type_b=f32,m=32,n=32,k=32,bs=[10,10],nr=[1,2]): [1;32mOK[0m
  MUL_MAT(type_a=f32,type_b=f32,m=32,n=32,k=32,bs=[10,10],nr=[2,2]): [1;32mOK[0m
  MUL_MAT(type_a=f16,type_b=f32,m=32,n=32,k=32,bs=[1,1],nr=[1,1]): [1;32mOK[0m
  MUL_MAT(type_a=f16,type_b=f32,m=32,n=32,k=32,bs=[10,1],nr=[1,1]): [1;32mOK[0m
  MUL_MAT(type_a=f16,type_b=f32,m=32,n=32,k=32,bs=[10,1],nr=[2,1]): [1;32mOK[0m
  MUL_MAT(type_a=f16,type_b=f32,m=32,n=32,k=32,bs=[10,10],nr=[1,1]): [1;32mOK[0m
  MUL_MAT(type_a=f16,type_b=f32,m=32,n=32,k=32,bs=[10,10],nr=[2,1]): [1;32mOK[0m
  MUL_MAT(type_a=f16,type_b=f32,m=32,n=32,k=32,bs=[10,10],nr=[1,2]): [1;32mOK[0m
  MUL_MAT(type_a=f16,type_b=f32,m=32,n=32,k=32,bs=[10,10],nr=[2,2]): [1;32mOK[0m
  SQR(type=f32,ne=[10,10,10,10]): [1;32mOK[0m
  CLAMP(type=f32,ne=[10,10,10,10],min=-0.500000,max=0.500000): [1;32mOK[0m
  DIAG_MASK_INF(type=f32,ne=[10,10,1,1],n_past=5): [1;32mOK[0m
  DIAG_MASK_INF(type=f32,ne=[10,10,10,1],n_past=5): [1;32mOK[0m
  DIAG_MASK_INF(type=f32,ne=[10,10,10,10],n_past=5): [1;32mOK[0m
  SOFT_MAX(type=f32,ne=[10,10,10,10]): [1;32mOK[0m
  ROPE(type=f32,ne=[128,32,10,1],n_dims=128,mode=0,n_ctx=512): [1;32mOK[0m
  ROPE(type=f32,ne=[128,40,10,1],n_dims=128,mode=0,n_ctx=512): [1;32mOK[0m
  ROPE(type=f32,ne=[128,52,10,1],n_dims=128,mode=0,n_ctx=512): [1;32mOK[0m
  ROPE(type=f32,ne=[128,64,10,1],n_dims=128,mode=0,n_ctx=512): [1;32mOK[0m
  ROPE(type=f32,ne=[64,1,10,1],n_dims=64,mode=2,n_ctx=512): [1;32mOK[0m
  ROPE(type=f32,ne=[64,71,10,1],n_dims=64,mode=2,n_ctx=512): [1;32mOK[0m
  ROPE(type=f32,ne=[64,8,10,1],n_dims=64,mode=2,n_ctx=512): [1;32mOK[0m
  ROPE(type=f32,ne=[64,128,10,1],n_dims=64,mode=2,n_ctx=512): [1;32mOK[0m
  ROPE(type=f16,ne=[128,32,10,1],n_dims=128,mode=0,n_ctx=512): [1;32mOK[0m
  ROPE(type=f16,ne=[128,40,10,1],n_dims=128,mode=0,n_ctx=512): [1;32mOK[0m
  ROPE(type=f16,ne=[128,52,10,1],n_dims=128,mode=0,n_ctx=512): [1;32mOK[0m
  ROPE(type=f16,ne=[128,64,10,1],n_dims=128,mode=0,n_ctx=512): [1;32mOK[0m
  ROPE(type=f16,ne=[64,1,10,1],n_dims=64,mode=2,n_ctx=512): [1;32mOK[0m
  ROPE(type=f16,ne=[64,71,10,1],n_dims=64,mode=2,n_ctx=512): [1;32mOK[0m
  ROPE(type=f16,ne=[64,8,10,1],n_dims=64,mode=2,n_ctx=512): [1;32mOK[0m
  ROPE(type=f16,ne=[64,128,10,1],n_dims=64,mode=2,n_ctx=512): [1;32mOK[0m
  ALIBI(type=f32,ne=[10,10,10,10],n_past=512,n_head=10,bias_max=0.500000): [1;32mOK[0m
  IM2COL(type_input=f32,type_kernel=f16,ne_input=[10,10,3,1],ne_kernel=[3,3,3,1],s0=1,s1=1,p0=1,p1=1,d0=1,d1=1,is_2D=1): [1;32mOK[0m
  CONCAT(type=f32,ne=[10,10,10,10],b_ne2=10): [1;32mOK[0m
  83/83 tests passed
  Backend CPU: [1;32mOK[0m

Backend 2/2 (CUDA0)
  Backend name: CUDA
  ABS: not supported
  SGN: not supported
  NEG: not supported
  STEP: not supported
  TANH: not supported
  ELU: not supported
  RELU(type=f32,ne=[128,10,10,10]): [1;32mOK[0m
  GELU(type=f32,ne=[128,10,10,10]): [1;32mOK[0m
  GELU_QUICK: not supported
  SILU(type=f32,ne=[128,10,10,10]): [1;32mOK[0m
  LEAKY: not supported
  GET_ROWS(type=f32,n=10,m=5,r=3): [1;32mOK[0m
  GET_ROWS(type=f32,n=16,m=5,r=3): [1;32mOK[0m
  GET_ROWS(type=f16,n=10,m=5,r=3): [1;32mOK[0m
  GET_ROWS(type=f16,n=16,m=5,r=3): [1;32mOK[0m
  REPEAT(type=f32,ne=[10,10,10,10],nr=[2,2,2,2]): [1;32mOK[0m
  DUP(type=f32,ne=[10,10,10,1]): [1;32mOK[0m
  CPY(type_src=f32,type_dst=f32,ne=[10,10,10,1]): [1;32mOK[0m
  CONT(type=f32,ne=[10,10,10,1]): [1;32mOK[0m
  ADD(type=f32,ne=[16,10,1,1],nr=[1,1,1,1]): [1;32mOK[0m
    Error: ADD: NMSE = 0.898849
  ADD(type=f32,ne=[16,10,10,1],nr=[1,1,1,1]): [1;31mFAIL[0m
    Error: ADD: NMSE = 0.999797
  ADD(type=f32,ne=[16,10,10,10],nr=[1,1,1,1]): [1;31mFAIL[0m
    Error: ADD: NMSE = 0.970187
  ADD(type=f32,ne=[16,10,10,10],nr=[1,2,1,1]): [1;31mFAIL[0m
    Error: ADD: NMSE = 0.978484
  ADD(type=f32,ne=[16,10,10,10],nr=[1,1,2,1]): [1;31mFAIL[0m
    Error: ADD: NMSE = 0.981133
  ADD(type=f32,ne=[16,10,10,10],nr=[1,1,1,2]): [1;31mFAIL[0m
    Error: ADD: NMSE = 0.995796
  ADD(type=f32,ne=[16,10,10,10],nr=[1,1,2,2]): [1;31mFAIL[0m
    Error: ADD: NMSE = 0.988458
  ADD(type=f32,ne=[16,10,10,10],nr=[1,2,2,2]): [1;31mFAIL[0m
  MUL(type=f32,ne=[16,10,1,1],nr=[1,1,1,1]): [1;32mOK[0m
    Error: MUL: NMSE = 1.821193
  MUL(type=f32,ne=[16,10,10,1],nr=[1,1,1,1]): [1;31mFAIL[0m
    Error: MUL: NMSE = 1.972689
  MUL(type=f32,ne=[16,10,10,10],nr=[1,1,1,1]): [1;31mFAIL[0m
    Error: MUL: NMSE = 2.072061
  MUL(type=f32,ne=[16,10,10,10],nr=[1,2,1,1]): [1;31mFAIL[0m
    Error: MUL: NMSE = 1.937688
  MUL(type=f32,ne=[16,10,10,10],nr=[1,1,2,1]): [1;31mFAIL[0m
    Error: MUL: NMSE = 1.877575
  MUL(type=f32,ne=[16,10,10,10],nr=[1,1,1,2]): [1;31mFAIL[0m
    Error: MUL: NMSE = 2.003267
  MUL(type=f32,ne=[16,10,10,10],nr=[1,1,2,2]): [1;31mFAIL[0m
    Error: MUL: NMSE = 1.994022
  MUL(type=f32,ne=[16,10,10,10],nr=[1,2,2,2]): [1;31mFAIL[0m
  SCALE(type=f32,ne=[10,10,10,10]): [1;32mOK[0m
  NORM(type=f32,ne=[64,10,10,10],eps=0.000001): [1;32mOK[0m
  RMS_NORM(type=f32,ne=[64,10,10,10],eps=0.000001): [1;32mOK[0m
  NORM(type=f32,ne=[64,10,10,10],eps=0.000010): [1;32mOK[0m
  RMS_NORM(type=f32,ne=[64,10,10,10],eps=0.000010): [1;32mOK[0m
    Error: NORM: NMSE = 0.000002
  NORM(type=f32,ne=[64,10,10,10],eps=0.001000): [1;31mFAIL[0m
  RMS_NORM(type=f32,ne=[64,10,10,10],eps=0.001000): [1;32mOK[0m
    Error: NORM: NMSE = 0.016035
  NORM(type=f32,ne=[64,10,10,10],eps=0.100000): [1;31mFAIL[0m
  RMS_NORM(type=f32,ne=[64,10,10,10],eps=0.100000): [1;32mOK[0m
  MUL_MAT(type_a=f32,type_b=f32,m=32,n=32,k=32,bs=[1,1],nr=[1,1]): [1;32mOK[0m
  MUL_MAT(type_a=f32,type_b=f32,m=32,n=32,k=32,bs=[10,1],nr=[1,1]): [1;32mOK[0m
  MUL_MAT(type_a=f32,type_b=f32,m=32,n=32,k=32,bs=[10,1],nr=[2,1]): [1;32mOK[0m
  MUL_MAT(type_a=f32,type_b=f32,m=32,n=32,k=32,bs=[10,10],nr=[1,1]): [1;32mOK[0m
  MUL_MAT(type_a=f32,type_b=f32,m=32,n=32,k=32,bs=[10,10],nr=[2,1]): [1;32mOK[0m
  MUL_MAT(type_a=f32,type_b=f32,m=32,n=32,k=32,bs=[10,10],nr=[1,2]): [1;32mOK[0m
  MUL_MAT(type_a=f32,type_b=f32,m=32,n=32,k=32,bs=[10,10],nr=[2,2]): [1;32mOK[0m
  MUL_MAT(type_a=f16,type_b=f32,m=32,n=32,k=32,bs=[1,1],nr=[1,1]): [1;32mOK[0m
  MUL_MAT(type_a=f16,type_b=f32,m=32,n=32,k=32,bs=[10,1],nr=[1,1]): [1;32mOK[0m
  MUL_MAT(type_a=f16,type_b=f32,m=32,n=32,k=32,bs=[10,1],nr=[2,1]): [1;32mOK[0m
  MUL_MAT(type_a=f16,type_b=f32,m=32,n=32,k=32,bs=[10,10],nr=[1,1]): [1;32mOK[0m
  MUL_MAT(type_a=f16,type_b=f32,m=32,n=32,k=32,bs=[10,10],nr=[2,1]): [1;32mOK[0m
  MUL_MAT(type_a=f16,type_b=f32,m=32,n=32,k=32,bs=[10,10],nr=[1,2]): [1;32mOK[0m
  MUL_MAT(type_a=f16,type_b=f32,m=32,n=32,k=32,bs=[10,10],nr=[2,2]): [1;32mOK[0m
  SQR(type=f32,ne=[10,10,10,10]): [1;32mOK[0m
  CLAMP(type=f32,ne=[10,10,10,10],min=-0.500000,max=0.500000): [1;32mOK[0m
  DIAG_MASK_INF(type=f32,ne=[10,10,1,1],n_past=5): [1;32mOK[0m
  DIAG_MASK_INF(type=f32,ne=[10,10,10,1],n_past=5): [1;32mOK[0m
  DIAG_MASK_INF(type=f32,ne=[10,10,10,10],n_past=5): [1;32mOK[0m
  SOFT_MAX(type=f32,ne=[10,10,10,10]): [1;32mOK[0m
  ROPE(type=f32,ne=[128,32,10,1],n_dims=128,mode=0,n_ctx=512): [1;32mOK[0m
  ROPE(type=f32,ne=[128,40,10,1],n_dims=128,mode=0,n_ctx=512): [1;32mOK[0m
  ROPE(type=f32,ne=[128,52,10,1],n_dims=128,mode=0,n_ctx=512): [1;32mOK[0m
  ROPE(type=f32,ne=[128,64,10,1],n_dims=128,mode=0,n_ctx=512): [1;32mOK[0m
  ROPE(type=f32,ne=[64,1,10,1],n_dims=64,mode=2,n_ctx=512): [1;32mOK[0m
  ROPE(type=f32,ne=[64,71,10,1],n_dims=64,mode=2,n_ctx=512): [1;32mOK[0m
  ROPE(type=f32,ne=[64,8,10,1],n_dims=64,mode=2,n_ctx=512): [1;32mOK[0m
  ROPE(type=f32,ne=[64,128,10,1],n_dims=64,mode=2,n_ctx=512): [1;32mOK[0m
  ROPE(type=f16,ne=[128,32,10,1],n_dims=128,mode=0,n_ctx=512): [1;32mOK[0m
  ROPE(type=f16,ne=[128,40,10,1],n_dims=128,mode=0,n_ctx=512): [1;32mOK[0m
  ROPE(type=f16,ne=[128,52,10,1],n_dims=128,mode=0,n_ctx=512): [1;32mOK[0m
  ROPE(type=f16,ne=[128,64,10,1],n_dims=128,mode=0,n_ctx=512): [1;32mOK[0m
  ROPE(type=f16,ne=[64,1,10,1],n_dims=64,mode=2,n_ctx=512): [1;32mOK[0m
  ROPE(type=f16,ne=[64,71,10,1],n_dims=64,mode=2,n_ctx=512): [1;32mOK[0m
  ROPE(type=f16,ne=[64,8,10,1],n_dims=64,mode=2,n_ctx=512): [1;32mOK[0m
  ROPE(type=f16,ne=[64,128,10,1],n_dims=64,mode=2,n_ctx=512): [1;32mOK[0m
  ALIBI(type=f32,ne=[10,10,10,10],n_past=512,n_head=10,bias_max=0.500000): [1;32mOK[0m
  IM2COL(type_input=f32,type_kernel=f16,ne_input=[10,10,3,1],ne_kernel=[3,3,3,1],s0=1,s1=1,p0=1,p1=1,d0=1,d1=1,is_2D=1): [1;32mOK[0m
  CONCAT: not supported
  67/83 tests passed
  Backend CUDA: [1;31mFAIL[0m

1/2 backends passed
[1;31mFAIL[0m


95% tests passed, 1 tests failed out of 19

Total Test time (real) =  34.45 sec

The following tests FAILED:
	 19 - test-backend-ops (Failed)
Errors while running CTest

real	0m34.484s
user	1m4.404s
sys	0m16.244s
+ cur=8
+ echo 8
+ set +x
