Running memcpy benchmark
memcpy:   10.58 GB/s (heat-up)
memcpy:   10.38 GB/s ( 1 thread)
memcpy:   10.61 GB/s ( 1 thread)
memcpy:   19.90 GB/s ( 2 thread)
memcpy:   24.67 GB/s ( 3 thread)
memcpy:   26.74 GB/s ( 4 thread)
sum:    -3071998818.000000
Running ggml_mul_mat benchmark with 4 threads
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
  64 x   64: Q4_0    19.2 GFLOPS (128 runs) | Q4_1    19.3 GFLOPS (128 runs)
  64 x   64: Q5_0    18.3 GFLOPS (128 runs) | Q5_1    18.1 GFLOPS (128 runs) | Q8_0    19.8 GFLOPS (128 runs)
  64 x   64: F16     17.4 GFLOPS (128 runs) | F32      9.2 GFLOPS (128 runs)
 128 x  128: Q4_0    43.0 GFLOPS (128 runs) | Q4_1    39.8 GFLOPS (128 runs)
 128 x  128: Q5_0    38.3 GFLOPS (128 runs) | Q5_1    37.1 GFLOPS (128 runs) | Q8_0    48.0 GFLOPS (128 runs)
 128 x  128: F16     34.0 GFLOPS (128 runs) | F32     22.5 GFLOPS (128 runs)
 256 x  256: Q4_0    52.9 GFLOPS (128 runs) | Q4_1    54.3 GFLOPS (128 runs)
 256 x  256: Q5_0    52.4 GFLOPS (128 runs) | Q5_1    50.4 GFLOPS (128 runs) | Q8_0    66.8 GFLOPS (128 runs)
 256 x  256: F16     56.5 GFLOPS (128 runs) | F32     32.7 GFLOPS (128 runs)
 512 x  512: Q4_0    75.9 GFLOPS (128 runs) | Q4_1    71.7 GFLOPS (128 runs)
 512 x  512: Q5_0    64.5 GFLOPS (128 runs) | Q5_1    61.0 GFLOPS (128 runs) | Q8_0    84.2 GFLOPS (128 runs)
 512 x  512: F16     69.5 GFLOPS (128 runs) | F32     39.9 GFLOPS (128 runs)
1024 x 1024: Q4_0    86.3 GFLOPS ( 41 runs) | Q4_1    86.6 GFLOPS ( 41 runs)
1024 x 1024: Q5_0    69.4 GFLOPS ( 33 runs) | Q5_1    69.2 GFLOPS ( 33 runs) | Q8_0   100.9 GFLOPS ( 47 runs)
1024 x 1024: F16     74.9 GFLOPS ( 35 runs) | F32     41.8 GFLOPS ( 20 runs)
2048 x 2048: Q4_0    90.3 GFLOPS (  6 runs) | Q4_1    93.6 GFLOPS (  6 runs)
2048 x 2048: Q5_0    75.2 GFLOPS (  5 runs) | Q5_1    74.2 GFLOPS (  5 runs) | Q8_0   108.9 GFLOPS (  7 runs)
2048 x 2048: F16     78.8 GFLOPS (  5 runs) | F32     42.8 GFLOPS (  3 runs)
4096 x 4096: Q4_0    95.5 GFLOPS (  3 runs) | Q4_1    97.2 GFLOPS (  3 runs)
4096 x 4096: Q5_0    77.4 GFLOPS (  3 runs) | Q5_1    77.5 GFLOPS (  3 runs) | Q8_0   115.4 GFLOPS (  3 runs)
4096 x 4096: F16     81.1 GFLOPS (  3 runs) | F32     36.4 GFLOPS (  3 runs)
Running benchmark for all models
|           Config |         Model |  Th |  FA |    Enc. |    Dec. |    Bch5 |      PP |  Commit |
|              --- |           --- | --- | --- |     --- |     --- |     --- |     --- |     --- |
Benchmarking model: tiny.en
|             AVX2 |       tiny.en |   4 |   0 |    6.16 |    1.02 |    0.28 |    0.01 | 7e23d8c6 |
Benchmarking model: tiny
|             AVX2 |          tiny |   4 |   0 |    5.80 |    1.00 |    0.33 |    0.01 | 7e23d8c6 |
Benchmarking model: base.en
|             AVX2 |       base.en |   4 |   0 |   10.64 |    1.42 |    0.41 |    0.02 | 7e23d8c6 |
Benchmarking model: base
|             AVX2 |          base |   4 |   0 |   10.58 |    1.42 |    0.43 |    0.02 | 7e23d8c6 |
Benchmarking model: small.en
|             AVX2 |      small.en |   4 |   0 |   29.97 |    2.81 |    0.83 |    0.04 | 7e23d8c6 |
Benchmarking model: small
|             AVX2 |         small |   4 |   0 |   29.99 |    2.78 |    0.88 |    0.04 | 7e23d8c6 |
Benchmarking model: medium.en
|             AVX2 |     medium.en |   4 |   0 |   81.47 |    5.91 |    1.81 |    0.09 | 7e23d8c6 |
Benchmarking model: medium
|             AVX2 |        medium |   4 |   0 |   81.47 |    5.97 |    1.80 |    0.09 | 7e23d8c6 |
Benchmarking model: large-v1
|             AVX2 |      large-v1 |   4 |   0 |  134.73 |    8.55 |    2.67 |    0.14 | 7e23d8c6 |
Benchmarking model: large-v2
|             AVX2 |      large-v2 |   4 |   0 |  134.68 |    8.57 |    2.70 |    0.14 | 7e23d8c6 |
Benchmarking model: large-v3
|             AVX2 |      large-v3 |   4 |   0 |  134.98 |    8.60 |    2.66 |    0.14 | 7e23d8c6 |
Benchmarking model: large-v3-turbo
|             AVX2 | large-v3-turbo |   4 |   0 |  124.44 |    1.37 |    0.44 |    0.02 | 7e23d8c6 |
